{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "592da263",
   "metadata": {},
   "source": [
    "# Identity Resolution: Matching Phase\n",
    "\n",
    "This notebook implements the matching phase of identity resolution, using candidate pairs generated by the hybrid blocking strategy from the blocking phase.\n",
    "\n",
    "## Workflow Overview\n",
    "\n",
    "1. **Setup**: Import libraries, configure paths, and initialize PyDI components\n",
    "2. **Load Data**: Load source tables, candidate pairs, and ground truth splits\n",
    "3. **Name Normalization**: Apply consistent name normalization across all matching methods\n",
    "4. **Common Matching Infrastructure**: Define reusable matching framework and evaluation functions\n",
    "5. **Rule-Based Matching**: Implement and evaluate rule-based matching with optimized variants\n",
    "6. **ML-Based Matching**: Train and evaluate multiple classifiers (LogisticRegression, RandomForest, GradientBoosting, XGBoost)\n",
    "7. **Post-Processing**: Apply global matching (one-to-one constraint) and cluster consistency analysis\n",
    "8. **Final Test Set Evaluation**: Evaluate all models on test set for final performance reporting (Section 6.4)\n",
    "9. **Export Results**: Save final correspondences from **GradientBoosting** model (after global matching) for data fusion phase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee371ad",
   "metadata": {},
   "source": [
    "## 0. Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c0624b",
   "metadata": {},
   "source": [
    "### 0.1 Import Libraries and Setup Logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d66cd730",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Matching phase logging enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Logging setup complete\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# Setup logging\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='[%(levelname)-5s] %(name)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('logs/matching.log'),\n",
    "        logging.StreamHandler()\n",
    "    ],\n",
    "    force=True\n",
    ")\n",
    "logging.getLogger().info('Matching phase logging enabled')\n",
    "\n",
    "print(\"✓ Logging setup complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01649e83",
   "metadata": {},
   "source": [
    "### 0.2 Define Paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f40666de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths configured:\n",
      "  Candidates LR: True\n",
      "  Candidates LS: True\n",
      "  Output dir: /Users/zhangzihan/Desktop/WBI_project/Schema_Mapped_Datasets/data/output/matching\n"
     ]
    }
   ],
   "source": [
    "# Project base directory\n",
    "BASE_DIR = Path('/Users/zhangzihan/Desktop/WBI_project/Schema_Mapped_Datasets')\n",
    "\n",
    "# Input: Candidate pairs from blocking phase\n",
    "CANDIDATES_DIR = BASE_DIR / 'data' / 'output' / 'workflow'\n",
    "CANDIDATES_LR = CANDIDATES_DIR / 'candidates_hybrid_LR.csv'\n",
    "CANDIDATES_LS = CANDIDATES_DIR / 'candidates_hybrid_LS.csv'\n",
    "\n",
    "# Input: Source data tables\n",
    "CLEAN_DIR = BASE_DIR / 'data' / 'output' / 'clean'\n",
    "LAHMAN_PATH = (CLEAN_DIR / 'Lahman_Mapped_dedup.xml'\n",
    "               if (CLEAN_DIR / 'Lahman_Mapped_dedup.xml').exists()\n",
    "               else BASE_DIR / 'Lahman_Mapped.xml')\n",
    "REFERENCE_PATH = (CLEAN_DIR / 'Reference_Mapped_dedup.xml'\n",
    "                  if (CLEAN_DIR / 'Reference_Mapped_dedup.xml').exists()\n",
    "                  else BASE_DIR / 'Reference_Mapped.xml')\n",
    "SAVANT_PATH = (CLEAN_DIR / 'Savant_Mapped_dedup.xml'\n",
    "               if (CLEAN_DIR / 'Savant_Mapped_dedup.xml').exists()\n",
    "               else BASE_DIR / 'Savant_Mapped.xml')\n",
    "\n",
    "# Input: Ground truth splits for evaluation\n",
    "SPLITS_DIR = BASE_DIR / 'data' / 'output' / 'gt' / 'splits'\n",
    "LR_TRAIN = SPLITS_DIR / 'gt_LR_train.csv'\n",
    "LR_VAL = SPLITS_DIR / 'gt_LR_val.csv'\n",
    "LR_TEST = SPLITS_DIR / 'gt_LR_test.csv'\n",
    "LS_TRAIN = SPLITS_DIR / 'gt_LS_train.csv'\n",
    "LS_VAL = SPLITS_DIR / 'gt_LS_val.csv'\n",
    "LS_TEST = SPLITS_DIR / 'gt_LS_test.csv'\n",
    "\n",
    "# Output: Matching results\n",
    "OUTPUT_DIR = BASE_DIR / 'data' / 'output' / 'matching'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Paths configured:\")\n",
    "print(f\"  Candidates LR: {CANDIDATES_LR.exists()}\")\n",
    "print(f\"  Candidates LS: {CANDIDATES_LS.exists()}\")\n",
    "print(f\"  Output dir: {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13510b4",
   "metadata": {},
   "source": [
    "### 0.3 Import PyDI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bbdc4ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ PyDI imported successfully\n",
      "✓ ML libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import importlib\n",
    "import PyDI  # noqa: F401\n",
    "\n",
    "from PyDI.io import load_xml\n",
    "from PyDI.entitymatching import RuleBasedMatcher, GreedyOneToOneMatchingAlgorithm, MLBasedMatcher, FeatureExtractor\n",
    "from PyDI.entitymatching.comparators import StringComparator, DateComparator\n",
    "from PyDI.entitymatching.evaluation import EntityMatchingEvaluator\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "print(\"✓ PyDI imported successfully\")\n",
    "print(\"✓ ML libraries imported successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2354ac3",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "\n",
    "### 1.1 Load Source Data Tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "10faac85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading source data tables...\n",
      "  L_full: 106,553 records\n",
      "  R_full: 15,215 records\n",
      "  S_full: 6,743 records\n",
      "✓ Source tables loaded\n"
     ]
    }
   ],
   "source": [
    "# Load source data tables\n",
    "print(\"Loading source data tables...\")\n",
    "L_full = load_xml(LAHMAN_PATH).convert_dtypes().reset_index(drop=True)\n",
    "R_full = load_xml(REFERENCE_PATH).convert_dtypes().reset_index(drop=True)\n",
    "S_full = load_xml(SAVANT_PATH).convert_dtypes().reset_index(drop=True)\n",
    "\n",
    "# Create _rid column for matching (same format as blocking phase)\n",
    "for df, tag in [(L_full, 'L'), (R_full, 'R'), (S_full, 'S')]:\n",
    "    if {'player_id', 'season_year'} <= set(df.columns):\n",
    "        pid = df['player_id'].astype('string').fillna('NA')\n",
    "        season = df['season_year'].astype('Int64').astype('string').fillna('NA')\n",
    "        df['_rid'] = pid + '|' + season + f'|{tag}'\n",
    "    else:\n",
    "        df['_rid'] = df.index.map(lambda i: f\"{tag}{i:06d}\")\n",
    "\n",
    "print(f\"  L_full: {len(L_full):,} records\")\n",
    "print(f\"  R_full: {len(R_full):,} records\")\n",
    "print(f\"  S_full: {len(S_full):,} records\")\n",
    "print(\"✓ Source tables loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4035d22",
   "metadata": {},
   "source": [
    "### 1.2 Load Candidate Pairs from Hybrid Blocking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "950f903a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading candidate pairs from hybrid blocking...\n",
      "  LR candidates: 5,994,373 pairs\n",
      "  LS candidates: 227,185 pairs\n",
      "  Total candidates: 6,221,558 pairs\n",
      "✓ Candidate pairs loaded\n"
     ]
    }
   ],
   "source": [
    "# Load candidate pairs generated by hybrid blocking strategy\n",
    "print(\"Loading candidate pairs from hybrid blocking...\")\n",
    "candidates_lr = pd.read_csv(CANDIDATES_LR)\n",
    "candidates_ls = pd.read_csv(CANDIDATES_LS)\n",
    "\n",
    "print(f\"  LR candidates: {len(candidates_lr):,} pairs\")\n",
    "print(f\"  LS candidates: {len(candidates_ls):,} pairs\")\n",
    "print(f\"  Total candidates: {len(candidates_lr) + len(candidates_ls):,} pairs\")\n",
    "print(\"✓ Candidate pairs loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "525cda2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using shared name_utils.normalize_name_for_blocking for all matching methods.\n"
     ]
    }
   ],
   "source": [
    "# Override local normalization with shared implementation\n",
    "from name_utils import normalize_name_for_blocking\n",
    "\n",
    "print(\"Using shared name_utils.normalize_name_for_blocking for all matching methods.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f5a819",
   "metadata": {},
   "source": [
    "### 1.3 Load Ground Truth Splits for Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "81c9e570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ground truth splits...\n",
      "✓ Ground truth splits loaded\n",
      "  LR: train=304, val=96, test=100\n",
      "  LS: train=296, val=101, test=103\n"
     ]
    }
   ],
   "source": [
    "# Load ground truth splits\n",
    "print(\"Loading ground truth splits...\")\n",
    "lr_train_df = pd.read_csv(LR_TRAIN)\n",
    "lr_val_df = pd.read_csv(LR_VAL)\n",
    "lr_test_df = pd.read_csv(LR_TEST)\n",
    "\n",
    "ls_train_df = pd.read_csv(LS_TRAIN)\n",
    "ls_val_df = pd.read_csv(LS_VAL)\n",
    "ls_test_df = pd.read_csv(LS_TEST)\n",
    "\n",
    "# Organize splits by edge\n",
    "splits = {\n",
    "    'LR': {'train': lr_train_df, 'val': lr_val_df, 'test': lr_test_df},\n",
    "    'LS': {'train': ls_train_df, 'val': ls_val_df, 'test': ls_test_df}\n",
    "}\n",
    "\n",
    "# Organize source tables by edge\n",
    "source_tables = {\n",
    "    'LR': (L_full, R_full),\n",
    "    'LS': (L_full, S_full)\n",
    "}\n",
    "\n",
    "# Organize candidate pairs by edge\n",
    "candidates = {\n",
    "    'LR': candidates_lr,\n",
    "    'LS': candidates_ls\n",
    "}\n",
    "\n",
    "print(\"✓ Ground truth splits loaded\")\n",
    "print(f\"  LR: train={len(lr_train_df)}, val={len(lr_val_df)}, test={len(lr_test_df)}\")\n",
    "print(f\"  LS: train={len(ls_train_df)}, val={len(ls_val_df)}, test={len(ls_test_df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869cc8e4",
   "metadata": {},
   "source": [
    "## 2. Name Normalization (Reusable)\n",
    "\n",
    "### 2.1 Apply Name normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3884db79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  L_full: 'full_name_normalized' column already exists\n",
      "  R_full: Created 'full_name_normalized' column\n",
      "  S_full: Created 'full_name_normalized' column\n",
      "✓ Name normalization complete\n"
     ]
    }
   ],
   "source": [
    "# Check if normalized names exist, if not, apply normalization\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def normalize_name_for_blocking(text: str) -> str:\n",
    "    r\"\"\"Normalize name for consistent matching (same as workflow notebook)\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    \n",
    "    # Decode literal backslash-x-hex patterns\n",
    "    def decode_literal_hex_sequence(match):\n",
    "        hex_bytes = []\n",
    "        for i in range(1, len(match.groups()) + 1):\n",
    "            hex_str = match.group(i)\n",
    "            try:\n",
    "                hex_bytes.append(int(hex_str, 16))\n",
    "            except ValueError:\n",
    "                return match.group(0)\n",
    "        try:\n",
    "            decoded = bytes(hex_bytes).decode('utf-8')\n",
    "            return decoded\n",
    "        except (UnicodeDecodeError, ValueError):\n",
    "            return match.group(0)\n",
    "    \n",
    "    text = re.sub(r'\\\\x([0-9a-fA-F]{2})\\\\x([0-9a-fA-F]{2})', decode_literal_hex_sequence, text)\n",
    "    text = re.sub(r'\\\\x([0-9a-fA-F]{2})\\\\x([0-9a-fA-F]{2})\\\\x([0-9a-fA-F]{2})', decode_literal_hex_sequence, text)\n",
    "    \n",
    "    def decode_single_hex(match):\n",
    "        hex_str = match.group(1)\n",
    "        try:\n",
    "            return chr(int(hex_str, 16))\n",
    "        except (ValueError, OverflowError):\n",
    "            return match.group(0)\n",
    "    text = re.sub(r'\\\\x([0-9a-fA-F]{2})', decode_single_hex, text)\n",
    "    \n",
    "    # Unicode normalization\n",
    "    text = unicodedata.normalize('NFD', text)\n",
    "    text = ''.join(c for c in text if unicodedata.category(c) != 'Mn')\n",
    "    \n",
    "    # Lowercase and strip\n",
    "    text = text.lower().strip()\n",
    "    \n",
    "    # Handle backslash escapes\n",
    "    text = text.replace('\\\\ ', ' ').replace('\\\\', ' ')\n",
    "    \n",
    "    # Standardize punctuation\n",
    "    text = text.replace('.', '').replace(',', '').replace('-', ' ').replace(\"'\", '')\n",
    "    \n",
    "    # Normalize spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove common suffixes\n",
    "    for suffix in [' jr', ' sr', ' ii', ' iii', ' iv', ' v']:\n",
    "        text = text.replace(suffix, '')\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply normalization if needed\n",
    "for name, df in [('L_full', L_full), ('R_full', R_full), ('S_full', S_full)]:\n",
    "    if 'full_name_normalized' not in df.columns and 'full_name' in df.columns:\n",
    "        df['full_name_normalized'] = df['full_name'].astype('string').map(normalize_name_for_blocking)\n",
    "        print(f\"  {name}: Created 'full_name_normalized' column\")\n",
    "    elif 'full_name_normalized' in df.columns:\n",
    "        print(f\"  {name}: 'full_name_normalized' column already exists\")\n",
    "\n",
    "print(\"✓ Name normalization complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02602ec",
   "metadata": {},
   "source": [
    "## 3. Common Matching Infrastructure (Reusable)\n",
    "\n",
    "This section defines reusable functions and frameworks for all matching methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a2cb66",
   "metadata": {},
   "source": [
    "### 3.1 Matching Function Interface\n",
    "\n",
    "Define a standard interface for matching functions. All matching methods should follow this pattern.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4d011031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Matching function interface defined\n"
     ]
    }
   ],
   "source": [
    "# Matching function interface\n",
    "# All matching methods should return a DataFrame with columns: ['id1', 'id2', 'sim']\n",
    "# where 'sim' is the similarity score (0.0 to 1.0)\n",
    "\n",
    "def apply_matching_method(\n",
    "    edge_name: str,\n",
    "    matching_func,\n",
    "    *args,\n",
    "    **kwargs\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply a matching method to candidate pairs for a given edge.\n",
    "    \n",
    "    Args:\n",
    "        edge_name: Edge name ('LR' or 'LS')\n",
    "        matching_func: Function that takes (left_df, right_df, candidates_df, *args, **kwargs)\n",
    "                       and returns DataFrame with ['id1', 'id2', 'sim']\n",
    "        *args, **kwargs: Additional arguments to pass to matching_func\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns ['id1', 'id2', 'sim']\n",
    "    \"\"\"\n",
    "    left_df, right_df = source_tables[edge_name]\n",
    "    cand_df = candidates[edge_name]\n",
    "    \n",
    "    print(f\"\\n=== {edge_name}: Applying matching method ===\")\n",
    "    print(f\"  Candidate pairs: {len(cand_df):,}\")\n",
    "    \n",
    "    result = matching_func(left_df, right_df, cand_df, *args, **kwargs)\n",
    "    \n",
    "    # Validate result format\n",
    "    required_cols = ['id1', 'id2', 'sim']\n",
    "    if not all(col in result.columns for col in required_cols):\n",
    "        raise ValueError(f\"Matching function must return DataFrame with columns: {required_cols}\")\n",
    "    \n",
    "    print(f\"  Generated {len(result):,} scored pairs\")\n",
    "    print(f\"  Similarity range: [{result['sim'].min():.3f}, {result['sim'].max():.3f}]\")\n",
    "    print(f\"  Mean similarity: {result['sim'].mean():.3f}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"✓ Matching function interface defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae14ddb",
   "metadata": {},
   "source": [
    "### 3.2 Evaluation Functions (Reusable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d851fe04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Evaluation functions defined\n"
     ]
    }
   ],
   "source": [
    "# Reusable evaluation functions\n",
    "from PyDI.entitymatching.evaluation import EntityMatchingEvaluator\n",
    "\n",
    "def evaluate_matching_thresholds(\n",
    "    scored_pairs: pd.DataFrame,\n",
    "    val_df: pd.DataFrame,\n",
    "    thresholds: list = [0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "    output_dir: Path = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate matching performance across different similarity thresholds.\n",
    "    \n",
    "    Args:\n",
    "        scored_pairs: DataFrame with columns ['id1', 'id2', 'sim']\n",
    "        val_df: Validation ground truth with columns ['id1', 'id2', 'label']\n",
    "        thresholds: List of similarity thresholds to evaluate\n",
    "        output_dir: Directory for evaluation outputs\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with best_threshold, best_f1, and detailed metrics\n",
    "    \"\"\"\n",
    "    best_threshold = None\n",
    "    best_f1 = 0.0\n",
    "    threshold_metrics = {}\n",
    "    \n",
    "    print(\"\\nThreshold Analysis:\")\n",
    "    print(f\"{'Threshold':<12} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'TP':<8} {'FP':<8} {'FN':<8}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        # Filter by threshold\n",
    "        matched_pairs = scored_pairs[scored_pairs['sim'] >= threshold][['id1', 'id2']].copy()\n",
    "        \n",
    "        # Evaluate\n",
    "        metrics = EntityMatchingEvaluator.evaluate_matching(\n",
    "            predicted_pairs=matched_pairs,\n",
    "            test_pairs=val_df,\n",
    "            out_dir=output_dir\n",
    "        )\n",
    "        \n",
    "        precision = metrics.get('precision', 0.0)\n",
    "        recall = metrics.get('recall', 0.0)\n",
    "        f1 = metrics.get('f1_score', 0.0)\n",
    "        tp = metrics.get('true_positives', 0)\n",
    "        fp = metrics.get('false_positives', 0)\n",
    "        fn = metrics.get('false_negatives', 0)\n",
    "        \n",
    "        threshold_metrics[threshold] = metrics\n",
    "        print(f\"{threshold:<12.1f} {precision:<12.3f} {recall:<12.3f} {f1:<12.3f} {tp:<8} {fp:<8} {fn:<8}\")\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    return {\n",
    "        'best_threshold': best_threshold,\n",
    "        'best_f1': best_f1,\n",
    "        'thresholds': thresholds,\n",
    "        'threshold_metrics': threshold_metrics\n",
    "    }\n",
    "\n",
    "def apply_season_year_constraint(\n",
    "    scored_pairs: pd.DataFrame,\n",
    "    left_df: pd.DataFrame,\n",
    "    right_df: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter scored pairs to ensure season_year matches exactly.\n",
    "    \n",
    "    Args:\n",
    "        scored_pairs: DataFrame with columns ['id1', 'id2', 'sim']\n",
    "        left_df: Left source table with '_rid' and 'season_year' columns\n",
    "        right_df: Right source table with '_rid' and 'season_year' columns\n",
    "    \n",
    "    Returns:\n",
    "        Filtered DataFrame with same columns\n",
    "    \"\"\"\n",
    "    # Merge to get season_year\n",
    "    matches = scored_pairs.merge(\n",
    "        left_df[['_rid', 'season_year']],\n",
    "        left_on='id1',\n",
    "        right_on='_rid',\n",
    "        suffixes=('', '_left')\n",
    "    ).merge(\n",
    "        right_df[['_rid', 'season_year']],\n",
    "        left_on='id2',\n",
    "        right_on='_rid',\n",
    "        suffixes=('', '_right')\n",
    "    )\n",
    "    \n",
    "    # Filter: season_year must match exactly\n",
    "    matches = matches[matches['season_year'] == matches['season_year_right']]\n",
    "    \n",
    "    # Return original columns\n",
    "    return matches[['id1', 'id2', 'sim']].copy()\n",
    "\n",
    "def apply_global_matching(\n",
    "    scored_pairs: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply greedy one-to-one matching to resolve conflicts.\n",
    "    \n",
    "    Args:\n",
    "        scored_pairs: DataFrame with columns ['id1', 'id2', 'sim']\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with one-to-one matches (same columns)\n",
    "    \"\"\"\n",
    "    # Convert 'sim' to 'score' if needed (PyDI expects 'score' column)\n",
    "    correspondences = scored_pairs.copy()\n",
    "    if 'sim' in correspondences.columns and 'score' not in correspondences.columns:\n",
    "        correspondences = correspondences.rename(columns={'sim': 'score'})\n",
    "    \n",
    "    # Apply greedy one-to-one matching using .cluster() method\n",
    "    global_matcher = GreedyOneToOneMatchingAlgorithm()\n",
    "    global_matched = global_matcher.cluster(correspondences)\n",
    "    \n",
    "    # Convert 'score' back to 'sim' if original had 'sim'\n",
    "    if 'sim' in scored_pairs.columns and 'score' in global_matched.columns:\n",
    "        global_matched = global_matched.rename(columns={'score': 'sim'})\n",
    "    \n",
    "    return global_matched\n",
    "\n",
    "print(\"✓ Evaluation functions defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b09db14",
   "metadata": {},
   "source": [
    "## 4. Rule-Based Matching\n",
    "\n",
    "Use similarity comparators to compute matching scores:\n",
    "- **Name similarity**: Levenshtein distance (weight: 0.7) and Jaccard similarity (weight: 0.3)\n",
    "- **Season year**: Must match exactly (hard constraint - if not matched, similarity = 0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "31b58bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule-based comparators configured:\n",
      "  Name comparators: Levenshtein (0.7), Jaccard (0.3)\n",
      "  Hard constraint: season_year must match exactly\n",
      "  Total comparators: 2\n"
     ]
    }
   ],
   "source": [
    "# Configure comparators for rule-based matching\n",
    "from PyDI.entitymatching.comparators import StringComparator, DateComparator\n",
    "\n",
    "# Name comparators (using normalized names)\n",
    "name_comparators = [\n",
    "    StringComparator(\n",
    "        column=\"full_name_normalized\" if 'full_name_normalized' in L_full.columns else \"full_name\",\n",
    "        similarity_function=\"levenshtein\",\n",
    "        preprocess=str.lower\n",
    "    ),\n",
    "    StringComparator(\n",
    "        column=\"full_name_normalized\" if 'full_name_normalized' in L_full.columns else \"full_name\",\n",
    "        similarity_function=\"jaccard\",\n",
    "        tokenization=\"word\",\n",
    "        preprocess=str.lower\n",
    "    )\n",
    "]\n",
    "name_weights = [0.7, 0.3]  # Emphasize Levenshtein for name matching\n",
    "\n",
    "# Matching strategy: name similarity + season_year hard constraint\n",
    "# season_year is checked as a hard constraint (must match exactly) before computing similarity\n",
    "# Only name comparators are used for similarity scoring\n",
    "\n",
    "# Use only name comparators (season_year is handled as hard constraint)\n",
    "rule_based_comparators = name_comparators\n",
    "rule_based_weights = name_weights  # Levenshtein (0.7), Jaccard (0.3)\n",
    "\n",
    "print(\"Rule-based comparators configured:\")\n",
    "print(f\"  Name comparators: Levenshtein (0.7), Jaccard (0.3)\")\n",
    "print(f\"  Hard constraint: season_year must match exactly\")\n",
    "print(f\"  Total comparators: {len(rule_based_comparators)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "21c4e226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized matching configuration:\n",
      "  Name variant dictionary: 38 variants\n",
      "  Adjusted threshold: 0.7 (from 0.7)\n",
      "  Birth year constraint: penalty=0.2 for year_diff > 1\n"
     ]
    }
   ],
   "source": [
    "# Name variant helpers and configuration\n",
    "\n",
    "NAME_VARIANTS = {\n",
    "    'dan': ['daniel'],\n",
    "    'daniel': ['dan', 'danny'],\n",
    "    'danny': ['daniel'],\n",
    "    'matt': ['matthew'],\n",
    "    'matthew': ['matt'],\n",
    "    'jon': ['jonathon'],\n",
    "    'jonathon': ['jon'],\n",
    "    'cal': ['calvin'],\n",
    "    'calvin': ['cal'],\n",
    "    'phil': ['phillip'],\n",
    "    'phillip': ['phil'],\n",
    "    'philip': ['phil'],\n",
    "    'rafael': ['raffy'],\n",
    "    'raffy': ['rafael'],\n",
    "    'jim': ['james'],\n",
    "    'james': ['jim'],\n",
    "    'bob': ['robert'],\n",
    "    'robert': ['bob', 'rob'],\n",
    "    'rob': ['robert'],\n",
    "    'bill': ['william'],\n",
    "    'william': ['bill'],\n",
    "    'mike': ['michael'],\n",
    "    'michael': ['mike'],\n",
    "    'dave': ['david'],\n",
    "    'david': ['dave'],\n",
    "    'chris': ['christopher'],\n",
    "    'christopher': ['chris'],\n",
    "    'tom': ['thomas'],\n",
    "    'thomas': ['tom'],\n",
    "    'ed': ['edward'],\n",
    "    'edward': ['ed'],\n",
    "    'rick': ['richard'],\n",
    "    'richard': ['rick'],\n",
    "    'nick': ['nicholas', 'nicky'],\n",
    "    'nicky': ['nick'],\n",
    "    'nicholas': ['nick'],\n",
    "    'nori': ['norichika'],\n",
    "    'norichika': ['nori'],\n",
    "}\n",
    "\n",
    "def _normalize_variant_targets(value):\n",
    "    if value is None:\n",
    "        return set()\n",
    "    if isinstance(value, str):\n",
    "        return {value}\n",
    "    if isinstance(value, (list, tuple, set)):\n",
    "        return set(value)\n",
    "    return set()\n",
    "\n",
    "def tokens_are_variants(token_a: str, token_b: str) -> bool:\n",
    "    a = (token_a or '').lower()\n",
    "    b = (token_b or '').lower()\n",
    "    if not a or not b:\n",
    "        return False\n",
    "    if a == b:\n",
    "        return True\n",
    "    return (\n",
    "        b in _normalize_variant_targets(NAME_VARIANTS.get(a))\n",
    "        or a in _normalize_variant_targets(NAME_VARIANTS.get(b))\n",
    "    )\n",
    "\n",
    "def split_first_last(name: str):\n",
    "    tokens = str(name).lower().split()\n",
    "    if not tokens:\n",
    "        return '', ''\n",
    "    return tokens[0], tokens[-1]\n",
    "\n",
    "def parse_birth_year(value):\n",
    "    try:\n",
    "        if value is None or (isinstance(value, float) and value != value) or value == '':\n",
    "            return None\n",
    "        return int(float(value))\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "def birth_year_diff(year1, year2):\n",
    "    y1 = parse_birth_year(year1)\n",
    "    y2 = parse_birth_year(year2)\n",
    "    if y1 is None or y2 is None:\n",
    "        return None\n",
    "    return abs(y1 - y2)\n",
    "\n",
    "def birth_year_within(year1, year2, tolerance: int = 1) -> bool:\n",
    "    diff = birth_year_diff(year1, year2)\n",
    "    return diff is not None and diff <= tolerance\n",
    "\n",
    "# Strategy 1: Name variant dictionary awareness for optimized matching\n",
    "\n",
    "def check_name_variant_match(name1, name2):\n",
    "    \"\"\"Check if two multi-token names are variants ignoring suffix differences.\"\"\"\n",
    "    name1_words = name1.lower().split()\n",
    "    name2_words = name2.lower().split()\n",
    "    if len(name1_words) != len(name2_words):\n",
    "        return False\n",
    "    for w1, w2 in zip(name1_words, name2_words):\n",
    "        if w1 == w2:\n",
    "            continue\n",
    "        if not tokens_are_variants(w1, w2):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def apply_birth_year_constraint(similarity_score, birth1, birth2, penalty=0.2):\n",
    "    \"\"\"Apply birth year soft constraint: reduce similarity if birth years differ by more than 1 year.\"\"\"\n",
    "    diff = birth_year_diff(birth1, birth2)\n",
    "    if diff is None:\n",
    "        return similarity_score\n",
    "    if diff > 1:\n",
    "        return similarity_score * (1 - penalty)\n",
    "    if diff == 1:\n",
    "        return similarity_score * (1 - penalty * 0.5)\n",
    "    return similarity_score\n",
    "\n",
    "def compute_enhanced_similarity(left_record, right_record, comparators, weights):\n",
    "    \"\"\"Compute enhanced similarity with name variant handling.\"\"\"\n",
    "    name1 = str(left_record.get('full_name_normalized', left_record.get('full_name', ''))).lower()\n",
    "    name2 = str(right_record.get('full_name_normalized', right_record.get('full_name', ''))).lower()\n",
    "    base_scores = []\n",
    "    for comparator in comparators:\n",
    "        col = comparator.column\n",
    "        left_val = str(left_record.get(col, ''))\n",
    "        right_val = str(right_record.get(col, ''))\n",
    "        if hasattr(comparator, 'similarity_function'):\n",
    "            if comparator.similarity_function == 'levenshtein':\n",
    "                from difflib import SequenceMatcher\n",
    "                sim = SequenceMatcher(None, left_val.lower(), right_val.lower()).ratio()\n",
    "            elif comparator.similarity_function == 'jaccard':\n",
    "                left_tokens = set(left_val.lower().split())\n",
    "                right_tokens = set(right_val.lower().split())\n",
    "                sim = 1.0 if not (left_tokens or right_tokens) else len(left_tokens & right_tokens) / len(left_tokens | right_tokens)\n",
    "            else:\n",
    "                sim = 0.0\n",
    "        else:\n",
    "            sim = 0.0\n",
    "        base_scores.append(sim)\n",
    "    base_score = sum(s * w for s, w in zip(base_scores, weights))\n",
    "    if check_name_variant_match(name1, name2):\n",
    "        base_score = min(1.0, base_score + 0.15)\n",
    "    return base_score\n",
    "\n",
    "optimized_threshold = 0.7\n",
    "\n",
    "print(\"Optimized matching configuration:\")\n",
    "print(f\"  Name variant dictionary: {len(NAME_VARIANTS)} variants\")\n",
    "print(f\"  Adjusted threshold: {optimized_threshold} (from 0.7)\")\n",
    "print(f\"  Birth year constraint: penalty={0.2} for year_diff > 1\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caedfb8",
   "metadata": {},
   "source": [
    "### 4.1 Define Rule-Based Matching Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f99b46f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Rule-based matching function defined\n"
     ]
    }
   ],
   "source": [
    "def rule_based_matching(\n",
    "    left_df: pd.DataFrame,\n",
    "    right_df: pd.DataFrame,\n",
    "    cand_df: pd.DataFrame,\n",
    "    comparators: list | None = None,\n",
    "    weights: list | None = None,\n",
    "    threshold: float = 0.7,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Match candidate pairs using PyDI's RuleBasedMatcher.\"\"\"\n",
    "    matcher = RuleBasedMatcher()\n",
    "    correspondences = matcher.match(\n",
    "        df_left=left_df,\n",
    "        df_right=right_df,\n",
    "        candidates=cand_df,\n",
    "        id_column='_rid',\n",
    "        comparators=comparators or rule_based_comparators,\n",
    "        weights=weights or rule_based_weights,\n",
    "        threshold=threshold,\n",
    "        debug=False,\n",
    "    )\n",
    "    # Normalize column name so downstream code can expect 'sim'\n",
    "    if 'score' in correspondences.columns and 'sim' not in correspondences.columns:\n",
    "        correspondences = correspondences.rename(columns={'score': 'sim'})\n",
    "    return correspondences[['id1', 'id2', 'sim']]\n",
    "\n",
    "print(\"✓ Rule-based matching function defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c479cdd",
   "metadata": {},
   "source": [
    "### 4.2 Apply Rule-Based Matching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c66fd9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LR: Applying matching method ===\n",
      "  Candidate pairs: 5,994,373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.rule_based.RuleBasedMatcher - Starting Entity Matching\n",
      "[INFO ] PyDI.entitymatching.rule_based.RuleBasedMatcher - Blocking 106553 x 15215 elements\n",
      "[INFO ] PyDI.entitymatching.rule_based.RuleBasedMatcher - Matching 106553 x 15215 elements after 0:00:0.212; 5994373 blocked pairs (reduction ratio: 0.9963025175189331)\n",
      "[INFO ] PyDI.entitymatching.rule_based.RuleBasedMatcher - Entity Matching finished after 0:00:907.779; found 136453 correspondences.\n",
      "[INFO ] PyDI.entitymatching.rule_based.RuleBasedMatcher - Starting Entity Matching\n",
      "[INFO ] PyDI.entitymatching.rule_based.RuleBasedMatcher - Blocking 106553 x 6743 elements\n",
      "[INFO ] PyDI.entitymatching.rule_based.RuleBasedMatcher - Matching 106553 x 6743 elements after 0:00:0.088; 227185 blocked pairs (reduction ratio: 0.999683800767084)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 136,453 scored pairs\n",
      "  Similarity range: [0.700, 1.000]\n",
      "  Mean similarity: 0.990\n",
      "\n",
      "=== LS: Applying matching method ===\n",
      "  Candidate pairs: 227,185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.rule_based.RuleBasedMatcher - Entity Matching finished after 0:00:27.619; found 54163 correspondences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 54,163 scored pairs\n",
      "  Similarity range: [0.700, 1.000]\n",
      "  Mean similarity: 0.993\n",
      "\n",
      "✓ Rule-based matching complete for all edges\n"
     ]
    }
   ],
   "source": [
    "# Apply rule-based matching to all edges\n",
    "matching_results_rule_based = {}\n",
    "\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    matching_results_rule_based[edge_name] = apply_matching_method(\n",
    "        edge_name,\n",
    "        rule_based_matching,\n",
    "        comparators=rule_based_comparators,\n",
    "        weights=rule_based_weights\n",
    "    )\n",
    "\n",
    "print(\"\\n✓ Rule-based matching complete for all edges\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e140d1",
   "metadata": {},
   "source": [
    "Apply RuleBasedMatcher to compute similarity scores for all candidate pairs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ee22b335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LR: Rule-Based Matching ===\n",
      "  Filtering candidate pairs by season_year constraint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.rule_based.RuleBasedMatcher - Starting Entity Matching\n",
      "[INFO ] PyDI.entitymatching.rule_based.RuleBasedMatcher - Blocking 106553 x 15215 elements\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  After season_year filter: 135,944 candidate pairs (from 5,994,373)\n",
      "  Computing similarity scores using PyDI RuleBasedMatcher (threshold=0.7)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.rule_based.RuleBasedMatcher - Matching 106553 x 15215 elements after 0:00:0.291; 135944 blocked pairs (reduction ratio: 0.9999161462661056)\n",
      "[INFO ] PyDI.entitymatching.rule_based.RuleBasedMatcher - Entity Matching finished after 0:00:21.904; found 14981 correspondences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 14,981 matched pairs (above threshold 0.7)\n",
      "  Similarity score range: [0.700, 1.000]\n",
      "  Mean similarity: 0.998\n",
      "\n",
      "=== LS: Rule-Based Matching ===\n",
      "  Filtering candidate pairs by season_year constraint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.rule_based.RuleBasedMatcher - Starting Entity Matching\n",
      "[INFO ] PyDI.entitymatching.rule_based.RuleBasedMatcher - Blocking 106553 x 6743 elements\n",
      "[INFO ] PyDI.entitymatching.rule_based.RuleBasedMatcher - Matching 106553 x 6743 elements after 0:00:0.088; 9729 blocked pairs (reduction ratio: 0.9999864590429076)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  After season_year filter: 9,729 candidate pairs (from 227,185)\n",
      "  Computing similarity scores using PyDI RuleBasedMatcher (threshold=0.7)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.rule_based.RuleBasedMatcher - Entity Matching finished after 0:00:1.723; found 6582 correspondences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 6,582 matched pairs (above threshold 0.7)\n",
      "  Similarity score range: [0.700, 1.000]\n",
      "  Mean similarity: 0.997\n"
     ]
    }
   ],
   "source": [
    "# Define matching threshold\n",
    "threshold = 0.7\n",
    "\n",
    "matching_results = {}\n",
    "matchers = {}\n",
    "\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    print(f\"\\n=== {edge_name}: Rule-Based Matching ===\")\n",
    "    \n",
    "    left_df, right_df = source_tables[edge_name]\n",
    "    cand_df = candidates[edge_name].copy()\n",
    "    \n",
    "    # Apply season_year hard constraint: filter candidate pairs where season_year matches\n",
    "    print(f\"  Filtering candidate pairs by season_year constraint...\")\n",
    "    cand_with_seasons = cand_df.merge(\n",
    "        left_df[['_rid', 'season_year']],\n",
    "        left_on='id1',\n",
    "        right_on='_rid',\n",
    "        how='left'\n",
    "    ).merge(\n",
    "        right_df[['_rid', 'season_year']],\n",
    "        left_on='id2',\n",
    "        right_on='_rid',\n",
    "        how='left',\n",
    "        suffixes=('', '_right')\n",
    "    )\n",
    "    \n",
    "    # Filter: season_year must match exactly\n",
    "    cand_df_filtered = cand_with_seasons[\n",
    "        (cand_with_seasons['season_year'].notna()) & \n",
    "        (cand_with_seasons['season_year_right'].notna()) &\n",
    "        (cand_with_seasons['season_year'] == cand_with_seasons['season_year_right'])\n",
    "    ][['id1', 'id2']].copy()\n",
    "    \n",
    "    print(f\"  After season_year filter: {len(cand_df_filtered):,} candidate pairs (from {len(cand_df):,})\")\n",
    "    \n",
    "    # Initialize matcher (following exercise file pattern)\n",
    "    matcher = RuleBasedMatcher()\n",
    "    \n",
    "    # Match with a single threshold (following exercise file pattern)\n",
    "    print(f\"  Computing similarity scores using PyDI RuleBasedMatcher (threshold={threshold})...\")\n",
    "    correspondences = matcher.match(\n",
    "        df_left=left_df,\n",
    "        df_right=right_df,\n",
    "        candidates=cand_df_filtered,\n",
    "        id_column='_rid',\n",
    "        comparators=rule_based_comparators,\n",
    "        weights=rule_based_weights,\n",
    "        threshold=threshold,\n",
    "        debug=False\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    matching_results[edge_name] = correspondences\n",
    "    matchers[edge_name] = matcher\n",
    "    \n",
    "    print(f\"  Generated {len(correspondences):,} matched pairs (above threshold {threshold})\")\n",
    "    if 'score' in correspondences.columns:\n",
    "        print(f\"  Similarity score range: [{correspondences['score'].min():.3f}, {correspondences['score'].max():.3f}]\")\n",
    "        print(f\"  Mean similarity: {correspondences['score'].mean():.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e4d066",
   "metadata": {},
   "source": [
    "### 4.3 Evaluate Rule-Based Matching\n",
    "\n",
    "Assess matching performance using different similarity thresholds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3a336e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LR: Matching Evaluation (Validation Set) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Confusion Matrix:\n",
      "[INFO ] root -   True Positives:  25\n",
      "[INFO ] root -   True Negatives:  65\n",
      "[INFO ] root -   False Positives: 1\n",
      "[INFO ] root -   False Negatives: 5\n",
      "[INFO ] root - Performance Metrics:\n",
      "[INFO ] root -   Accuracy:  0.938\n",
      "[INFO ] root -   Precision: 0.962\n",
      "[INFO ] root -   Recall:    0.833\n",
      "[INFO ] root -   F1-Score:  0.893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Precision: 0.962\n",
      "  Recall:    0.833\n",
      "  F1-Score:  0.893\n",
      "  TP: 25\n",
      "  FP: 1\n",
      "  FN: 5\n",
      "\n",
      "=== LS: Matching Evaluation (Validation Set) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Confusion Matrix:\n",
      "[INFO ] root -   True Positives:  41\n",
      "[INFO ] root -   True Negatives:  45\n",
      "[INFO ] root -   False Positives: 2\n",
      "[INFO ] root -   False Negatives: 8\n",
      "[INFO ] root - Performance Metrics:\n",
      "[INFO ] root -   Accuracy:  0.896\n",
      "[INFO ] root -   Precision: 0.953\n",
      "[INFO ] root -   Recall:    0.837\n",
      "[INFO ] root -   F1-Score:  0.891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Precision: 0.953\n",
      "  Recall:    0.837\n",
      "  F1-Score:  0.891\n",
      "  TP: 41\n",
      "  FP: 2\n",
      "  FN: 8\n"
     ]
    }
   ],
   "source": [
    "# Evaluate matching on validation set (following exercise file pattern)\n",
    "# Simple evaluation with a single threshold\n",
    "\n",
    "from PyDI.entitymatching.evaluation import EntityMatchingEvaluator\n",
    "\n",
    "matching_metrics_val = {}\n",
    "\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    print(f\"\\n=== {edge_name}: Matching Evaluation (Validation Set) ===\")\n",
    "    \n",
    "    correspondences = matching_results[edge_name]\n",
    "    val_df = splits[edge_name]['val'][['id1', 'id2', 'label']].copy()\n",
    "    \n",
    "    # Evaluate matching (following exercise file pattern)\n",
    "    eval_results = EntityMatchingEvaluator.evaluate_matching(\n",
    "        correspondences=correspondences,\n",
    "        test_pairs=val_df,\n",
    "        out_dir=OUTPUT_DIR / 'matching-evaluation',\n",
    "        matcher_instance=matchers[edge_name]\n",
    "    )\n",
    "    \n",
    "    matching_metrics_val[edge_name] = eval_results\n",
    "    \n",
    "    print(f\"\\n  Precision: {eval_results.get('precision', 0.0):.3f}\")\n",
    "    print(f\"  Recall:    {eval_results.get('recall', 0.0):.3f}\")\n",
    "    print(f\"  F1-Score:  {eval_results.get('f1', 0.0):.3f}\")\n",
    "    print(f\"  TP: {eval_results.get('true_positives', 0)}\")\n",
    "    print(f\"  FP: {eval_results.get('false_positives', 0)}\")\n",
    "    print(f\"  FN: {eval_results.get('false_negatives', 0)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29886579",
   "metadata": {},
   "source": [
    "### 4.4 Analyze Rule-Based Error Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "587a9677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LR: Error Cases Analysis\n",
      "================================================================================\n",
      "\n",
      "[FALSE NEGATIVES] (5 cases):\n",
      "\n",
      "  4770032011|2011|L <-> 4770032011|2011|R\n",
      "    Left:  'jonathon niese' | Season: 2011 | Birth: 1986\n",
      "    Right: 'jon niese' | Season: 2011 | Birth: 1987\n",
      "    In candidates: True\n",
      "\n",
      "  6211992018|2018|L <-> 6211992018|2018|R\n",
      "    Left:  'matthew bowman' | Season: 2018 | Birth: 1991\n",
      "    Right: 'matt bowman' | Season: 2018 | Birth: 1991\n",
      "    In candidates: True\n",
      "\n",
      "  6687512022|2022|L <-> 6687512022|2022|R\n",
      "    Left:  'cal mitchell' | Season: 2022 | Birth: 1999\n",
      "    Right: 'calvin mitchell' | Season: 2022 | Birth: 1999\n",
      "    In candidates: True\n",
      "\n",
      "  6135642021|2021|L <-> 6135642021|2021|R\n",
      "    Left:  'jason vosler' | Season: 2021 | Birth: 1993\n",
      "    Right: 'jason vosler' | Season: 2021 | Birth: 1994\n",
      "    In candidates: True\n",
      "\n",
      "  4770032014|2014|L <-> 4770032014|2014|R\n",
      "    Left:  'jonathon niese' | Season: 2014 | Birth: 1986\n",
      "    Right: 'jon niese' | Season: 2014 | Birth: 1987\n",
      "    In candidates: True\n",
      "\n",
      "[FALSE POSITIVES] (1 cases):\n",
      "\n",
      "  6756562021|2021|L <-> 6073452021|2021|R (score: 0.736)\n",
      "    Left:  'kevin smith' | Season: 2021 | Birth: 1996\n",
      "    Right: 'kevan smith' | Season: 2021 | Birth: 1988\n",
      "\n",
      "================================================================================\n",
      "LS: Error Cases Analysis\n",
      "================================================================================\n",
      "\n",
      "[FALSE NEGATIVES] (8 cases):\n",
      "\n",
      "  6404472020|2020|L <-> 6404472020|2020|S\n",
      "    Left:  'phil ervin' | Season: 2020 | Birth: 1992\n",
      "    Right: 'phillip ervin' | Season: 2020 | Birth: 1993\n",
      "    In candidates: True\n",
      "\n",
      "  5437062017|2017|L <-> 5437062017|2017|S\n",
      "    Left:  'dan robertson' | Season: 2017 | Birth: 1985\n",
      "    Right: 'daniel robertson' | Season: 2017 | Birth: 1986\n",
      "    In candidates: True\n",
      "\n",
      "  6232052018|2018|L <-> 6232052018|2018|S\n",
      "    Left:  'andrew velazquez' | Season: 2018 | Birth: 1994\n",
      "    Right: 'andrew velazquez' | Season: 2018 | Birth: 1995\n",
      "    In candidates: True\n",
      "\n",
      "  5437062016|2016|L <-> 5437062016|2016|S\n",
      "    Left:  'dan robertson' | Season: 2016 | Birth: 1985\n",
      "    Right: 'daniel robertson' | Season: 2016 | Birth: 1986\n",
      "    In candidates: True\n",
      "\n",
      "  5949402015|2015|L <-> 5949402015|2015|S\n",
      "    Left:  'daniel muno' | Season: 2015 | Birth: 1989\n",
      "    Right: 'danny muno' | Season: 2015 | Birth: 1989\n",
      "    In candidates: True\n",
      "\n",
      "  6072572018|2018|L <-> 6072572018|2018|S\n",
      "    Left:  'rafael lopez' | Season: 2018 | Birth: 1987\n",
      "    Right: 'raffy lopez' | Season: 2018 | Birth: 1988\n",
      "    In candidates: True\n",
      "\n",
      "  6404472018|2018|L <-> 6404472018|2018|S\n",
      "    Left:  'phil ervin' | Season: 2018 | Birth: 1992\n",
      "    Right: 'phillip ervin' | Season: 2018 | Birth: 1993\n",
      "    In candidates: True\n",
      "\n",
      "  6072572017|2017|L <-> 6072572017|2017|S\n",
      "    Left:  'rafael lopez' | Season: 2017 | Birth: 1987\n",
      "    Right: 'raffy lopez' | Season: 2017 | Birth: 1988\n",
      "    In candidates: True\n",
      "\n",
      "[FALSE POSITIVES] (2 cases):\n",
      "\n",
      "  5167142015|2015|L <-> 6458482015|2015|S (score: 0.700)\n",
      "    Left:  'dario alvarez' | Season: 2015 | Birth: 1989\n",
      "    Right: 'dariel alvarez' | Season: 2015 | Birth: 1989\n",
      "\n",
      "  6703512022|2022|L <-> 6689422022|2022|S (score: 0.730)\n",
      "    Left:  'jose rojas' | Season: 2022 | Birth: 1993\n",
      "    Right: 'josh rojas' | Season: 2022 | Birth: 1994\n"
     ]
    }
   ],
   "source": [
    "for edge_name in ['LR', 'LS']:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{edge_name}: Error Cases Analysis\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    left_df, right_df = source_tables[edge_name]\n",
    "    val_df = splits[edge_name]['val'][['id1', 'id2', 'label']].copy()\n",
    "    correspondences = matching_results[edge_name]\n",
    "    \n",
    "    # Get true matches and false matches in validation set\n",
    "    true_matches = val_df[val_df['label'].astype(str).str.strip().str.upper() == 'TRUE']\n",
    "    false_matches = val_df[val_df['label'].astype(str).str.strip().str.upper() == 'FALSE']\n",
    "    true_set = set(zip(true_matches['id1'], true_matches['id2']))\n",
    "    false_set = set(zip(false_matches['id1'], false_matches['id2']))\n",
    "    \n",
    "    # Get predicted matches (only those in validation set)\n",
    "    pred_set = set(zip(correspondences['id1'], correspondences['id2']))\n",
    "    val_set = set(zip(val_df['id1'], val_df['id2']))  # All pairs in validation set\n",
    "    \n",
    "    # False Negatives: True matches in validation set that were not predicted\n",
    "    fn_pairs = true_set - pred_set\n",
    "    print(f\"\\n[FALSE NEGATIVES] ({len(fn_pairs)} cases):\")\n",
    "    if len(fn_pairs) > 0:\n",
    "        for id1, id2 in fn_pairs:\n",
    "            left_rec = left_df[left_df['_rid'] == id1].iloc[0] if len(left_df[left_df['_rid'] == id1]) > 0 else None\n",
    "            right_rec = right_df[right_df['_rid'] == id2].iloc[0] if len(right_df[right_df['_rid'] == id2]) > 0 else None\n",
    "            \n",
    "            if left_rec is not None and right_rec is not None:\n",
    "                print(f\"\\n  {id1} <-> {id2}\")\n",
    "                print(f\"    Left:  '{left_rec.get('full_name', 'N/A')}' | Season: {left_rec.get('season_year', 'N/A')} | Birth: {left_rec.get('birth_year', 'N/A')}\")\n",
    "                print(f\"    Right: '{right_rec.get('full_name', 'N/A')}' | Season: {right_rec.get('season_year', 'N/A')} | Birth: {right_rec.get('birth_year', 'N/A')}\")\n",
    "                \n",
    "                # Check if in candidates\n",
    "                in_candidates = len(candidates[edge_name][\n",
    "                    (candidates[edge_name]['id1'] == id1) & \n",
    "                    (candidates[edge_name]['id2'] == id2)\n",
    "                ]) > 0\n",
    "                print(f\"    In candidates: {in_candidates}\")\n",
    "    \n",
    "    # False Positives: Predicted matches that are in validation set but labeled as FALSE\n",
    "    # Only analyze pairs that are in the validation set\n",
    "    fp_pairs = (pred_set & val_set) & false_set\n",
    "    print(f\"\\n[FALSE POSITIVES] ({len(fp_pairs)} cases):\")\n",
    "    if len(fp_pairs) > 0:\n",
    "        for id1, id2 in fp_pairs:\n",
    "            left_rec = left_df[left_df['_rid'] == id1].iloc[0] if len(left_df[left_df['_rid'] == id1]) > 0 else None\n",
    "            right_rec = right_df[right_df['_rid'] == id2].iloc[0] if len(right_df[right_df['_rid'] == id2]) > 0 else None\n",
    "            score_row = correspondences[(correspondences['id1'] == id1) & (correspondences['id2'] == id2)]\n",
    "            score = score_row['score'].iloc[0] if len(score_row) > 0 else None\n",
    "            \n",
    "            if left_rec is not None and right_rec is not None:\n",
    "                score_str = f\"{score:.3f}\" if score is not None else \"N/A\"\n",
    "                print(f\"\\n  {id1} <-> {id2} (score: {score_str})\")\n",
    "                print(f\"    Left:  '{left_rec.get('full_name', 'N/A')}' | Season: {left_rec.get('season_year', 'N/A')} | Birth: {left_rec.get('birth_year', 'N/A')}\")\n",
    "                print(f\"    Right: '{right_rec.get('full_name', 'N/A')}' | Season: {right_rec.get('season_year', 'N/A')} | Birth: {right_rec.get('birth_year', 'N/A')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e713603",
   "metadata": {},
   "source": [
    "### 4.5 Optimized Rule-Based Matching (Name Variants + Birth Year Constraint)\n",
    "Based on error analysis, we implement three optimization strategies:\n",
    "\n",
    "1. **Name Variant Handling**: Handle common name variants (dan/daniel, matt/matthew, etc.)\n",
    "2. **Adjusted Threshold**: Lower threshold from 0.7 to 0.65 to capture more true matches\n",
    "3. **Birth Year Soft Constraint**: Apply penalty when birth years differ by more than 1 year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b3287f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LR: Optimized Rule-Based Matching ===\n",
      "  Filtering candidate pairs by season_year constraint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.rule_based.RuleBasedMatcher - Starting Entity Matching\n",
      "[INFO ] PyDI.entitymatching.rule_based.RuleBasedMatcher - Blocking 106553 x 15215 elements\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  After season_year filter: 135,944 candidate pairs (from 5,994,373)\n",
      "  Computing base similarity scores using PyDI RuleBasedMatcher...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.rule_based.RuleBasedMatcher - Matching 106553 x 15215 elements after 0:00:0.224; 135944 blocked pairs (reduction ratio: 0.9999161462661056)\n",
      "[INFO ] PyDI.entitymatching.rule_based.RuleBasedMatcher - Entity Matching finished after 0:00:30.459; found 135944 correspondences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Base correspondences: 135,944\n",
      "  Applying name variant handling and birth year constraint...\n",
      "  Generated 14,828 matched pairs (above threshold 0.75)\n",
      "  Similarity score range: [0.761, 1.000]\n",
      "  Mean similarity: 0.948\n",
      "\n",
      "=== LS: Optimized Rule-Based Matching ===\n",
      "  Filtering candidate pairs by season_year constraint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.rule_based.RuleBasedMatcher - Starting Entity Matching\n",
      "[INFO ] PyDI.entitymatching.rule_based.RuleBasedMatcher - Blocking 106553 x 6743 elements\n",
      "[INFO ] PyDI.entitymatching.rule_based.RuleBasedMatcher - Matching 106553 x 6743 elements after 0:00:0.102; 9729 blocked pairs (reduction ratio: 0.9999864590429076)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  After season_year filter: 9,729 candidate pairs (from 227,185)\n",
      "  Computing base similarity scores using PyDI RuleBasedMatcher...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.rule_based.RuleBasedMatcher - Entity Matching finished after 0:00:2.336; found 9729 correspondences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Base correspondences: 9,729\n",
      "  Applying name variant handling and birth year constraint...\n",
      "  Generated 6,522 matched pairs (above threshold 0.75)\n",
      "  Similarity score range: [0.750, 1.000]\n",
      "  Mean similarity: 0.948\n",
      "\n",
      "✓ Optimized matching complete for all edges\n"
     ]
    }
   ],
   "source": [
    "# Apply optimized matching with name variants and birth year constraint\n",
    "\n",
    "optimized_matching_results = {}\n",
    "optimized_matchers = {}\n",
    "\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    print(f\"\\n=== {edge_name}: Optimized Rule-Based Matching ===\")\n",
    "    \n",
    "    left_df, right_df = source_tables[edge_name]\n",
    "    cand_df = candidates[edge_name].copy()\n",
    "    \n",
    "    # Apply season_year hard constraint: filter candidate pairs where season_year matches\n",
    "    print(f\"  Filtering candidate pairs by season_year constraint...\")\n",
    "    cand_with_seasons = cand_df.merge(\n",
    "        left_df[['_rid', 'season_year', 'birth_year']],\n",
    "        left_on='id1',\n",
    "        right_on='_rid',\n",
    "        how='left'\n",
    "    ).merge(\n",
    "        right_df[['_rid', 'season_year', 'birth_year']],\n",
    "        left_on='id2',\n",
    "        right_on='_rid',\n",
    "        how='left',\n",
    "        suffixes=('', '_right')\n",
    "    )\n",
    "    \n",
    "    # Filter: season_year must match exactly\n",
    "    cand_df_filtered = cand_with_seasons[\n",
    "        (cand_with_seasons['season_year'].notna()) & \n",
    "        (cand_with_seasons['season_year_right'].notna()) &\n",
    "        (cand_with_seasons['season_year'] == cand_with_seasons['season_year_right'])\n",
    "    ][['id1', 'id2', 'birth_year', 'birth_year_right']].copy()\n",
    "    \n",
    "    print(f\"  After season_year filter: {len(cand_df_filtered):,} candidate pairs (from {len(cand_df):,})\")\n",
    "    \n",
    "    # Initialize matcher\n",
    "    matcher = RuleBasedMatcher()\n",
    "    \n",
    "    # First, compute base similarity using PyDI (with original threshold to get all scores)\n",
    "    print(f\"  Computing base similarity scores using PyDI RuleBasedMatcher...\")\n",
    "    base_correspondences = matcher.match(\n",
    "        df_left=left_df,\n",
    "        df_right=right_df,\n",
    "        candidates=cand_df_filtered[['id1', 'id2']],\n",
    "        id_column='_rid',\n",
    "        comparators=rule_based_comparators,\n",
    "        weights=rule_based_weights,\n",
    "        threshold=0.0,  # Get all scores, we'll filter later\n",
    "        debug=False\n",
    "    )\n",
    "    \n",
    "    print(f\"  Base correspondences: {len(base_correspondences):,}\")\n",
    "    \n",
    "    # Apply enhanced similarity computation with name variants and birth year constraint\n",
    "    print(f\"  Applying name variant handling and birth year constraint...\")\n",
    "    \n",
    "    # Merge with source data to get names and birth years\n",
    "    enhanced_correspondences = base_correspondences.merge(\n",
    "        left_df[['_rid', 'full_name_normalized', 'full_name', 'birth_year']],\n",
    "        left_on='id1',\n",
    "        right_on='_rid',\n",
    "        how='left'\n",
    "    ).merge(\n",
    "        right_df[['_rid', 'full_name_normalized', 'full_name', 'birth_year']],\n",
    "        left_on='id2',\n",
    "        right_on='_rid',\n",
    "        how='left',\n",
    "        suffixes=('', '_right')\n",
    "    )\n",
    "    \n",
    "    # Convert birth_year columns to numeric once for downstream logic\n",
    "    enhanced_correspondences['birth_year'] = pd.to_numeric(enhanced_correspondences['birth_year'], errors='coerce')\n",
    "    enhanced_correspondences['birth_year_right'] = pd.to_numeric(enhanced_correspondences['birth_year_right'], errors='coerce')\n",
    "\n",
    "    # Apply name variant enhancement (Strategy 1)\n",
    "    def apply_name_variant_boost(row):\n",
    "        base_score = row['score']\n",
    "        name1 = str(row.get('full_name_normalized', row.get('full_name', ''))).lower()\n",
    "        name2 = str(row.get('full_name_normalized_right', row.get('full_name_right', ''))).lower()\n",
    "        first1, last1 = split_first_last(name1)\n",
    "        first2, last2 = split_first_last(name2)\n",
    "\n",
    "        if (\n",
    "            last1\n",
    "            and last1 == last2\n",
    "            and tokens_are_variants(first1, first2)\n",
    "            and birth_year_within(row.get('birth_year'), row.get('birth_year_right'))\n",
    "        ):\n",
    "            return max(base_score, 0.95)\n",
    "        \n",
    "        if check_name_variant_match(name1, name2):\n",
    "            # Variant match: boost score (but cap at 1.0)\n",
    "            return min(1.0, base_score + 0.15)\n",
    "        return base_score\n",
    "    \n",
    "    enhanced_correspondences['enhanced_score'] = enhanced_correspondences.apply(apply_name_variant_boost, axis=1)\n",
    "    \n",
    "    # Apply birth year constraint (Strategy 3)\n",
    "    \n",
    "    def apply_final_adjustments(row):\n",
    "        adjusted = apply_birth_year_constraint(\n",
    "            row['enhanced_score'],\n",
    "            row.get('birth_year'),\n",
    "            row.get('birth_year_right'),\n",
    "            penalty=0.2\n",
    "        )\n",
    "        name_equal = bool(\n",
    "            row.get('full_name_normalized')\n",
    "            and row.get('full_name_normalized_right')\n",
    "            and str(row['full_name_normalized']).lower() == str(row['full_name_normalized_right']).lower()\n",
    "        )\n",
    "        if name_equal:\n",
    "            gap = birth_year_diff(row.get('birth_year'), row.get('birth_year_right'))\n",
    "            if gap is not None and gap >= 2:\n",
    "                return adjusted * 0.2\n",
    "        return adjusted\n",
    "\n",
    "    enhanced_correspondences['final_score'] = enhanced_correspondences.apply(apply_final_adjustments, axis=1)\n",
    "    \n",
    "    # Keep only necessary columns\n",
    "    enhanced_correspondences = enhanced_correspondences[['id1', 'id2', 'final_score']].rename(columns={'final_score': 'score'})\n",
    "    \n",
    "    # Apply optimized threshold (Strategy 2: use edge-specific values)\n",
    "    edge_threshold = 0.75 if edge_name == 'LR' else 0.75\n",
    "    optimized_correspondences = enhanced_correspondences[enhanced_correspondences['score'] >= edge_threshold].copy()\n",
    "    \n",
    "    # Store results\n",
    "    optimized_matching_results[edge_name] = optimized_correspondences\n",
    "    optimized_matchers[edge_name] = matcher\n",
    "    \n",
    "    print(f\"  Generated {len(optimized_correspondences):,} matched pairs (above threshold {edge_threshold})\")\n",
    "    if len(optimized_correspondences) > 0:\n",
    "        print(f\"  Similarity score range: [{optimized_correspondences['score'].min():.3f}, {optimized_correspondences['score'].max():.3f}]\")\n",
    "        print(f\"  Mean similarity: {optimized_correspondences['score'].mean():.3f}\")\n",
    "    \n",
    "print(\"\\n✓ Optimized matching complete for all edges\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edaae30",
   "metadata": {},
   "source": [
    "### 4.6 Evaluate Optimized Matching\n",
    "Evaluate the performance of optimized matching on the validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f322d9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LR: Optimized Matching Evaluation (Validation Set) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Confusion Matrix:\n",
      "[INFO ] root -   True Positives:  28\n",
      "[INFO ] root -   True Negatives:  66\n",
      "[INFO ] root -   False Positives: 0\n",
      "[INFO ] root -   False Negatives: 2\n",
      "[INFO ] root - Performance Metrics:\n",
      "[INFO ] root -   Accuracy:  0.979\n",
      "[INFO ] root -   Precision: 1.000\n",
      "[INFO ] root -   Recall:    0.933\n",
      "[INFO ] root -   F1-Score:  0.966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Precision: 1.000\n",
      "  Recall:    0.933\n",
      "  F1-Score:  0.966\n",
      "  TP: 28\n",
      "  FP: 0\n",
      "  FN: 2\n",
      "\n",
      "=== LS: Optimized Matching Evaluation (Validation Set) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Confusion Matrix:\n",
      "[INFO ] root -   True Positives:  46\n",
      "[INFO ] root -   True Negatives:  47\n",
      "[INFO ] root -   False Positives: 0\n",
      "[INFO ] root -   False Negatives: 3\n",
      "[INFO ] root - Performance Metrics:\n",
      "[INFO ] root -   Accuracy:  0.969\n",
      "[INFO ] root -   Precision: 1.000\n",
      "[INFO ] root -   Recall:    0.939\n",
      "[INFO ] root -   F1-Score:  0.968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Precision: 1.000\n",
      "  Recall:    0.939\n",
      "  F1-Score:  0.968\n",
      "  TP: 46\n",
      "  FP: 0\n",
      "  FN: 3\n",
      "\n",
      "✓ Optimized matching evaluation complete\n"
     ]
    }
   ],
   "source": [
    "# Evaluate optimized matching on validation set\n",
    "\n",
    "from PyDI.entitymatching.evaluation import EntityMatchingEvaluator\n",
    "\n",
    "optimized_matching_metrics_val = {}\n",
    "\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    print(f\"\\n=== {edge_name}: Optimized Matching Evaluation (Validation Set) ===\")\n",
    "    \n",
    "    correspondences = optimized_matching_results[edge_name]\n",
    "    val_df = splits[edge_name]['val'][['id1', 'id2', 'label']].copy()\n",
    "    \n",
    "    # Rename 'score' to match PyDI evaluator expectations (if needed)\n",
    "    correspondences_for_eval = correspondences.copy()\n",
    "    if 'score' not in correspondences_for_eval.columns and 'sim' in correspondences_for_eval.columns:\n",
    "        correspondences_for_eval = correspondences_for_eval.rename(columns={'sim': 'score'})\n",
    "    \n",
    "    # Evaluate matching\n",
    "    try:\n",
    "        eval_results = EntityMatchingEvaluator.evaluate_matching(\n",
    "            correspondences=correspondences_for_eval,\n",
    "            test_pairs=val_df,\n",
    "            out_dir=OUTPUT_DIR / 'matching-evaluation-optimized',\n",
    "            matcher_instance=optimized_matchers[edge_name]\n",
    "        )\n",
    "        \n",
    "        optimized_matching_metrics_val[edge_name] = eval_results\n",
    "        \n",
    "        print(f\"\\n  Precision: {eval_results.get('precision', 0.0):.3f}\")\n",
    "        print(f\"  Recall:    {eval_results.get('recall', 0.0):.3f}\")\n",
    "        print(f\"  F1-Score:  {eval_results.get('f1', 0.0):.3f}\")\n",
    "        print(f\"  TP: {eval_results.get('true_positives', 0)}\")\n",
    "        print(f\"  FP: {eval_results.get('false_positives', 0)}\")\n",
    "        print(f\"  FN: {eval_results.get('false_negatives', 0)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  PyDI evaluator failed: {e}\")\n",
    "        # Manual evaluation fallback\n",
    "        true_matches = val_df[val_df['label'].astype(str).str.strip().str.upper() == 'TRUE']\n",
    "        true_set = set(zip(true_matches['id1'], true_matches['id2']))\n",
    "        pred_set = set(zip(correspondences['id1'], correspondences['id2']))\n",
    "        \n",
    "        tp = len(true_set & pred_set)\n",
    "        fp = len(pred_set - true_set)\n",
    "        fn = len(true_set - pred_set)\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        \n",
    "        print(f\"\\n  Precision: {precision:.3f}\")\n",
    "        print(f\"  Recall:    {recall:.3f}\")\n",
    "        print(f\"  F1-Score:  {f1:.3f}\")\n",
    "        print(f\"  TP: {tp}\")\n",
    "        print(f\"  FP: {fp}\")\n",
    "        print(f\"  FN: {fn}\")\n",
    "\n",
    "print(\"\\n✓ Optimized matching evaluation complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2d0e10",
   "metadata": {},
   "source": [
    "### 4.7 Comparison: Original vs Optimized Matching\n",
    "\n",
    "Compare the performance of original matching (threshold=0.7) with optimized matching (name variants + birth year constraint + threshold=0.75).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a50f06af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Matching Performance Comparison: Original vs Optimized\n",
      "================================================================================\n",
      "\n",
      "LR Edge:\n",
      "--------------------------------------------------------------------------------\n",
      "  Metric              | Original  | Optimized | Improvement\n",
      "  --------------------|-----------|-----------|------------\n",
      "  Precision           |   0.962  |   1.000  |  +0.038\n",
      "  Recall              |   0.833  |   0.933  |  +0.100\n",
      "  F1-Score            |   0.893  |   0.966  |  +0.073\n",
      "  True Positives      |      25  |      28  |      +3\n",
      "  False Positives     |       1  |       0  |      -1\n",
      "  False Negatives     |       5  |       2  |      -3\n",
      "\n",
      "LS Edge:\n",
      "--------------------------------------------------------------------------------\n",
      "  Metric              | Original  | Optimized | Improvement\n",
      "  --------------------|-----------|-----------|------------\n",
      "  Precision           |   0.953  |   1.000  |  +0.047\n",
      "  Recall              |   0.837  |   0.939  |  +0.102\n",
      "  F1-Score            |   0.891  |   0.968  |  +0.077\n",
      "  True Positives      |      41  |      46  |      +5\n",
      "  False Positives     |       2  |       0  |      -2\n",
      "  False Negatives     |       8  |       3  |      -5\n",
      "\n",
      "================================================================================\n",
      "Summary:\n",
      "================================================================================\n",
      "  Optimizations applied:\n",
      "    1. Name variant handling (dan/daniel, matt/matthew, etc.)\n",
      "    2. Lowered threshold from 0.7 to 0.65\n",
      "    3. Birth year soft constraint (penalty for year_diff > 1)\n",
      "\n",
      "  Comparison results saved to: /Users/zhangzihan/Desktop/WBI_project/Schema_Mapped_Datasets/data/output/matching/matching-comparison.csv\n"
     ]
    }
   ],
   "source": [
    "# Compare original vs optimized matching results\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Matching Performance Comparison: Original vs Optimized\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison_data = []\n",
    "\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    print(f\"\\n{edge_name} Edge:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Original results\n",
    "    orig_metrics = matching_metrics_val.get(edge_name, {})\n",
    "    orig_precision = orig_metrics.get('precision', 0.0)\n",
    "    orig_recall = orig_metrics.get('recall', 0.0)\n",
    "    orig_f1 = orig_metrics.get('f1', 0.0)\n",
    "    orig_tp = orig_metrics.get('true_positives', 0)\n",
    "    orig_fp = orig_metrics.get('false_positives', 0)\n",
    "    orig_fn = orig_metrics.get('false_negatives', 0)\n",
    "    \n",
    "    # Optimized results\n",
    "    opt_metrics = optimized_matching_metrics_val.get(edge_name, {})\n",
    "    opt_precision = opt_metrics.get('precision', 0.0)\n",
    "    opt_recall = opt_metrics.get('recall', 0.0)\n",
    "    opt_f1 = opt_metrics.get('f1', 0.0)\n",
    "    opt_tp = opt_metrics.get('true_positives', 0)\n",
    "    opt_fp = opt_metrics.get('false_positives', 0)\n",
    "    opt_fn = opt_metrics.get('false_negatives', 0)\n",
    "    \n",
    "    # Calculate improvements\n",
    "    precision_improvement = opt_precision - orig_precision\n",
    "    recall_improvement = opt_recall - orig_recall\n",
    "    f1_improvement = opt_f1 - orig_f1\n",
    "    tp_improvement = opt_tp - orig_tp\n",
    "    fp_improvement = opt_fp - orig_fp\n",
    "    fn_improvement = opt_fn - orig_fn\n",
    "    \n",
    "    print(f\"  Metric              | Original  | Optimized | Improvement\")\n",
    "    print(f\"  --------------------|-----------|-----------|------------\")\n",
    "    print(f\"  Precision           | {orig_precision:7.3f}  | {opt_precision:7.3f}  | {precision_improvement:+7.3f}\")\n",
    "    print(f\"  Recall              | {orig_recall:7.3f}  | {opt_recall:7.3f}  | {recall_improvement:+7.3f}\")\n",
    "    print(f\"  F1-Score            | {orig_f1:7.3f}  | {opt_f1:7.3f}  | {f1_improvement:+7.3f}\")\n",
    "    print(f\"  True Positives      | {orig_tp:7d}  | {opt_tp:7d}  | {tp_improvement:+7d}\")\n",
    "    print(f\"  False Positives     | {orig_fp:7d}  | {opt_fp:7d}  | {fp_improvement:+7d}\")\n",
    "    print(f\"  False Negatives     | {orig_fn:7d}  | {opt_fn:7d}  | {fn_improvement:+7d}\")\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'edge': edge_name,\n",
    "        'original_precision': orig_precision,\n",
    "        'optimized_precision': opt_precision,\n",
    "        'original_recall': orig_recall,\n",
    "        'optimized_recall': opt_recall,\n",
    "        'original_f1': orig_f1,\n",
    "        'optimized_f1': opt_f1,\n",
    "        'precision_improvement': precision_improvement,\n",
    "        'recall_improvement': recall_improvement,\n",
    "        'f1_improvement': f1_improvement,\n",
    "    })\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Summary:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"  Optimizations applied:\")\n",
    "print(f\"    1. Name variant handling (dan/daniel, matt/matthew, etc.)\")\n",
    "print(f\"    2. Lowered threshold from 0.7 to 0.65\")\n",
    "print(f\"    3. Birth year soft constraint (penalty for year_diff > 1)\")\n",
    "\n",
    "# Save comparison results\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df.to_csv(OUTPUT_DIR / 'matching-comparison.csv', index=False)\n",
    "print(f\"\\n  Comparison results saved to: {OUTPUT_DIR / 'matching-comparison.csv'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b796819",
   "metadata": {},
   "source": [
    "## 5. ML-Based Matching\n",
    "\n",
    "For use with scikit-learn classifiers. Comparators are the features. Train on labeled pairs to learn optimal weights.\n",
    "\n",
    "**Feature Extraction**: Feature extraction converts record pairs into feature vectors using the set of comparators. The `FeatureExtractor` class handles this transformation.\n",
    "\n",
    "**Workflow**:\n",
    "1. Define feature extractors (comparators)\n",
    "2. Extract features from training pairs\n",
    "3. Train a classifier (e.g., RandomForestClassifier)\n",
    "4. Apply ML-based matcher to candidate pairs\n",
    "5. Evaluate performance\n",
    "6. Error cases analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8fce1e",
   "metadata": {},
   "source": [
    "### 5.0 Export Candidate Error Cases\n",
    "\n",
    "Persist currently detected validation errors to `data/output/gt/manual_cases/` so the ground-truth notebook can ingest them (section 5.8). Run this after completing the rule-based/optimized evaluations and before the ML feature extractor setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f3b8dcaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LR] Exported 17 cases to /Users/zhangzihan/Desktop/WBI_project/Schema_Mapped_Datasets/data/output/gt/manual_cases/manual_cases_LR.csv\n",
      "[LS] Exported 21 cases to /Users/zhangzihan/Desktop/WBI_project/Schema_Mapped_Datasets/data/output/gt/manual_cases/manual_cases_LS.csv\n"
     ]
    }
   ],
   "source": [
    "# Export candidate error cases for GT augmentation (section 5.8 in GT notebook)\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "MANUAL_CASES_DIR = BASE_DIR / 'data' / 'output' / 'gt' / 'manual_cases'\n",
    "MANUAL_CASES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "error_records = {'LR': [], 'LS': []}\n",
    "\n",
    "def _collect_errors(result_dict, label: str):\n",
    "    if result_dict is None:\n",
    "        return\n",
    "    for edge_name in ['LR', 'LS']:\n",
    "        if edge_name not in result_dict:\n",
    "            continue\n",
    "        correspondences = result_dict[edge_name]\n",
    "        val_df = splits[edge_name]['val'][['id1', 'id2', 'label']].copy()\n",
    "        val_df['label'] = val_df['label'].astype(str).str.strip().str.upper()\n",
    "        true_set = set(zip(val_df[val_df['label'] == 'TRUE']['id1'], val_df[val_df['label'] == 'TRUE']['id2']))\n",
    "        false_set = set(zip(val_df[val_df['label'] == 'FALSE']['id1'], val_df[val_df['label'] == 'FALSE']['id2']))\n",
    "        pred_set = set(zip(correspondences['id1'], correspondences['id2']))\n",
    "        val_pairs = set(zip(val_df['id1'], val_df['id2']))\n",
    "\n",
    "        fn_pairs = true_set - pred_set\n",
    "        for id1, id2 in fn_pairs:\n",
    "            error_records[edge_name].append({\n",
    "                'id1': id1,\n",
    "                'id2': id2,\n",
    "                'label': 'TRUE',\n",
    "                'edge': edge_name,\n",
    "                'source': label,\n",
    "                'error_type': 'FN'\n",
    "            })\n",
    "\n",
    "        fp_pairs = (pred_set & val_pairs) & false_set\n",
    "        for id1, id2 in fp_pairs:\n",
    "            error_records[edge_name].append({\n",
    "                'id1': id1,\n",
    "                'id2': id2,\n",
    "                'label': 'FALSE',\n",
    "                'edge': edge_name,\n",
    "                'source': label,\n",
    "                'error_type': 'FP'\n",
    "            })\n",
    "\n",
    "_collect_errors(globals().get('matching_results'), 'rule_based')\n",
    "_collect_errors(globals().get('optimized_matching_results'), 'optimized')\n",
    "_collect_errors(globals().get('logreg_matching_results'), 'logreg')\n",
    "_collect_errors(globals().get('ml_matching_results'), 'random_forest')\n",
    "_collect_errors(globals().get('gb_matching_results'), 'gradient_boosting')\n",
    "_collect_errors(globals().get('xgb_matching_results'), 'xgboost')\n",
    "\n",
    "for edge_name, records in error_records.items():\n",
    "    if not records:\n",
    "        continue\n",
    "    df_edge = pd.DataFrame(records)\n",
    "    manual_path = MANUAL_CASES_DIR / f'manual_cases_{edge_name}.csv'\n",
    "    if manual_path.exists():\n",
    "        existing = pd.read_csv(manual_path)\n",
    "        # Align columns\n",
    "        for col in df_edge.columns:\n",
    "            if col not in existing.columns:\n",
    "                existing[col] = ''\n",
    "        for col in existing.columns:\n",
    "            if col not in df_edge.columns:\n",
    "                df_edge[col] = ''\n",
    "        df_edge = pd.concat([existing, df_edge], ignore_index=True)\n",
    "    df_edge = df_edge.drop_duplicates(subset=['id1', 'id2'])\n",
    "    df_edge.to_csv(manual_path, index=False)\n",
    "    print(f\"[{edge_name}] Exported {len(df_edge)} cases to {manual_path}\")\n",
    "\n",
    "if not any(error_records.values()):\n",
    "    print(\"No error cases collected (ensure evaluation cells were executed).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a004cd2",
   "metadata": {},
   "source": [
    "### 5.1 Define Feature Extractors (Comparators)\n",
    "\n",
    "Define feature extractors for ML training. Enhanced with birth year comparator and additional custom features to improve matching accuracy:\n",
    "- **Name similarity**: Levenshtein distance and Jaccard similarity on normalized names\n",
    "- **Birth year**: DateComparator to penalize pairs with different birth years (max_days_difference=365)\n",
    "- **Birth year difference**: Numeric feature calculating absolute difference between birth years\n",
    "- **Name variant flag**: Binary feature indicating if first names are known variants (e.g., dan/daniel, matt/matthew)\n",
    "- **Phonetic match**: Soundex-based phonetic similarity to capture pronunciation variants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "56646c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML-based matching feature extractors configured:\n",
      "  Feature functions: 6\n",
      "    - Levenshtein distance on normalized name\n",
      "    - Jaccard similarity on normalized name (word tokenization)\n",
      "    - Birth year comparator (max_days_difference=365)\n",
      "    - Birth year difference (numeric feature)\n",
      "    - Name variant flag (last name match + first-name variants)\n",
      "    - Soundex phonetic match (captures pronunciation variants)\n",
      "✓ Feature extractor initialized\n"
     ]
    }
   ],
   "source": [
    "# Define feature extractors (comparators) for ML-based matching\n",
    "# Enhanced with birth year comparator to improve matching accuracy\n",
    "\n",
    "from PyDI.entitymatching import MLBasedMatcher, FeatureExtractor, StringComparator\n",
    "from PyDI.entitymatching.comparators import DateComparator\n",
    "from PyDI.io import load_csv\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from PyDI.entitymatching import VectorFeatureExtractor\n",
    "\n",
    "\n",
    "def feature_birth_year_diff(record1, record2):\n",
    "    diff = birth_year_diff(record1.get('birth_year'), record2.get('birth_year'))\n",
    "    return float(diff if diff is not None else 0.0)\n",
    "\n",
    "\n",
    "def feature_name_variant_flag(record1, record2):\n",
    "    left_name = str(record1.get('full_name_normalized', record1.get('full_name', ''))).lower()\n",
    "    right_name = str(record2.get('full_name_normalized', record2.get('full_name', ''))).lower()\n",
    "    first_left, last_left = split_first_last(left_name)\n",
    "    first_right, last_right = split_first_last(right_name)\n",
    "    if last_left and last_left == last_right and tokens_are_variants(first_left, first_right):\n",
    "        return 1.0\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def _soundex(name: str) -> str:\n",
    "    \"\"\"Return a simple Soundex code to capture pronunciation.\"\"\"\n",
    "    if not name:\n",
    "        return \"\"\n",
    "    name = name.upper()\n",
    "    first_letter = name[0]\n",
    "    mapping = {\n",
    "        **{c: \"1\" for c in \"BFPV\"},\n",
    "        **{c: \"2\" for c in \"CGJKQSXZ\"},\n",
    "        **{c: \"3\" for c in \"DT\"},\n",
    "        \"L\": \"4\",\n",
    "        **{c: \"5\" for c in \"MN\"},\n",
    "        \"R\": \"6\",\n",
    "    }\n",
    "    prev_digit = mapping.get(first_letter, \"\")\n",
    "    digits = []\n",
    "    for ch in name[1:]:\n",
    "        digit = mapping.get(ch, \"\")\n",
    "        if digit and digit != prev_digit:\n",
    "            digits.append(digit)\n",
    "        prev_digit = digit\n",
    "    code = (first_letter + \"\".join(digits) + \"000\")[:4]\n",
    "    return code\n",
    "\n",
    "\n",
    "def feature_phonetic_match(record1, record2) -> float:\n",
    "    left_name = str(record1.get('full_name_normalized', record1.get('full_name', '')))\n",
    "    right_name = str(record2.get('full_name_normalized', record2.get('full_name', '')))\n",
    "    left_code = _soundex(left_name)\n",
    "    right_code = _soundex(right_name)\n",
    "    if not left_code or not right_code:\n",
    "        return 0.0\n",
    "    return 1.0 if left_code == right_code else 0.0\n",
    "\n",
    "\n",
    "ml_comparators = [\n",
    "    StringComparator(\n",
    "        column=\"full_name_normalized\" if 'full_name_normalized' in L_full.columns else \"full_name\",\n",
    "        similarity_function=\"levenshtein\",\n",
    "        preprocess=str.lower\n",
    "    ),\n",
    "    StringComparator(\n",
    "        column=\"full_name_normalized\" if 'full_name_normalized' in L_full.columns else \"full_name\",\n",
    "        similarity_function=\"jaccard\",\n",
    "        tokenization=\"word\",\n",
    "        preprocess=str.lower\n",
    "    ),\n",
    "    # Enhanced: Add birth year comparator to penalize pairs with different birth years\n",
    "    DateComparator(\n",
    "        column=\"birth_year\",\n",
    "        max_days_difference=365  # Allow 1 year difference (365 days)\n",
    "    ),\n",
    "    {\"function\": feature_birth_year_diff, \"name\": \"birth_year_diff\"},\n",
    "    {\"function\": feature_name_variant_flag, \"name\": \"is_name_variant\"},\n",
    "    {\"function\": feature_phonetic_match, \"name\": \"phonetic_soundex\"}\n",
    "]\n",
    "\n",
    "# Initialize feature extractor\n",
    "ml_feature_extractor = FeatureExtractor(ml_comparators)\n",
    "\n",
    "print(\"ML-based matching feature extractors configured:\")\n",
    "print(f\"  Feature functions: {len(ml_comparators)}\")\n",
    "print(f\"    - Levenshtein distance on normalized name\")\n",
    "print(f\"    - Jaccard similarity on normalized name (word tokenization)\")\n",
    "print(f\"    - Birth year comparator (max_days_difference=365)\")\n",
    "print(f\"    - Birth year difference (numeric feature)\")\n",
    "print(f\"    - Name variant flag (last name match + first-name variants)\")\n",
    "print(f\"    - Soundex phonetic match (captures pronunciation variants)\")\n",
    "print(\"✓ Feature extractor initialized\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e15b00",
   "metadata": {},
   "source": [
    "### 5.2 Train and Apply LogisticRegression Matcher\n",
    "\n",
    "Train a baseline LogisticRegression model using the shared feature extractor. This provides a lightweight reference point before the tree-based ensembles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a9329e",
   "metadata": {},
   "source": [
    "#### 5.2.1 Train LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "49b58dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LR: Training LogisticRegression Matcher ===\n",
      "  Training pairs: 304\n",
      "    True matches: 90\n",
      "    False matches: 214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Label distribution: 90 positive, 214 negative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training features shape: (304, 6)\n",
      "  Feature columns: ['StringComparator(full_name_normalized, levenshtein, tokenization=char, list_strategy=None)', 'StringComparator(full_name_normalized, jaccard, tokenization=word, list_strategy=None)', 'DateComparator(birth_year, list_strategy=None)', 'birth_year_diff', 'is_name_variant', 'phonetic_soundex']\n",
      "\n",
      "  Top coefficients (magnitude):\n",
      "                                                                                   feature  coefficient\n",
      "                                                                           is_name_variant     3.174990\n",
      "StringComparator(full_name_normalized, levenshtein, tokenization=char, list_strategy=None)     2.999598\n",
      "                                                                           birth_year_diff    -1.675443\n",
      "                                            DateComparator(birth_year, list_strategy=None)    -1.600098\n",
      "                                                                          phonetic_soundex    -0.281519\n",
      "    StringComparator(full_name_normalized, jaccard, tokenization=word, list_strategy=None)     0.013693\n",
      "  Intercept: -0.4116\n",
      "  ✓ LogisticRegression trained for LR\n",
      "\n",
      "=== LS: Training LogisticRegression Matcher ===\n",
      "  Training pairs: 296\n",
      "    True matches: 142\n",
      "    False matches: 154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Label distribution: 142 positive, 154 negative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training features shape: (296, 6)\n",
      "  Feature columns: ['StringComparator(full_name_normalized, levenshtein, tokenization=char, list_strategy=None)', 'StringComparator(full_name_normalized, jaccard, tokenization=word, list_strategy=None)', 'DateComparator(birth_year, list_strategy=None)', 'birth_year_diff', 'is_name_variant', 'phonetic_soundex']\n",
      "\n",
      "  Top coefficients (magnitude):\n",
      "                                                                                   feature  coefficient\n",
      "                                                                           is_name_variant     3.381306\n",
      "StringComparator(full_name_normalized, levenshtein, tokenization=char, list_strategy=None)     2.518544\n",
      "                                            DateComparator(birth_year, list_strategy=None)    -1.884224\n",
      "                                                                           birth_year_diff    -1.421372\n",
      "    StringComparator(full_name_normalized, jaccard, tokenization=word, list_strategy=None)    -0.178423\n",
      "                                                                          phonetic_soundex     0.078866\n",
      "  Intercept: -0.5133\n",
      "  ✓ LogisticRegression trained for LS\n",
      "\n",
      "✓ LogisticRegression matchers trained for all edges\n"
     ]
    }
   ],
   "source": [
    "# Train LogisticRegression-based matchers for each edge\n",
    "logreg_classifiers = {}\n",
    "logreg_matchers = {}\n",
    "\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    print(f\"\\n=== {edge_name}: Training LogisticRegression Matcher ===\")\n",
    "    \n",
    "    left_df, right_df = source_tables[edge_name]\n",
    "    train_df = splits[edge_name]['train'][['id1', 'id2', 'label']].copy()\n",
    "    \n",
    "    # Normalize labels\n",
    "    train_df['label'] = train_df['label'].astype(str).str.strip().str.upper()\n",
    "    train_df['label_binary'] = (train_df['label'] == 'TRUE').astype(int)\n",
    "    \n",
    "    print(f\"  Training pairs: {len(train_df)}\")\n",
    "    print(f\"    True matches: {train_df['label_binary'].sum()}\")\n",
    "    print(f\"    False matches: {len(train_df) - train_df['label_binary'].sum()}\")\n",
    "    \n",
    "    # Extract shared features\n",
    "    train_features = ml_feature_extractor.create_features(\n",
    "        df_left=left_df,\n",
    "        df_right=right_df,\n",
    "        pairs=train_df[['id1', 'id2']],\n",
    "        labels=train_df['label_binary'],\n",
    "        id_column='_rid'\n",
    "    )\n",
    "    \n",
    "    feature_columns = [col for col in train_features.columns if col not in ['id1', 'id2', 'label']]\n",
    "    X_train = train_features[feature_columns]\n",
    "    y_train = train_features['label']\n",
    "    \n",
    "    print(f\"  Training features shape: {X_train.shape}\")\n",
    "    print(f\"  Feature columns: {feature_columns}\")\n",
    "    \n",
    "    # Configure Logistic Regression\n",
    "    clf = LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        class_weight='balanced',\n",
    "        solver='liblinear'\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    logreg_classifiers[edge_name] = clf\n",
    "    logreg_matchers[edge_name] = MLBasedMatcher(ml_feature_extractor)\n",
    "    \n",
    "    # Inspect coefficients for interpretability\n",
    "    coef_df = pd.DataFrame({\n",
    "        'feature': feature_columns,\n",
    "        'coefficient': clf.coef_[0]\n",
    "    }).sort_values('coefficient', key=lambda s: s.abs(), ascending=False)\n",
    "    print(\"\\n  Top coefficients (magnitude):\")\n",
    "    print(coef_df.to_string(index=False))\n",
    "    print(f\"  Intercept: {clf.intercept_[0]:.4f}\")\n",
    "    print(f\"  ✓ LogisticRegression trained for {edge_name}\")\n",
    "\n",
    "print(\"\\n✓ LogisticRegression matchers trained for all edges\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f687dec",
   "metadata": {},
   "source": [
    "#### 5.2.2 Apply LogisticRegression Matcher\n",
    "\n",
    "Reuse the shared matching pipeline to score candidate pairs with the LogisticRegression classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3788e984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LR: LogisticRegression Matching ===\n",
      "  Filtering candidate pairs by season_year constraint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Starting Entity Matching\n",
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Blocking 106553 x 15215 elements\n",
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Matching 106553 x 15215 elements after 0:00:0.000; 135944 blocked pairs (reduction ratio: 0.9999161462661056)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  After season_year filter: 135,944 candidate pairs (from 5,994,373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Entity Matching finished after 0:00:49.957; found 23177 correspondences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 23,177 matched pairs\n",
      "  Score range: [0.402, 0.980]\n",
      "  Mean score: 0.790\n",
      "\n",
      "=== LS: LogisticRegression Matching ===\n",
      "  Filtering candidate pairs by season_year constraint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Starting Entity Matching\n",
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Blocking 106553 x 6743 elements\n",
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Matching 106553 x 6743 elements after 0:00:0.000; 9729 blocked pairs (reduction ratio: 0.9999864590429076)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  After season_year filter: 9,729 candidate pairs (from 227,185)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Entity Matching finished after 0:00:3.250; found 6853 correspondences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 6,853 matched pairs\n",
      "  Score range: [0.400, 0.979]\n",
      "  Mean score: 0.951\n",
      "\n",
      "✓ LogisticRegression matching complete for all edges\n"
     ]
    }
   ],
   "source": [
    "# Apply LogisticRegression-based matcher to candidate pairs\n",
    "logreg_matching_results = {}\n",
    "\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    print(f\"\\n=== {edge_name}: LogisticRegression Matching ===\")\n",
    "    \n",
    "    left_df, right_df = source_tables[edge_name]\n",
    "    cand_df = candidates[edge_name].copy()\n",
    "    matcher = logreg_matchers[edge_name]\n",
    "    clf = logreg_classifiers[edge_name]\n",
    "    \n",
    "    # Apply season_year hard constraint\n",
    "    print(\"  Filtering candidate pairs by season_year constraint...\")\n",
    "    cand_with_seasons = cand_df.merge(\n",
    "        left_df[['_rid', 'season_year']],\n",
    "        left_on='id1',\n",
    "        right_on='_rid',\n",
    "        how='left'\n",
    "    ).merge(\n",
    "        right_df[['_rid', 'season_year']],\n",
    "        left_on='id2',\n",
    "        right_on='_rid',\n",
    "        how='left',\n",
    "        suffixes=('', '_right')\n",
    "    )\n",
    "    \n",
    "    cand_df_filtered = cand_with_seasons[\n",
    "        (cand_with_seasons['season_year'].notna()) &\n",
    "        (cand_with_seasons['season_year_right'].notna()) &\n",
    "        (cand_with_seasons['season_year'] == cand_with_seasons['season_year_right'])\n",
    "    ][['id1', 'id2']].copy()\n",
    "    \n",
    "    print(f\"  After season_year filter: {len(cand_df_filtered):,} candidate pairs (from {len(cand_df):,})\")\n",
    "    \n",
    "    correspondences = matcher.match(\n",
    "        df_left=left_df,\n",
    "        df_right=right_df,\n",
    "        candidates=cand_df_filtered,\n",
    "        id_column='_rid',\n",
    "        use_probabilities=True,\n",
    "        trained_classifier=clf,\n",
    "        threshold=0.4\n",
    "    )\n",
    "    \n",
    "    logreg_matching_results[edge_name] = correspondences\n",
    "    \n",
    "    print(f\"  Generated {len(correspondences):,} matched pairs\")\n",
    "    if 'score' in correspondences.columns:\n",
    "        print(f\"  Score range: [{correspondences['score'].min():.3f}, {correspondences['score'].max():.3f}]\")\n",
    "        print(f\"  Mean score: {correspondences['score'].mean():.3f}\")\n",
    "\n",
    "print(\"\\n✓ LogisticRegression matching complete for all edges\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551b1c1f",
   "metadata": {},
   "source": [
    "#### 5.2.3 Evaluate LogisticRegression Matching\n",
    "\n",
    "Assess LogisticRegression performance on the validation set using the PyDI evaluator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "88945a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LR: LogisticRegression Evaluation (Validation Set) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Confusion Matrix:\n",
      "[INFO ] root -   True Positives:  30\n",
      "[INFO ] root -   True Negatives:  63\n",
      "[INFO ] root -   False Positives: 3\n",
      "[INFO ] root -   False Negatives: 0\n",
      "[INFO ] root - Performance Metrics:\n",
      "[INFO ] root -   Accuracy:  0.969\n",
      "[INFO ] root -   Precision: 0.909\n",
      "[INFO ] root -   Recall:    1.000\n",
      "[INFO ] root -   F1-Score:  0.952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Precision: 0.909\n",
      "  Recall:    1.000\n",
      "  F1-Score:  0.952\n",
      "  TP: 30\n",
      "  FP: 3\n",
      "  FN: 0\n",
      "\n",
      "=== LS: LogisticRegression Evaluation (Validation Set) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Confusion Matrix:\n",
      "[INFO ] root -   True Positives:  49\n",
      "[INFO ] root -   True Negatives:  45\n",
      "[INFO ] root -   False Positives: 2\n",
      "[INFO ] root -   False Negatives: 0\n",
      "[INFO ] root - Performance Metrics:\n",
      "[INFO ] root -   Accuracy:  0.979\n",
      "[INFO ] root -   Precision: 0.961\n",
      "[INFO ] root -   Recall:    1.000\n",
      "[INFO ] root -   F1-Score:  0.980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Precision: 0.961\n",
      "  Recall:    1.000\n",
      "  F1-Score:  0.980\n",
      "  TP: 49\n",
      "  FP: 2\n",
      "  FN: 0\n",
      "\n",
      "✓ LogisticRegression evaluation complete\n"
     ]
    }
   ],
   "source": [
    "# Evaluate LogisticRegression matching on validation set\n",
    "logreg_matching_metrics_val = {}\n",
    "\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    print(f\"\\n=== {edge_name}: LogisticRegression Evaluation (Validation Set) ===\")\n",
    "    \n",
    "    correspondences = logreg_matching_results[edge_name]\n",
    "    val_df = splits[edge_name]['val'][['id1', 'id2', 'label']].copy()\n",
    "    \n",
    "    correspondences_for_eval = correspondences.copy()\n",
    "    if 'score' not in correspondences_for_eval.columns and 'sim' in correspondences_for_eval.columns:\n",
    "        correspondences_for_eval = correspondences_for_eval.rename(columns={'sim': 'score'})\n",
    "    \n",
    "    try:\n",
    "        eval_results = EntityMatchingEvaluator.evaluate_matching(\n",
    "            correspondences=correspondences_for_eval,\n",
    "            test_pairs=val_df,\n",
    "            out_dir=OUTPUT_DIR / 'matching-evaluation-logreg',\n",
    "            matcher_instance=logreg_matchers[edge_name]\n",
    "        )\n",
    "        logreg_matching_metrics_val[edge_name] = eval_results\n",
    "        \n",
    "        print(f\"\\n  Precision: {eval_results.get('precision', 0.0):.3f}\")\n",
    "        print(f\"  Recall:    {eval_results.get('recall', 0.0):.3f}\")\n",
    "        print(f\"  F1-Score:  {eval_results.get('f1', 0.0):.3f}\")\n",
    "        print(f\"  TP: {eval_results.get('true_positives', 0)}\")\n",
    "        print(f\"  FP: {eval_results.get('false_positives', 0)}\")\n",
    "        print(f\"  FN: {eval_results.get('false_negatives', 0)}\")\n",
    "    except Exception as exc:\n",
    "        print(f\"  PyDI evaluator failed: {exc}\")\n",
    "        true_matches = val_df[val_df['label'].astype(str).str.strip().str.upper() == 'TRUE']\n",
    "        true_set = set(zip(true_matches['id1'], true_matches['id2']))\n",
    "        pred_set = set(zip(correspondences['id1'], correspondences['id2']))\n",
    "        \n",
    "        tp = len(true_set & pred_set)\n",
    "        fp = len(pred_set - true_set)\n",
    "        fn = len(true_set - pred_set)\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        \n",
    "        logreg_matching_metrics_val[edge_name] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'true_positives': tp,\n",
    "            'false_positives': fp,\n",
    "            'false_negatives': fn\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n  Precision: {precision:.3f}\")\n",
    "        print(f\"  Recall:    {recall:.3f}\")\n",
    "        print(f\"  F1-Score:  {f1:.3f}\")\n",
    "        print(f\"  TP: {tp}\")\n",
    "        print(f\"  FP: {fp}\")\n",
    "        print(f\"  FN: {fn}\")\n",
    "\n",
    "print(\"\\n✓ LogisticRegression evaluation complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd8dcdc",
   "metadata": {},
   "source": [
    "#### 5.2.4 Analyze LogisticRegression Error Cases\n",
    "\n",
    "Investigate False Positives/Negatives for the LogisticRegression matcher using the same diagnostic template.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "39ee7fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LR: LogisticRegression Error Cases Analysis\n",
      "================================================================================\n",
      "\n",
      "[FALSE NEGATIVES] (0 cases):\n",
      "  No false negatives found!\n",
      "\n",
      "[FALSE POSITIVES] (3 cases):\n",
      "\n",
      "  4082302010|2010|L <-> 1502682010|2010|R (score: 0.410)\n",
      "    Left:  'pedro feliciano' | Season: 2010 | Birth: 1976\n",
      "    Right: 'pedro feliz' | Season: 2010 | Birth: 1975\n",
      "\n",
      "  5169492015|2015|L <-> 5023272015|2015|R (score: 0.410)\n",
      "    Left:  'hector sanchez' | Season: 2015 | Birth: 1989\n",
      "    Right: 'h\\xc3\\xa9ctor santiago' | Season: 2015 | Birth: 1988\n",
      "\n",
      "  4585892010|2010|L <-> 4566962010|2010|R (score: 0.478)\n",
      "    Left:  'david herndon' | Season: 2010 | Birth: 1985\n",
      "    Right: 'david hernandez' | Season: 2010 | Birth: 1985\n",
      "\n",
      "================================================================================\n",
      "LS: LogisticRegression Error Cases Analysis\n",
      "================================================================================\n",
      "\n",
      "[FALSE NEGATIVES] (0 cases):\n",
      "  No false negatives found!\n",
      "\n",
      "[FALSE POSITIVES] (2 cases):\n",
      "\n",
      "  5167142015|2015|L <-> 6458482015|2015|S (score: 0.426)\n",
      "    Left:  'dario alvarez' | Season: 2015 | Birth: 1989\n",
      "    Right: 'dariel alvarez' | Season: 2015 | Birth: 1989\n",
      "\n",
      "  6703512022|2022|L <-> 6689422022|2022|S (score: 0.587)\n",
      "    Left:  'jose rojas' | Season: 2022 | Birth: 1993\n",
      "    Right: 'josh rojas' | Season: 2022 | Birth: 1994\n",
      "\n",
      "✓ LogisticRegression error analysis complete\n"
     ]
    }
   ],
   "source": [
    "# Analyze error cases for LogisticRegression\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{edge_name}: LogisticRegression Error Cases Analysis\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    left_df, right_df = source_tables[edge_name]\n",
    "    val_df = splits[edge_name]['val'][['id1', 'id2', 'label']].copy()\n",
    "    correspondences = logreg_matching_results[edge_name]\n",
    "    \n",
    "    true_matches = val_df[val_df['label'].astype(str).str.strip().str.upper() == 'TRUE']\n",
    "    false_matches = val_df[val_df['label'].astype(str).str.strip().str.upper() == 'FALSE']\n",
    "    true_set = set(zip(true_matches['id1'], true_matches['id2']))\n",
    "    false_set = set(zip(false_matches['id1'], false_matches['id2']))\n",
    "    pred_set = set(zip(correspondences['id1'], correspondences['id2']))\n",
    "    val_set = set(zip(val_df['id1'], val_df['id2']))\n",
    "    \n",
    "    # False negatives\n",
    "    fn_pairs = true_set - pred_set\n",
    "    print(f\"\\n[FALSE NEGATIVES] ({len(fn_pairs)} cases):\")\n",
    "    if fn_pairs:\n",
    "        for id1, id2 in fn_pairs:\n",
    "            left_rec = left_df[left_df['_rid'] == id1].iloc[0] if len(left_df[left_df['_rid'] == id1]) > 0 else None\n",
    "            right_rec = right_df[right_df['_rid'] == id2].iloc[0] if len(right_df[right_df['_rid'] == id2]) > 0 else None\n",
    "            if left_rec is not None and right_rec is not None:\n",
    "                print(f\"\\n  {id1} <-> {id2}\")\n",
    "                print(f\"    Left:  '{left_rec.get('full_name', 'N/A')}' | Season: {left_rec.get('season_year', 'N/A')} | Birth: {left_rec.get('birth_year', 'N/A')}\")\n",
    "                print(f\"    Right: '{right_rec.get('full_name', 'N/A')}' | Season: {right_rec.get('season_year', 'N/A')} | Birth: {right_rec.get('birth_year', 'N/A')}\")\n",
    "                in_candidates = len(candidates[edge_name][\n",
    "                    (candidates[edge_name]['id1'] == id1) &\n",
    "                    (candidates[edge_name]['id2'] == id2)\n",
    "                ]) > 0\n",
    "                print(f\"    In candidates: {in_candidates}\")\n",
    "                score_row = correspondences[(correspondences['id1'] == id1) & (correspondences['id2'] == id2)]\n",
    "                if len(score_row) > 0 and 'score' in score_row.columns:\n",
    "                    print(f\"    LogReg Score: {score_row['score'].iloc[0]:.3f}\")\n",
    "    else:\n",
    "        print(\"  No false negatives found!\")\n",
    "    \n",
    "    # False positives\n",
    "    fp_pairs = (pred_set & val_set) & false_set\n",
    "    print(f\"\\n[FALSE POSITIVES] ({len(fp_pairs)} cases):\")\n",
    "    if fp_pairs:\n",
    "        for id1, id2 in fp_pairs:\n",
    "            left_rec = left_df[left_df['_rid'] == id1].iloc[0] if len(left_df[left_df['_rid'] == id1]) > 0 else None\n",
    "            right_rec = right_df[right_df['_rid'] == id2].iloc[0] if len(right_df[right_df['_rid'] == id2]) > 0 else None\n",
    "            score_row = correspondences[(correspondences['id1'] == id1) & (correspondences['id2'] == id2)]\n",
    "            score = score_row['score'].iloc[0] if len(score_row) > 0 and 'score' in score_row.columns else None\n",
    "            if left_rec is not None and right_rec is not None:\n",
    "                score_str = f\"{score:.3f}\" if score is not None else \"N/A\"\n",
    "                print(f\"\\n  {id1} <-> {id2} (score: {score_str})\")\n",
    "                print(f\"    Left:  '{left_rec.get('full_name', 'N/A')}' | Season: {left_rec.get('season_year', 'N/A')} | Birth: {left_rec.get('birth_year', 'N/A')}\")\n",
    "                print(f\"    Right: '{right_rec.get('full_name', 'N/A')}' | Season: {right_rec.get('season_year', 'N/A')} | Birth: {right_rec.get('birth_year', 'N/A')}\")\n",
    "    else:\n",
    "        print(\"  No false positives found!\")\n",
    "\n",
    "print(\"\\n✓ LogisticRegression error analysis complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f3c7c0",
   "metadata": {},
   "source": [
    "### 5.3 Train and Apply RandomForestClassifier Matcher\n",
    "\n",
    "Extract features from training pairs and train a RandomForestClassifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e446985a",
   "metadata": {},
   "source": [
    "#### 5.3.1 Train RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f6440750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LR: Training ML-Based Matcher ===\n",
      "  Training pairs: 304\n",
      "    True matches: 90\n",
      "    False matches: 214\n",
      "  Extracting features from training pairs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Label distribution: 90 positive, 214 negative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Extracted features: 304 pairs\n",
      "  Feature columns: 6\n",
      "  Training features shape: (304, 6)\n",
      "  Training labels distribution: {0: 214, 1: 90}\n",
      "  Training RandomForestClassifier...\n",
      "\n",
      "  Top 5 Feature Importances:\n",
      "    is_name_variant: 0.3784\n",
      "    StringComparator(full_name_normalized, levenshtein, tokenization=char, list_strategy=None): 0.3433\n",
      "    birth_year_diff: 0.1693\n",
      "    StringComparator(full_name_normalized, jaccard, tokenization=word, list_strategy=None): 0.0890\n",
      "    DateComparator(birth_year, list_strategy=None): 0.0102\n",
      "  ✓ Classifier trained for LR\n",
      "\n",
      "=== LS: Training ML-Based Matcher ===\n",
      "  Training pairs: 296\n",
      "    True matches: 142\n",
      "    False matches: 154\n",
      "  Extracting features from training pairs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Label distribution: 142 positive, 154 negative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Extracted features: 296 pairs\n",
      "  Feature columns: 6\n",
      "  Training features shape: (296, 6)\n",
      "  Training labels distribution: {0: 154, 1: 142}\n",
      "  Training RandomForestClassifier...\n",
      "\n",
      "  Top 5 Feature Importances:\n",
      "    StringComparator(full_name_normalized, levenshtein, tokenization=char, list_strategy=None): 0.3548\n",
      "    is_name_variant: 0.3343\n",
      "    birth_year_diff: 0.1563\n",
      "    StringComparator(full_name_normalized, jaccard, tokenization=word, list_strategy=None): 0.1235\n",
      "    phonetic_soundex: 0.0251\n",
      "  ✓ Classifier trained for LS\n",
      "\n",
      "✓ ML-based matchers trained for all edges\n"
     ]
    }
   ],
   "source": [
    "# Train ML-based matchers for each edge\n",
    "ml_classifiers = {}\n",
    "ml_matchers = {}\n",
    "\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    print(f\"\\n=== {edge_name}: Training ML-Based Matcher ===\")\n",
    "    \n",
    "    left_df, right_df = source_tables[edge_name]\n",
    "    train_df = splits[edge_name]['train'][['id1', 'id2', 'label']].copy()\n",
    "    \n",
    "    # Prepare labels: convert 'TRUE'/'FALSE' to 1/0\n",
    "    train_df['label'] = train_df['label'].astype(str).str.strip().str.upper()\n",
    "    train_df['label_binary'] = (train_df['label'] == 'TRUE').astype(int)\n",
    "    \n",
    "    print(f\"  Training pairs: {len(train_df)}\")\n",
    "    print(f\"    True matches: {train_df['label_binary'].sum()}\")\n",
    "    print(f\"    False matches: {len(train_df) - train_df['label_binary'].sum()}\")\n",
    "    \n",
    "    # Extract features for training pairs\n",
    "    print(f\"  Extracting features from training pairs...\")\n",
    "    train_features = ml_feature_extractor.create_features(\n",
    "        df_left=left_df,\n",
    "        df_right=right_df,\n",
    "        pairs=train_df[['id1', 'id2']],\n",
    "        labels=train_df['label_binary'],\n",
    "        id_column='_rid'\n",
    "    )\n",
    "    \n",
    "    print(f\"  Extracted features: {len(train_features)} pairs\")\n",
    "    print(f\"  Feature columns: {len([col for col in train_features.columns if col not in ['id1', 'id2', 'label']])}\")\n",
    "    \n",
    "    # Prepare training data\n",
    "    feature_columns = [col for col in train_features.columns if col not in ['id1', 'id2', 'label']]\n",
    "    X_train = train_features[feature_columns]\n",
    "    y_train = train_features['label']\n",
    "    \n",
    "    print(f\"  Training features shape: {X_train.shape}\")\n",
    "    print(f\"  Training labels distribution: {y_train.value_counts().to_dict()}\")\n",
    "    \n",
    "    # Train classifier\n",
    "    print(f\"  Training RandomForestClassifier...\")\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Store classifier and create matcher\n",
    "    ml_classifiers[edge_name] = clf\n",
    "    ml_matchers[edge_name] = MLBasedMatcher(ml_feature_extractor)\n",
    "    \n",
    "    # Log feature importance if available\n",
    "    if hasattr(clf, 'feature_importances_'):\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': feature_columns,\n",
    "            'importance': clf.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        print(f\"\\n  Top 5 Feature Importances:\")\n",
    "        for idx, row in feature_importance.head(5).iterrows():\n",
    "            print(f\"    {row['feature']}: {row['importance']:.4f}\")\n",
    "    \n",
    "    print(f\"  ✓ Classifier trained for {edge_name}\")\n",
    "\n",
    "print(\"\\n✓ ML-based matchers trained for all edges\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66828372",
   "metadata": {},
   "source": [
    "#### 5.3.2 Apply RandomForest Matcher\n",
    "Apply the trained RandomForest-based matcher to candidate pairs from the blocking phase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "38bef601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LR: ML-Based Matching ===\n",
      "  Filtering candidate pairs by season_year constraint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Starting Entity Matching\n",
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Blocking 106553 x 15215 elements\n",
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Matching 106553 x 15215 elements after 0:00:0.000; 135944 blocked pairs (reduction ratio: 0.9999161462661056)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  After season_year filter: 135,944 candidate pairs (from 5,994,373)\n",
      "  Applying ML-based matcher...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Entity Matching finished after 0:00:50.375; found 15375 correspondences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 15,375 matched pairs\n",
      "  Score range: [1.000, 1.000]\n",
      "  Mean score: 1.000\n",
      "\n",
      "=== LS: ML-Based Matching ===\n",
      "  Filtering candidate pairs by season_year constraint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Starting Entity Matching\n",
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Blocking 106553 x 6743 elements\n",
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Matching 106553 x 6743 elements after 0:00:0.000; 9729 blocked pairs (reduction ratio: 0.9999864590429076)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  After season_year filter: 9,729 candidate pairs (from 227,185)\n",
      "  Applying ML-based matcher...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Entity Matching finished after 0:00:3.921; found 6727 correspondences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 6,727 matched pairs\n",
      "  Score range: [1.000, 1.000]\n",
      "  Mean score: 1.000\n",
      "\n",
      "✓ ML-based matching complete for all edges\n"
     ]
    }
   ],
   "source": [
    "# Apply ML-based matching to candidate pairs\n",
    "ml_matching_results = {}\n",
    "\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    print(f\"\\n=== {edge_name}: ML-Based Matching ===\")\n",
    "    \n",
    "    left_df, right_df = source_tables[edge_name]\n",
    "    cand_df = candidates[edge_name].copy()\n",
    "    matcher = ml_matchers[edge_name]\n",
    "    clf = ml_classifiers[edge_name]\n",
    "    \n",
    "    # Apply season_year hard constraint: filter candidate pairs where season_year matches\n",
    "    print(f\"  Filtering candidate pairs by season_year constraint...\")\n",
    "    cand_with_seasons = cand_df.merge(\n",
    "        left_df[['_rid', 'season_year']],\n",
    "        left_on='id1',\n",
    "        right_on='_rid',\n",
    "        how='left'\n",
    "    ).merge(\n",
    "        right_df[['_rid', 'season_year']],\n",
    "        left_on='id2',\n",
    "        right_on='_rid',\n",
    "        how='left',\n",
    "        suffixes=('', '_right')\n",
    "    )\n",
    "    \n",
    "    # Filter: season_year must match exactly\n",
    "    cand_df_filtered = cand_with_seasons[\n",
    "        (cand_with_seasons['season_year'].notna()) & \n",
    "        (cand_with_seasons['season_year_right'].notna()) &\n",
    "        (cand_with_seasons['season_year'] == cand_with_seasons['season_year_right'])\n",
    "    ][['id1', 'id2']].copy()\n",
    "    \n",
    "    print(f\"  After season_year filter: {len(cand_df_filtered):,} candidate pairs (from {len(cand_df):,})\")\n",
    "    \n",
    "    # Apply ML-based matcher\n",
    "    print(f\"  Applying ML-based matcher...\")\n",
    "    correspondences = matcher.match(\n",
    "        df_left=left_df,\n",
    "        df_right=right_df,\n",
    "        candidates=cand_df_filtered,\n",
    "        id_column='_rid',\n",
    "        trained_classifier=clf\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    ml_matching_results[edge_name] = correspondences\n",
    "    \n",
    "    print(f\"  Generated {len(correspondences):,} matched pairs\")\n",
    "    if 'score' in correspondences.columns:\n",
    "        print(f\"  Score range: [{correspondences['score'].min():.3f}, {correspondences['score'].max():.3f}]\")\n",
    "        print(f\"  Mean score: {correspondences['score'].mean():.3f}\")\n",
    "\n",
    "print(\"\\n✓ ML-based matching complete for all edges\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3369139",
   "metadata": {},
   "source": [
    "#### 5.3.3 Evaluate RandomForest Matching\n",
    "\n",
    "Evaluate the performance of the RandomForest-based matcher on the validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "095ec0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LR: ML-Based Matching Evaluation (Validation Set) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Confusion Matrix:\n",
      "[INFO ] root -   True Positives:  30\n",
      "[INFO ] root -   True Negatives:  66\n",
      "[INFO ] root -   False Positives: 0\n",
      "[INFO ] root -   False Negatives: 0\n",
      "[INFO ] root - Performance Metrics:\n",
      "[INFO ] root -   Accuracy:  1.000\n",
      "[INFO ] root -   Precision: 1.000\n",
      "[INFO ] root -   Recall:    1.000\n",
      "[INFO ] root -   F1-Score:  1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Precision: 1.000\n",
      "  Recall:    1.000\n",
      "  F1-Score:  1.000\n",
      "  TP: 30\n",
      "  FP: 0\n",
      "  FN: 0\n",
      "\n",
      "=== LS: ML-Based Matching Evaluation (Validation Set) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Confusion Matrix:\n",
      "[INFO ] root -   True Positives:  49\n",
      "[INFO ] root -   True Negatives:  45\n",
      "[INFO ] root -   False Positives: 2\n",
      "[INFO ] root -   False Negatives: 0\n",
      "[INFO ] root - Performance Metrics:\n",
      "[INFO ] root -   Accuracy:  0.979\n",
      "[INFO ] root -   Precision: 0.961\n",
      "[INFO ] root -   Recall:    1.000\n",
      "[INFO ] root -   F1-Score:  0.980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Precision: 0.961\n",
      "  Recall:    1.000\n",
      "  F1-Score:  0.980\n",
      "  TP: 49\n",
      "  FP: 2\n",
      "  FN: 0\n",
      "\n",
      "✓ ML-based matching evaluation complete\n"
     ]
    }
   ],
   "source": [
    "# Evaluate ML-based matching on validation set\n",
    "\n",
    "ml_matching_metrics_val = {}\n",
    "\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    print(f\"\\n=== {edge_name}: ML-Based Matching Evaluation (Validation Set) ===\")\n",
    "    \n",
    "    correspondences = ml_matching_results[edge_name]\n",
    "    val_df = splits[edge_name]['val'][['id1', 'id2', 'label']].copy()\n",
    "    \n",
    "    # Rename 'score' to match PyDI evaluator expectations (if needed)\n",
    "    correspondences_for_eval = correspondences.copy()\n",
    "    if 'score' not in correspondences_for_eval.columns and 'sim' in correspondences_for_eval.columns:\n",
    "        correspondences_for_eval = correspondences_for_eval.rename(columns={'sim': 'score'})\n",
    "    \n",
    "    # Evaluate matching\n",
    "    try:\n",
    "        eval_results = EntityMatchingEvaluator.evaluate_matching(\n",
    "            correspondences=correspondences_for_eval,\n",
    "            test_pairs=val_df,\n",
    "            out_dir=OUTPUT_DIR / 'matching-evaluation-ml',\n",
    "            matcher_instance=ml_matchers[edge_name]\n",
    "        )\n",
    "        \n",
    "        ml_matching_metrics_val[edge_name] = eval_results\n",
    "        \n",
    "        print(f\"\\n  Precision: {eval_results.get('precision', 0.0):.3f}\")\n",
    "        print(f\"  Recall:    {eval_results.get('recall', 0.0):.3f}\")\n",
    "        print(f\"  F1-Score:  {eval_results.get('f1', 0.0):.3f}\")\n",
    "        print(f\"  TP: {eval_results.get('true_positives', 0)}\")\n",
    "        print(f\"  FP: {eval_results.get('false_positives', 0)}\")\n",
    "        print(f\"  FN: {eval_results.get('false_negatives', 0)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  PyDI evaluator failed: {e}\")\n",
    "        # Manual evaluation fallback\n",
    "        true_matches = val_df[val_df['label'].astype(str).str.strip().str.upper() == 'TRUE']\n",
    "        true_set = set(zip(true_matches['id1'], true_matches['id2']))\n",
    "        pred_set = set(zip(correspondences['id1'], correspondences['id2']))\n",
    "        \n",
    "        tp = len(true_set & pred_set)\n",
    "        fp = len(pred_set - true_set)\n",
    "        fn = len(true_set - pred_set)\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        \n",
    "        print(f\"\\n  Precision: {precision:.3f}\")\n",
    "        print(f\"  Recall:    {recall:.3f}\")\n",
    "        print(f\"  F1-Score:  {f1:.3f}\")\n",
    "        print(f\"  TP: {tp}\")\n",
    "        print(f\"  FP: {fp}\")\n",
    "        print(f\"  FN: {fn}\")\n",
    "\n",
    "print(\"\\n✓ ML-based matching evaluation complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7621fcf",
   "metadata": {},
   "source": [
    "#### 5.3.4 Analyze RandomForest Error Cases\n",
    "\n",
    "Analyze False Positives and False Negatives for RandomForestClassifier to identify patterns for further improvement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "947b13a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LR: RandomForestClassifier Error Cases Analysis\n",
      "================================================================================\n",
      "\n",
      "[FALSE NEGATIVES] (0 cases):\n",
      "  No false negatives found!\n",
      "\n",
      "[FALSE POSITIVES] (0 cases):\n",
      "  No false positives found!\n",
      "\n",
      "================================================================================\n",
      "LS: RandomForestClassifier Error Cases Analysis\n",
      "================================================================================\n",
      "\n",
      "[FALSE NEGATIVES] (0 cases):\n",
      "  No false negatives found!\n",
      "\n",
      "[FALSE POSITIVES] (2 cases):\n",
      "\n",
      "  5437062017|2017|L <-> 6210022017|2017|S (score: 1.000)\n",
      "    Left:  'dan robertson' | Season: 2017 | Birth: 1985\n",
      "    Right: 'daniel robertson' | Season: 2017 | Birth: 1994\n",
      "\n",
      "  6703512022|2022|L <-> 6689422022|2022|S (score: 1.000)\n",
      "    Left:  'jose rojas' | Season: 2022 | Birth: 1993\n",
      "    Right: 'josh rojas' | Season: 2022 | Birth: 1994\n",
      "\n",
      "✓ RandomForestClassifier error analysis complete\n"
     ]
    }
   ],
   "source": [
    "# Analyze error cases for RandomForestClassifier (reusing code structure from 3.2)\n",
    "\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{edge_name}: RandomForestClassifier Error Cases Analysis\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    left_df, right_df = source_tables[edge_name]\n",
    "    val_df = splits[edge_name]['val'][['id1', 'id2', 'label']].copy()\n",
    "    correspondences = ml_matching_results[edge_name]\n",
    "    \n",
    "    # Get true matches and false matches in validation set\n",
    "    true_matches = val_df[val_df['label'].astype(str).str.strip().str.upper() == 'TRUE']\n",
    "    false_matches = val_df[val_df['label'].astype(str).str.strip().str.upper() == 'FALSE']\n",
    "    true_set = set(zip(true_matches['id1'], true_matches['id2']))\n",
    "    false_set = set(zip(false_matches['id1'], false_matches['id2']))\n",
    "    \n",
    "    # Get predicted matches (only those in validation set)\n",
    "    pred_set = set(zip(correspondences['id1'], correspondences['id2']))\n",
    "    val_set = set(zip(val_df['id1'], val_df['id2']))  # All pairs in validation set\n",
    "    \n",
    "    # False Negatives: True matches in validation set that were not predicted\n",
    "    fn_pairs = true_set - pred_set\n",
    "    print(f\"\\n[FALSE NEGATIVES] ({len(fn_pairs)} cases):\")\n",
    "    if len(fn_pairs) > 0:\n",
    "        for id1, id2 in fn_pairs:\n",
    "            left_rec = left_df[left_df['_rid'] == id1].iloc[0] if len(left_df[left_df['_rid'] == id1]) > 0 else None\n",
    "            right_rec = right_df[right_df['_rid'] == id2].iloc[0] if len(right_df[right_df['_rid'] == id2]) > 0 else None\n",
    "            \n",
    "            if left_rec is not None and right_rec is not None:\n",
    "                print(f\"\\n  {id1} <-> {id2}\")\n",
    "                print(f\"    Left:  '{left_rec.get('full_name', 'N/A')}' | Season: {left_rec.get('season_year', 'N/A')} | Birth: {left_rec.get('birth_year', 'N/A')}\")\n",
    "                print(f\"    Right: '{right_rec.get('full_name', 'N/A')}' | Season: {right_rec.get('season_year', 'N/A')} | Birth: {right_rec.get('birth_year', 'N/A')}\")\n",
    "                \n",
    "                # Check if in candidates\n",
    "                in_candidates = len(candidates[edge_name][\n",
    "                    (candidates[edge_name]['id1'] == id1) & \n",
    "                    (candidates[edge_name]['id2'] == id2)\n",
    "                ]) > 0\n",
    "                print(f\"    In candidates: {in_candidates}\")\n",
    "                \n",
    "                # Check if pair was scored by RF (even if below threshold)\n",
    "                score_row = correspondences[(correspondences['id1'] == id1) & (correspondences['id2'] == id2)]\n",
    "                if len(score_row) > 0:\n",
    "                    score = score_row['score'].iloc[0] if 'score' in score_row.columns else None\n",
    "                    score_str = f\"{score:.3f}\" if score is not None else \"N/A\"\n",
    "                    print(f\"    RF Score: {score_str}\")\n",
    "    else:\n",
    "        print(\"  No false negatives found!\")\n",
    "    \n",
    "    # False Positives: Predicted matches that are in validation set but labeled as FALSE\n",
    "    # Only analyze pairs that are in the validation set\n",
    "    fp_pairs = (pred_set & val_set) & false_set\n",
    "    print(f\"\\n[FALSE POSITIVES] ({len(fp_pairs)} cases):\")\n",
    "    if len(fp_pairs) > 0:\n",
    "        for id1, id2 in fp_pairs:\n",
    "            left_rec = left_df[left_df['_rid'] == id1].iloc[0] if len(left_df[left_df['_rid'] == id1]) > 0 else None\n",
    "            right_rec = right_df[right_df['_rid'] == id2].iloc[0] if len(right_df[right_df['_rid'] == id2]) > 0 else None\n",
    "            score_row = correspondences[(correspondences['id1'] == id1) & (correspondences['id2'] == id2)]\n",
    "            score = score_row['score'].iloc[0] if len(score_row) > 0 and 'score' in score_row.columns else None\n",
    "            \n",
    "            if left_rec is not None and right_rec is not None:\n",
    "                score_str = f\"{score:.3f}\" if score is not None else \"N/A\"\n",
    "                print(f\"\\n  {id1} <-> {id2} (score: {score_str})\")\n",
    "                print(f\"    Left:  '{left_rec.get('full_name', 'N/A')}' | Season: {left_rec.get('season_year', 'N/A')} | Birth: {left_rec.get('birth_year', 'N/A')}\")\n",
    "                print(f\"    Right: '{right_rec.get('full_name', 'N/A')}' | Season: {right_rec.get('season_year', 'N/A')} | Birth: {right_rec.get('birth_year', 'N/A')}\")\n",
    "    else:\n",
    "        print(\"  No false positives found!\")\n",
    "\n",
    "print(\"\\n✓ RandomForestClassifier error analysis complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b8b4e4",
   "metadata": {},
   "source": [
    "### 5.4 Train and Apply GradientBoostingClassifier Matcher\n",
    "\n",
    "Train and apply GradientBoostingClassifier using the same feature extractor for comparison with RandomForest.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3347d7b6",
   "metadata": {},
   "source": [
    "#### 5.4.1 Train and Apply GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3bd5a79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LR: GradientBoostingClassifier ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Label distribution: 90 positive, 214 negative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training GradientBoostingClassifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Starting Entity Matching\n",
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Blocking 106553 x 15215 elements\n",
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Matching 106553 x 15215 elements after 0:00:0.000; 135944 blocked pairs (reduction ratio: 0.9999161462661056)\n",
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Entity Matching finished after 0:00:52.456; found 15533 correspondences.\n",
      "[INFO ] root - Label distribution: 142 positive, 154 negative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 15,533 matched pairs\n",
      "\n",
      "=== LS: GradientBoostingClassifier ===\n",
      "  Training GradientBoostingClassifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Starting Entity Matching\n",
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Blocking 106553 x 6743 elements\n",
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Matching 106553 x 6743 elements after 0:00:0.000; 9729 blocked pairs (reduction ratio: 0.9999864590429076)\n",
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Entity Matching finished after 0:00:3.650; found 6692 correspondences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 6,692 matched pairs\n",
      "\n",
      "✓ GradientBoostingClassifier matching complete\n"
     ]
    }
   ],
   "source": [
    "# Train and apply GradientBoostingClassifier (reusing feature extractor from 4.2)\n",
    "\n",
    "# Ensure GradientBoostingClassifier is imported\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_classifiers = {}\n",
    "gb_matchers = {}\n",
    "gb_matching_results = {}\n",
    "\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    print(f\"\\n=== {edge_name}: GradientBoostingClassifier ===\")\n",
    "    \n",
    "    left_df, right_df = source_tables[edge_name]\n",
    "    train_df = splits[edge_name]['train'][['id1', 'id2', 'label']].copy()\n",
    "    train_df['label'] = train_df['label'].astype(str).str.strip().str.upper()\n",
    "    train_df['label_binary'] = (train_df['label'] == 'TRUE').astype(int)\n",
    "    \n",
    "    # Reuse feature extraction from 4.2\n",
    "    train_features = ml_feature_extractor.create_features(\n",
    "        df_left=left_df,\n",
    "        df_right=right_df,\n",
    "        pairs=train_df[['id1', 'id2']],\n",
    "        labels=train_df['label_binary'],\n",
    "        id_column='_rid'\n",
    "    )\n",
    "    \n",
    "    feature_columns = [col for col in train_features.columns if col not in ['id1', 'id2', 'label']]\n",
    "    X_train = train_features[feature_columns]\n",
    "    y_train = train_features['label']\n",
    "    \n",
    "    # Train GradientBoostingClassifier\n",
    "    print(f\"  Training GradientBoostingClassifier...\")\n",
    "    gb_clf = GradientBoostingClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        random_state=42\n",
    "    )\n",
    "    gb_clf.fit(X_train, y_train)\n",
    "    \n",
    "    gb_classifiers[edge_name] = gb_clf\n",
    "    gb_matchers[edge_name] = MLBasedMatcher(ml_feature_extractor)\n",
    "    \n",
    "    # Apply to candidate pairs (with season_year constraint)\n",
    "    cand_df = candidates[edge_name].copy()\n",
    "    cand_with_seasons = cand_df.merge(\n",
    "        left_df[['_rid', 'season_year']],\n",
    "        left_on='id1', right_on='_rid', how='left'\n",
    "    ).merge(\n",
    "        right_df[['_rid', 'season_year']],\n",
    "        left_on='id2', right_on='_rid', how='left', suffixes=('', '_right')\n",
    "    )\n",
    "    cand_df_filtered = cand_with_seasons[\n",
    "        (cand_with_seasons['season_year'].notna()) & \n",
    "        (cand_with_seasons['season_year_right'].notna()) &\n",
    "        (cand_with_seasons['season_year'] == cand_with_seasons['season_year_right'])\n",
    "    ][['id1', 'id2']].copy()\n",
    "    \n",
    "    # Apply matcher\n",
    "    correspondences = gb_matchers[edge_name].match(\n",
    "        df_left=left_df,\n",
    "        df_right=right_df,\n",
    "        candidates=cand_df_filtered,\n",
    "        id_column='_rid',\n",
    "        trained_classifier=gb_clf\n",
    "    )\n",
    "    gb_matching_results[edge_name] = correspondences\n",
    "    \n",
    "    print(f\"  Generated {len(correspondences):,} matched pairs\")\n",
    "\n",
    "print(\"\\n✓ GradientBoostingClassifier matching complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388fec75",
   "metadata": {},
   "source": [
    "#### 5.4.2 Hyperparameter Tuning for GradientBoostingClassifier\n",
    "\n",
    "Use RandomizedSearchCV to find optimal hyperparameters for GradientBoostingClassifier. This uses the **validation set** for tuning (NOT the test set). The tuned model will be stored separately to compare with the default model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ef367d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GradientBoosting Hyperparameter Tuning (Using Validation Set)\n",
      "================================================================================\n",
      "⚠️  Using validation set for tuning. Test set will be used only for final evaluation.\n",
      "\n",
      "\n",
      "================================================================================\n",
      "LR Edge: Hyperparameter Tuning\n",
      "================================================================================\n",
      "  Training pairs: 304\n",
      "  Validation pairs: 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Label distribution: 90 positive, 214 negative\n",
      "[INFO ] root - Label distribution: 30 positive, 66 negative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Feature dimensions: 6\n",
      "  Training: 304 samples\n",
      "  Validation: 96 samples\n",
      "\n",
      "  Running RandomizedSearchCV (n_iter=15 for speed)...\n",
      "Fitting 3 folds for each of 15 candidates, totalling 45 fits\n",
      "\n",
      "  Best cross-validation F1 score: 1.0000\n",
      "  Best parameters:\n",
      "    learning_rate: 0.05\n",
      "    max_depth: 4\n",
      "    min_samples_leaf: 2\n",
      "    min_samples_split: 5\n",
      "    n_estimators: 300\n",
      "    subsample: 0.8\n",
      "\n",
      "  Validation set F1 score (best model): 1.0000\n",
      "  Validation set F1 score (default model): 1.0000\n",
      "  Improvement: +0.0000 (+0.00%)\n",
      "\n",
      "  ✓ Hyperparameter tuning complete for LR\n",
      "\n",
      "================================================================================\n",
      "LS Edge: Hyperparameter Tuning\n",
      "================================================================================\n",
      "  Training pairs: 296\n",
      "  Validation pairs: 101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Label distribution: 142 positive, 154 negative\n",
      "[INFO ] root - Label distribution: 49 positive, 52 negative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Feature dimensions: 6\n",
      "  Training: 296 samples\n",
      "  Validation: 101 samples\n",
      "\n",
      "  Running RandomizedSearchCV (n_iter=15 for speed)...\n",
      "Fitting 3 folds for each of 15 candidates, totalling 45 fits\n",
      "\n",
      "  Best cross-validation F1 score: 0.9856\n",
      "  Best parameters:\n",
      "    learning_rate: 0.05\n",
      "    max_depth: 3\n",
      "    min_samples_leaf: 2\n",
      "    min_samples_split: 5\n",
      "    n_estimators: 300\n",
      "    subsample: 0.9\n",
      "\n",
      "  Validation set F1 score (best model): 0.9796\n",
      "  Validation set F1 score (default model): 0.9796\n",
      "  Improvement: +0.0000 (+0.00%)\n",
      "\n",
      "  ✓ Hyperparameter tuning complete for LS\n",
      "\n",
      "================================================================================\n",
      "✓ GradientBoosting hyperparameter tuning complete for all edges\n",
      "================================================================================\n",
      "\n",
      "Tuning Summary:\n",
      "--------------------------------------------------------------------------------\n",
      "LR:\n",
      "  Default F1: 1.0000\n",
      "  Tuned F1:   1.0000\n",
      "  Improvement: +0.0000 (+0.00%)\n",
      "LS:\n",
      "  Default F1: 0.9796\n",
      "  Tuned F1:   0.9796\n",
      "  Improvement: +0.0000 (+0.00%)\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning for GradientBoostingClassifier using validation set\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "\n",
    "# Define parameter distribution for RandomizedSearchCV\n",
    "gb_param_dist = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.05, 0.1, 0.15],\n",
    "    'max_depth': [3, 4, 5, 6, 7],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'subsample': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "# F1 score scorer for optimization\n",
    "f1_scorer = make_scorer(f1_score)\n",
    "\n",
    "# Dictionary to store tuned classifiers (separate from default)\n",
    "gb_tuned_classifiers = {}\n",
    "gb_tuning_results = {}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"GradientBoosting Hyperparameter Tuning (Using Validation Set)\")\n",
    "print(\"=\"*80)\n",
    "print(\"⚠️  Using validation set for tuning. Test set will be used only for final evaluation.\\n\")\n",
    "\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{edge_name} Edge: Hyperparameter Tuning\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    left_df, right_df = source_tables[edge_name]\n",
    "    \n",
    "    # Prepare training data\n",
    "    train_df = splits[edge_name]['train'][['id1', 'id2', 'label']].copy()\n",
    "    train_df['label'] = train_df['label'].astype(str).str.strip().str.upper()\n",
    "    train_df['label_binary'] = (train_df['label'] == 'TRUE').astype(int)\n",
    "    \n",
    "    # Prepare validation data\n",
    "    val_df = splits[edge_name]['val'][['id1', 'id2', 'label']].copy()\n",
    "    val_df['label'] = val_df['label'].astype(str).str.strip().str.upper()\n",
    "    val_df['label_binary'] = (val_df['label'] == 'TRUE').astype(int)\n",
    "    \n",
    "    print(f\"  Training pairs: {len(train_df)}\")\n",
    "    print(f\"  Validation pairs: {len(val_df)}\")\n",
    "    \n",
    "    # Extract features for training\n",
    "    train_features = ml_feature_extractor.create_features(\n",
    "        df_left=left_df,\n",
    "        df_right=right_df,\n",
    "        pairs=train_df[['id1', 'id2']],\n",
    "        labels=train_df['label_binary'],\n",
    "        id_column='_rid'\n",
    "    )\n",
    "    \n",
    "    # Extract features for validation\n",
    "    val_features = ml_feature_extractor.create_features(\n",
    "        df_left=left_df,\n",
    "        df_right=right_df,\n",
    "        pairs=val_df[['id1', 'id2']],\n",
    "        labels=val_df['label_binary'],\n",
    "        id_column='_rid'\n",
    "    )\n",
    "    \n",
    "    feature_columns = [col for col in train_features.columns if col not in ['id1', 'id2', 'label']]\n",
    "    X_train = train_features[feature_columns]\n",
    "    y_train = train_features['label']\n",
    "    X_val = val_features[feature_columns]\n",
    "    y_val = val_features['label']\n",
    "    \n",
    "    print(f\"  Feature dimensions: {X_train.shape[1]}\")\n",
    "    print(f\"  Training: {len(X_train)} samples\")\n",
    "    print(f\"  Validation: {len(X_val)} samples\")\n",
    "    \n",
    "    # Base GradientBoosting classifier\n",
    "    base_clf = GradientBoostingClassifier(\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # RandomizedSearchCV with limited iterations for speed\n",
    "    print(f\"\\n  Running RandomizedSearchCV (n_iter=15 for speed)...\")\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=base_clf,\n",
    "        param_distributions=gb_param_dist,\n",
    "        n_iter=15,  # Limited iterations for faster execution\n",
    "        scoring=f1_scorer,\n",
    "        cv=3,  # 3-fold cross-validation on training set\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Fit on training data\n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get best parameters\n",
    "    best_params = random_search.best_params_\n",
    "    best_score = random_search.best_score_\n",
    "    \n",
    "    print(f\"\\n  Best cross-validation F1 score: {best_score:.4f}\")\n",
    "    print(f\"  Best parameters:\")\n",
    "    for param, value in sorted(best_params.items()):\n",
    "        print(f\"    {param}: {value}\")\n",
    "    \n",
    "    # Evaluate on validation set with best model\n",
    "    best_clf = random_search.best_estimator_\n",
    "    y_val_pred = best_clf.predict(X_val)\n",
    "    val_f1 = f1_score(y_val, y_val_pred)\n",
    "    \n",
    "    print(f\"\\n  Validation set F1 score (best model): {val_f1:.4f}\")\n",
    "    \n",
    "    # Compare with default model (from 5.4.1)\n",
    "    default_clf = gb_classifiers[edge_name]  # Use the already trained default model\n",
    "    y_val_pred_default = default_clf.predict(X_val)\n",
    "    val_f1_default = f1_score(y_val, y_val_pred_default)\n",
    "    \n",
    "    print(f\"  Validation set F1 score (default model): {val_f1_default:.4f}\")\n",
    "    improvement = val_f1 - val_f1_default\n",
    "    print(f\"  Improvement: {improvement:+.4f} ({improvement/val_f1_default*100:+.2f}%)\")\n",
    "    \n",
    "    # Store results (separate from default model)\n",
    "    gb_tuned_classifiers[edge_name] = best_clf\n",
    "    gb_tuning_results[edge_name] = {\n",
    "        'best_params': best_params,\n",
    "        'best_cv_score': best_score,\n",
    "        'val_f1': val_f1,\n",
    "        'val_f1_default': val_f1_default,\n",
    "        'improvement': improvement\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n  ✓ Hyperparameter tuning complete for {edge_name}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"✓ GradientBoosting hyperparameter tuning complete for all edges\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nTuning Summary:\")\n",
    "print(\"-\" * 80)\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    results = gb_tuning_results[edge_name]\n",
    "    print(f\"{edge_name}:\")\n",
    "    print(f\"  Default F1: {results['val_f1_default']:.4f}\")\n",
    "    print(f\"  Tuned F1:   {results['val_f1']:.4f}\")\n",
    "    print(f\"  Improvement: {results['improvement']:+.4f} ({results['improvement']/results['val_f1_default']*100:+.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623da52b",
   "metadata": {},
   "source": [
    "#### 5.4.3 Evaluate GradientBoosting Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "51f2be7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LR: GradientBoostingClassifier Evaluation ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Confusion Matrix:\n",
      "[INFO ] root -   True Positives:  30\n",
      "[INFO ] root -   True Negatives:  66\n",
      "[INFO ] root -   False Positives: 0\n",
      "[INFO ] root -   False Negatives: 0\n",
      "[INFO ] root - Performance Metrics:\n",
      "[INFO ] root -   Accuracy:  1.000\n",
      "[INFO ] root -   Precision: 1.000\n",
      "[INFO ] root -   Recall:    1.000\n",
      "[INFO ] root -   F1-Score:  1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Precision: 1.000\n",
      "  Recall:    1.000\n",
      "  F1-Score:  1.000\n",
      "\n",
      "=== LS: GradientBoostingClassifier Evaluation ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Confusion Matrix:\n",
      "[INFO ] root -   True Positives:  48\n",
      "[INFO ] root -   True Negatives:  46\n",
      "[INFO ] root -   False Positives: 1\n",
      "[INFO ] root -   False Negatives: 1\n",
      "[INFO ] root - Performance Metrics:\n",
      "[INFO ] root -   Accuracy:  0.979\n",
      "[INFO ] root -   Precision: 0.980\n",
      "[INFO ] root -   Recall:    0.980\n",
      "[INFO ] root -   F1-Score:  0.980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Precision: 0.980\n",
      "  Recall:    0.980\n",
      "  F1-Score:  0.980\n",
      "\n",
      "✓ GradientBoostingClassifier evaluation complete\n"
     ]
    }
   ],
   "source": [
    "# Evaluate GradientBoostingClassifier matching\n",
    "\n",
    "gb_matching_metrics_val = {}\n",
    "\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    print(f\"\\n=== {edge_name}: GradientBoostingClassifier Evaluation ===\")\n",
    "    \n",
    "    correspondences = gb_matching_results[edge_name]\n",
    "    val_df = splits[edge_name]['val'][['id1', 'id2', 'label']].copy()\n",
    "    \n",
    "    correspondences_for_eval = correspondences.copy()\n",
    "    if 'score' not in correspondences_for_eval.columns and 'sim' in correspondences_for_eval.columns:\n",
    "        correspondences_for_eval = correspondences_for_eval.rename(columns={'sim': 'score'})\n",
    "    \n",
    "    try:\n",
    "        eval_results = EntityMatchingEvaluator.evaluate_matching(\n",
    "            correspondences=correspondences_for_eval,\n",
    "            test_pairs=val_df,\n",
    "            out_dir=OUTPUT_DIR / 'matching-evaluation-gb',\n",
    "            matcher_instance=gb_matchers[edge_name]\n",
    "        )\n",
    "        gb_matching_metrics_val[edge_name] = eval_results\n",
    "        print(f\"  Precision: {eval_results.get('precision', 0.0):.3f}\")\n",
    "        print(f\"  Recall:    {eval_results.get('recall', 0.0):.3f}\")\n",
    "        print(f\"  F1-Score:  {eval_results.get('f1', 0.0):.3f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Evaluation failed: {e}\")\n",
    "\n",
    "print(\"\\n✓ GradientBoostingClassifier evaluation complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f4309b",
   "metadata": {},
   "source": [
    "#### 5.4.5 Apply Tuned GradientBoostingClassifier Matcher\n",
    "\n",
    "Apply the tuned GradientBoostingClassifier model to candidate pairs. This allows comparison with the default model results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "aa625c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LR: Tuned GradientBoostingClassifier Matching ===\n",
      "  Filtering candidate pairs by season_year constraint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Starting Entity Matching\n",
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Blocking 106553 x 15215 elements\n",
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Matching 106553 x 15215 elements after 0:00:0.000; 135944 blocked pairs (reduction ratio: 0.9999161462661056)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  After season_year filter: 135,944 candidate pairs (from 5,994,373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Entity Matching finished after 0:00:47.165; found 15496 correspondences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 15,496 matched pairs\n",
      "  Score range: [1.000, 1.000]\n",
      "  Mean score: 1.000\n",
      "=== LS: Tuned GradientBoostingClassifier Matching ===\n",
      "  Filtering candidate pairs by season_year constraint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Starting Entity Matching\n",
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Blocking 106553 x 6743 elements\n",
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Matching 106553 x 6743 elements after 0:00:0.000; 9729 blocked pairs (reduction ratio: 0.9999864590429076)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  After season_year filter: 9,729 candidate pairs (from 227,185)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Entity Matching finished after 0:00:3.262; found 6679 correspondences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 6,679 matched pairs\n",
      "  Score range: [1.000, 1.000]\n",
      "  Mean score: 1.000\n",
      "\n",
      "✓ Tuned GradientBoostingClassifier matching complete for all edges\n"
     ]
    }
   ],
   "source": [
    "# Apply tuned GradientBoostingClassifier to candidate pairs\n",
    "gb_tuned_matching_results = {}\n",
    "gb_tuned_matchers = {}\n",
    "\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    print(f\"=== {edge_name}: Tuned GradientBoostingClassifier Matching ===\")\n",
    "    \n",
    "    left_df, right_df = source_tables[edge_name]\n",
    "    cand_df = candidates[edge_name].copy()\n",
    "    clf = gb_tuned_classifiers[edge_name]\n",
    "    \n",
    "    # Create matcher with tuned classifier\n",
    "    matcher = MLBasedMatcher(ml_feature_extractor)\n",
    "    gb_tuned_matchers[edge_name] = matcher\n",
    "    \n",
    "    print(\"  Filtering candidate pairs by season_year constraint...\")\n",
    "    cand_with_seasons = cand_df.merge(\n",
    "        left_df[['_rid', 'season_year']],\n",
    "        left_on='id1', right_on='_rid', how='left'\n",
    "    ).merge(\n",
    "        right_df[['_rid', 'season_year']],\n",
    "        left_on='id2', right_on='_rid', how='left', suffixes=('', '_right')\n",
    "    )\n",
    "    cand_df_filtered = cand_with_seasons[\n",
    "        (cand_with_seasons['season_year'].notna()) & \n",
    "        (cand_with_seasons['season_year_right'].notna()) &\n",
    "        (cand_with_seasons['season_year'] == cand_with_seasons['season_year_right'])\n",
    "    ][['id1', 'id2']].copy()\n",
    "    \n",
    "    print(f\"  After season_year filter: {len(cand_df_filtered):,} candidate pairs (from {len(cand_df):,})\")\n",
    "    \n",
    "    correspondences = matcher.match(\n",
    "        df_left=left_df,\n",
    "        df_right=right_df,\n",
    "        candidates=cand_df_filtered,\n",
    "        id_column='_rid',\n",
    "        trained_classifier=clf\n",
    "    )\n",
    "    \n",
    "    gb_tuned_matching_results[edge_name] = correspondences\n",
    "    \n",
    "    print(f\"  Generated {len(correspondences):,} matched pairs\")\n",
    "    if 'score' in correspondences.columns:\n",
    "        print(f\"  Score range: [{correspondences['score'].min():.3f}, {correspondences['score'].max():.3f}]\")\n",
    "        print(f\"  Mean score: {correspondences['score'].mean():.3f}\")\n",
    "\n",
    "print(\"\\n✓ Tuned GradientBoostingClassifier matching complete for all edges\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6514158b",
   "metadata": {},
   "source": [
    "#### 5.4.6 Evaluate Tuned GradientBoostingClassifier Matching\n",
    "\n",
    "Evaluate the tuned model performance on validation set and compare with default model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8b0c26e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Tuned GradientBoostingClassifier Evaluation (Validation Set)\n",
      "================================================================================\n",
      "\n",
      "=== LR: Tuned GradientBoostingClassifier Evaluation ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Confusion Matrix:\n",
      "[INFO ] root -   True Positives:  30\n",
      "[INFO ] root -   True Negatives:  66\n",
      "[INFO ] root -   False Positives: 0\n",
      "[INFO ] root -   False Negatives: 0\n",
      "[INFO ] root - Performance Metrics:\n",
      "[INFO ] root -   Accuracy:  1.000\n",
      "[INFO ] root -   Precision: 1.000\n",
      "[INFO ] root -   Recall:    1.000\n",
      "[INFO ] root -   F1-Score:  1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Precision: 1.000\n",
      "  Recall:    1.000\n",
      "  F1-Score:  1.000\n",
      "  TP: 30\n",
      "  FP: 0\n",
      "  FN: 0\n",
      "\n",
      "  Comparison with Default Model:\n",
      "    Default F1: 1.000\n",
      "    Tuned F1:   1.000\n",
      "    Improvement: +0.0000\n",
      "\n",
      "=== LS: Tuned GradientBoostingClassifier Evaluation ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Confusion Matrix:\n",
      "[INFO ] root -   True Positives:  48\n",
      "[INFO ] root -   True Negatives:  46\n",
      "[INFO ] root -   False Positives: 1\n",
      "[INFO ] root -   False Negatives: 1\n",
      "[INFO ] root - Performance Metrics:\n",
      "[INFO ] root -   Accuracy:  0.979\n",
      "[INFO ] root -   Precision: 0.980\n",
      "[INFO ] root -   Recall:    0.980\n",
      "[INFO ] root -   F1-Score:  0.980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Precision: 0.980\n",
      "  Recall:    0.980\n",
      "  F1-Score:  0.980\n",
      "  TP: 48\n",
      "  FP: 1\n",
      "  FN: 1\n",
      "\n",
      "  Comparison with Default Model:\n",
      "    Default F1: 0.980\n",
      "    Tuned F1:   0.980\n",
      "    Improvement: +0.0000\n",
      "\n",
      "✓ Tuned GradientBoostingClassifier evaluation complete\n"
     ]
    }
   ],
   "source": [
    "# Evaluate tuned GradientBoostingClassifier matching\n",
    "gb_tuned_matching_metrics_val = {}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Tuned GradientBoostingClassifier Evaluation (Validation Set)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    print(f\"\\n=== {edge_name}: Tuned GradientBoostingClassifier Evaluation ===\")\n",
    "    \n",
    "    correspondences = gb_tuned_matching_results[edge_name]\n",
    "    val_df = splits[edge_name]['val'][['id1', 'id2', 'label']].copy()\n",
    "    \n",
    "    correspondences_for_eval = correspondences.copy()\n",
    "    if 'score' not in correspondences_for_eval.columns and 'sim' in correspondences_for_eval.columns:\n",
    "        correspondences_for_eval = correspondences_for_eval.rename(columns={'sim': 'score'})\n",
    "    \n",
    "    try:\n",
    "        eval_results = EntityMatchingEvaluator.evaluate_matching(\n",
    "            correspondences=correspondences_for_eval,\n",
    "            test_pairs=val_df,\n",
    "            out_dir=OUTPUT_DIR / 'matching-evaluation-gb-tuned',\n",
    "            matcher_instance=gb_tuned_matchers[edge_name]\n",
    "        )\n",
    "        gb_tuned_matching_metrics_val[edge_name] = eval_results\n",
    "        \n",
    "        print(f\"  Precision: {eval_results.get('precision', 0.0):.3f}\")\n",
    "        print(f\"  Recall:    {eval_results.get('recall', 0.0):.3f}\")\n",
    "        print(f\"  F1-Score:  {eval_results.get('f1', 0.0):.3f}\")\n",
    "        print(f\"  TP: {eval_results.get('true_positives', 0)}\")\n",
    "        print(f\"  FP: {eval_results.get('false_positives', 0)}\")\n",
    "        print(f\"  FN: {eval_results.get('false_negatives', 0)}\")\n",
    "        \n",
    "        # Compare with default model\n",
    "        default_metrics = gb_matching_metrics_val.get(edge_name, {})\n",
    "        if default_metrics:\n",
    "            print(f\"\\n  Comparison with Default Model:\")\n",
    "            print(f\"    Default F1: {default_metrics.get('f1', 0.0):.3f}\")\n",
    "            print(f\"    Tuned F1:   {eval_results.get('f1', 0.0):.3f}\")\n",
    "            improvement = eval_results.get('f1', 0.0) - default_metrics.get('f1', 0.0)\n",
    "            print(f\"    Improvement: {improvement:+.4f}\")\n",
    "            if improvement > 0:\n",
    "                print(f\"    Relative improvement: {improvement/default_metrics.get('f1', 0.0)*100:+.2f}%\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Evaluation failed: {e}\")\n",
    "        # Fallback to manual calculation\n",
    "        true_matches = val_df[val_df['label'].astype(str).str.strip().str.upper() == 'TRUE']\n",
    "        true_set = set(zip(true_matches['id1'], true_matches['id2']))\n",
    "        pred_set = set(zip(correspondences['id1'], correspondences['id2']))\n",
    "        tp = len(true_set & pred_set)\n",
    "        fp = len(pred_set - true_set)\n",
    "        fn = len(true_set - pred_set)\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        gb_tuned_matching_metrics_val[edge_name] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'true_positives': tp,\n",
    "            'false_positives': fp,\n",
    "            'false_negatives': fn\n",
    "        }\n",
    "        print(f\"  Precision: {precision:.3f}\")\n",
    "        print(f\"  Recall:    {recall:.3f}\")\n",
    "        print(f\"  F1-Score:  {f1:.3f}\")\n",
    "        print(f\"  TP: {tp}\")\n",
    "        print(f\"  FP: {fp}\")\n",
    "        print(f\"  FN: {fn}\")\n",
    "\n",
    "print(\"\\n✓ Tuned GradientBoostingClassifier evaluation complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313ded61",
   "metadata": {},
   "source": [
    "#### 5.5.4 Analyze XGBoostClassifier Error Cases\n",
    "Analyze False Positives and False Negatives for GradientBoostingClassifier to identify patterns for further improvement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e7647603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LR: GradientBoostingClassifier Error Cases Analysis\n",
      "================================================================================\n",
      "\n",
      "[FALSE NEGATIVES] (0 cases):\n",
      "  No false negatives found!\n",
      "\n",
      "[FALSE POSITIVES] (0 cases):\n",
      "  No false positives found!\n",
      "\n",
      "================================================================================\n",
      "LS: GradientBoostingClassifier Error Cases Analysis\n",
      "================================================================================\n",
      "\n",
      "[FALSE NEGATIVES] (1 cases):\n",
      "\n",
      "  4762702015|2015|L <-> 4762702015|2015|S\n",
      "    Left:  'steven tolleson' | Season: 2015 | Birth: 1983\n",
      "    Right: 'steve tolleson' | Season: 2015 | Birth: 1984\n",
      "    In candidates: True\n",
      "\n",
      "[FALSE POSITIVES] (1 cases):\n",
      "\n",
      "  5167142015|2015|L <-> 6458482015|2015|S (score: 1.000)\n",
      "    Left:  'dario alvarez' | Season: 2015 | Birth: 1989\n",
      "    Right: 'dariel alvarez' | Season: 2015 | Birth: 1989\n",
      "\n",
      "✓ GradientBoostingClassifier error analysis complete\n"
     ]
    }
   ],
   "source": [
    "# Analyze error cases for GradientBoostingClassifier (reusing code structure from 3.2)\n",
    "\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{edge_name}: GradientBoostingClassifier Error Cases Analysis\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    left_df, right_df = source_tables[edge_name]\n",
    "    val_df = splits[edge_name]['val'][['id1', 'id2', 'label']].copy()\n",
    "    correspondences = gb_matching_results[edge_name]\n",
    "    \n",
    "    # Get true matches and false matches in validation set\n",
    "    true_matches = val_df[val_df['label'].astype(str).str.strip().str.upper() == 'TRUE']\n",
    "    false_matches = val_df[val_df['label'].astype(str).str.strip().str.upper() == 'FALSE']\n",
    "    true_set = set(zip(true_matches['id1'], true_matches['id2']))\n",
    "    false_set = set(zip(false_matches['id1'], false_matches['id2']))\n",
    "    \n",
    "    # Get predicted matches (only those in validation set)\n",
    "    pred_set = set(zip(correspondences['id1'], correspondences['id2']))\n",
    "    val_set = set(zip(val_df['id1'], val_df['id2']))  # All pairs in validation set\n",
    "    \n",
    "    # False Negatives: True matches in validation set that were not predicted\n",
    "    fn_pairs = true_set - pred_set\n",
    "    print(f\"\\n[FALSE NEGATIVES] ({len(fn_pairs)} cases):\")\n",
    "    if len(fn_pairs) > 0:\n",
    "        for id1, id2 in fn_pairs:\n",
    "            left_rec = left_df[left_df['_rid'] == id1].iloc[0] if len(left_df[left_df['_rid'] == id1]) > 0 else None\n",
    "            right_rec = right_df[right_df['_rid'] == id2].iloc[0] if len(right_df[right_df['_rid'] == id2]) > 0 else None\n",
    "            \n",
    "            if left_rec is not None and right_rec is not None:\n",
    "                print(f\"\\n  {id1} <-> {id2}\")\n",
    "                print(f\"    Left:  '{left_rec.get('full_name', 'N/A')}' | Season: {left_rec.get('season_year', 'N/A')} | Birth: {left_rec.get('birth_year', 'N/A')}\")\n",
    "                print(f\"    Right: '{right_rec.get('full_name', 'N/A')}' | Season: {right_rec.get('season_year', 'N/A')} | Birth: {right_rec.get('birth_year', 'N/A')}\")\n",
    "                \n",
    "                # Check if in candidates\n",
    "                in_candidates = len(candidates[edge_name][\n",
    "                    (candidates[edge_name]['id1'] == id1) & \n",
    "                    (candidates[edge_name]['id2'] == id2)\n",
    "                ]) > 0\n",
    "                print(f\"    In candidates: {in_candidates}\")\n",
    "                \n",
    "                # Check if pair was scored by GB (even if below threshold)\n",
    "                score_row = correspondences[(correspondences['id1'] == id1) & (correspondences['id2'] == id2)]\n",
    "                if len(score_row) > 0:\n",
    "                    score = score_row['score'].iloc[0] if 'score' in score_row.columns else None\n",
    "                    score_str = f\"{score:.3f}\" if score is not None else \"N/A\"\n",
    "                    print(f\"    GB Score: {score_str}\")\n",
    "    else:\n",
    "        print(\"  No false negatives found!\")\n",
    "    \n",
    "    # False Positives: Predicted matches that are in validation set but labeled as FALSE\n",
    "    # Only analyze pairs that are in the validation set\n",
    "    fp_pairs = (pred_set & val_set) & false_set\n",
    "    print(f\"\\n[FALSE POSITIVES] ({len(fp_pairs)} cases):\")\n",
    "    if len(fp_pairs) > 0:\n",
    "        for id1, id2 in fp_pairs:\n",
    "            left_rec = left_df[left_df['_rid'] == id1].iloc[0] if len(left_df[left_df['_rid'] == id1]) > 0 else None\n",
    "            right_rec = right_df[right_df['_rid'] == id2].iloc[0] if len(right_df[right_df['_rid'] == id2]) > 0 else None\n",
    "            score_row = correspondences[(correspondences['id1'] == id1) & (correspondences['id2'] == id2)]\n",
    "            score = score_row['score'].iloc[0] if len(score_row) > 0 and 'score' in score_row.columns else None\n",
    "            \n",
    "            if left_rec is not None and right_rec is not None:\n",
    "                score_str = f\"{score:.3f}\" if score is not None else \"N/A\"\n",
    "                print(f\"\\n  {id1} <-> {id2} (score: {score_str})\")\n",
    "                print(f\"    Left:  '{left_rec.get('full_name', 'N/A')}' | Season: {left_rec.get('season_year', 'N/A')} | Birth: {left_rec.get('birth_year', 'N/A')}\")\n",
    "                print(f\"    Right: '{right_rec.get('full_name', 'N/A')}' | Season: {right_rec.get('season_year', 'N/A')} | Birth: {right_rec.get('birth_year', 'N/A')}\")\n",
    "    else:\n",
    "        print(\"  No false positives found!\")\n",
    "\n",
    "print(\"\\n✓ GradientBoostingClassifier error analysis complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e168fb",
   "metadata": {},
   "source": [
    "### 4.8 Error Analysis Summary and Improvement Plan\n",
    "\n",
    "Based on the error analysis, we identify the following patterns and propose targeted improvements:\n",
    "\n",
    "#### **LR Edge Error Patterns:**\n",
    "\n",
    "1. **Blocking Issues (2 cases):**\n",
    "   - `dan vogelbach` vs `daniel vogelbach` (2 instances) - Not in candidate pairs\n",
    "   - **Root Cause:** The Enhanced TokenBlocker is missing these name variant pairs during blocking phase\n",
    "   - **Impact:** These are true matches that never reach the matching stage\n",
    "\n",
    "2. **Matching Issues (1 case):**\n",
    "   - `jonathon niese` vs `jon niese` - In candidates but missed by GradientBoostingClassifier\n",
    "   - **Root Cause:** ML model not recognizing common name variants despite high name similarity\n",
    "   - **Impact:** True match filtered out by classifier\n",
    "\n",
    "3. **False Positives:** 0 cases (Perfect Precision)\n",
    "\n",
    "#### **LS Edge Error Patterns:**\n",
    "\n",
    "1. **Matching Issues - False Negatives (1 case):**\n",
    "   - `dan robertson` vs `daniel robertson` - In candidates but missed by classifier\n",
    "   - **Root Cause:** Same as LR - name variant not recognized by ML model\n",
    "\n",
    "2. **Matching Issues - False Positives (3 cases, all score=1.000):**\n",
    "   - `josh rojas` vs `jose rojas` (birth years: 1994 vs 1993)\n",
    "   - `kevan smith` vs `kevin smith` (birth years: 1988 vs 1997)\n",
    "   - `matt duffy` vs `matt duffy` (birth years: 1989 vs 1991)\n",
    "   - **Root Cause:** Model overconfident on name similarity, insufficiently penalizing birth year differences\n",
    "   - **Impact:** High-confidence false matches reduce precision\n",
    "\n",
    "#### **Proposed Improvement Strategies:**\n",
    "\n",
    "**Strategy 1: Enhance Feature Engineering for ML Models**\n",
    "- **Add Birth Year Difference Feature:** Calculate `abs(birth_year_left - birth_year_right)` as an explicit feature\n",
    "- **Add Name Variant Indicator:** Create binary feature indicating if names are known variants (using `NAME_VARIANTS` dictionary)\n",
    "- **Action:** Extend `ml_comparators` to include `DateComparator` for birth_year, or add custom feature extraction\n",
    "\n",
    "**Strategy 2: Improve Blocking for Name Variants (LR Edge)**\n",
    "- **Enhance TokenBlocker:** Investigate why `dan`/`daniel` variants are missed in blocking phase\n",
    "- **Action:** Review `normalize_name_for_blocking` function or add name variant expansion to blocking keys\n",
    "\n",
    "**Strategy 3: Hyperparameter Tuning**\n",
    "- **Adjust GradientBoostingClassifier:** Fine-tune parameters to better balance name similarity vs. birth year constraints\n",
    "- **Action:** Grid search or Bayesian optimization for optimal parameters\n",
    "\n",
    "**Strategy 4: Ensemble Methods**\n",
    "- **Combine Models:** Use voting or stacking to combine RandomForest and GradientBoosting predictions\n",
    "- **Action:** Implement ensemble matcher that leverages strengths of both models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40af5094",
   "metadata": {},
   "source": [
    "### 5.5 Train and Apply XGBoostClassifier Matcher\n",
    "\n",
    "Train a gradient-boosted tree model (XGBoost) on the same feature set to capture non-linear interactions beyond RandomForest/GradientBoosting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c5727b",
   "metadata": {},
   "source": [
    "#### 5.5.1 Train XGBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "9612ab22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LR: Training XGBoostClassifier ===\n",
      "  Training pairs: 304\n",
      "  True matches: 90\n",
      "  False matches: 214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Label distribution: 90 positive, 214 negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Label distribution: 142 positive, 154 negative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top feature importances:\n",
      " is_name_variant: 0.5842\n",
      " birth_year_diff: 0.2692\n",
      " StringComparator(full_name_normalized, levenshtein, tokenization=char, list_strategy=None): 0.0739\n",
      " StringComparator(full_name_normalized, jaccard, tokenization=word, list_strategy=None): 0.0694\n",
      " phonetic_soundex: 0.0034\n",
      " DateComparator(birth_year, list_strategy=None): 0.0000\n",
      " XGBoostClassifier trained for LR\n",
      "=== LS: Training XGBoostClassifier ===\n",
      "  Training pairs: 296\n",
      "  True matches: 142\n",
      "  False matches: 154\n",
      "Top feature importances:\n",
      " is_name_variant: 0.3946\n",
      " birth_year_diff: 0.2974\n",
      " StringComparator(full_name_normalized, levenshtein, tokenization=char, list_strategy=None): 0.1367\n",
      " StringComparator(full_name_normalized, jaccard, tokenization=word, list_strategy=None): 0.1082\n",
      " DateComparator(birth_year, list_strategy=None): 0.0329\n",
      " phonetic_soundex: 0.0304\n",
      " XGBoostClassifier trained for LS\n",
      "XGBoostClassifier training complete for all edges\n"
     ]
    }
   ],
   "source": [
    "# Train XGBoost-based matchers for each edge\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_classifiers = {}\n",
    "xgb_matchers = {}\n",
    "\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    print(f\"=== {edge_name}: Training XGBoostClassifier ===\")\n",
    "    \n",
    "    left_df, right_df = source_tables[edge_name]\n",
    "    train_df = splits[edge_name]['train'][['id1', 'id2', 'label']].copy()\n",
    "    train_df['label'] = train_df['label'].astype(str).str.strip().str.upper()\n",
    "    train_df['label_binary'] = (train_df['label'] == 'TRUE').astype(int)\n",
    "    \n",
    "    print(f\"  Training pairs: {len(train_df)}\")\n",
    "    print(f\"  True matches: {train_df['label_binary'].sum()}\")\n",
    "    print(f\"  False matches: {len(train_df) - train_df['label_binary'].sum()}\")\n",
    "    \n",
    "    train_features = ml_feature_extractor.create_features(\n",
    "        df_left=left_df,\n",
    "        df_right=right_df,\n",
    "        pairs=train_df[['id1', 'id2']],\n",
    "        labels=train_df['label_binary'],\n",
    "        id_column='_rid'\n",
    "    )\n",
    "    \n",
    "    feature_columns = [col for col in train_features.columns if col not in ['id1', 'id2', 'label']]\n",
    "    X_train = train_features[feature_columns]\n",
    "    y_train = train_features['label']\n",
    "    \n",
    "    clf = XGBClassifier(\n",
    "        n_estimators=400,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        reg_lambda=1.0,\n",
    "        reg_alpha=0.0,\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='logloss',\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    xgb_classifiers[edge_name] = clf\n",
    "    xgb_matchers[edge_name] = MLBasedMatcher(ml_feature_extractor)\n",
    "    \n",
    "    importance = sorted(\n",
    "        zip(feature_columns, clf.feature_importances_),\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )\n",
    "    print(\"Top feature importances:\")\n",
    "    for feat, val in importance:\n",
    "        print(f\" {feat}: {val:.4f}\")\n",
    "    print(f\" XGBoostClassifier trained for {edge_name}\")\n",
    "\n",
    "print(\"XGBoostClassifier training complete for all edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce15b21",
   "metadata": {},
   "source": [
    "#### 5.5.2 Hyperparameter Tuning for XGBoostClassifier\n",
    "\n",
    "Use RandomizedSearchCV to find optimal hyperparameters for XGBoostClassifier. This uses the **validation set** for tuning (NOT the test set). RandomizedSearchCV is faster than GridSearchCV while still exploring a good parameter space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "36645ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "XGBoost Hyperparameter Tuning (Using Validation Set)\n",
      "================================================================================\n",
      "Using validation set for tuning. Test set will be used only for final evaluation.\n",
      "\n",
      "\n",
      "================================================================================\n",
      "LR Edge: Hyperparameter Tuning\n",
      "================================================================================\n",
      "  Training pairs: 304\n",
      "  Validation pairs: 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Label distribution: 90 positive, 214 negative\n",
      "[INFO ] root - Label distribution: 30 positive, 66 negative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Feature dimensions: 6\n",
      "  Training: 304 samples\n",
      "  Validation: 96 samples\n",
      "\n",
      "  Running RandomizedSearchCV (n_iter=15 for speed)...\n",
      "Fitting 3 folds for each of 15 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Label distribution: 142 positive, 154 negative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Best cross-validation F1 score: 1.0000\n",
      "  Best parameters:\n",
      "    colsample_bytree: 0.8\n",
      "    learning_rate: 0.05\n",
      "    max_depth: 8\n",
      "    min_child_weight: 1\n",
      "    n_estimators: 500\n",
      "    reg_alpha: 1.0\n",
      "    subsample: 0.9\n",
      "\n",
      "  Validation set F1 score (best model): 1.0000\n",
      "  Validation set F1 score (default model): 1.0000\n",
      "  Improvement: +0.0000 (+0.00%)\n",
      "\n",
      "  ✓ Hyperparameter tuning complete for LR\n",
      "\n",
      "================================================================================\n",
      "LS Edge: Hyperparameter Tuning\n",
      "================================================================================\n",
      "  Training pairs: 296\n",
      "  Validation pairs: 101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Label distribution: 49 positive, 52 negative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Feature dimensions: 6\n",
      "  Training: 296 samples\n",
      "  Validation: 101 samples\n",
      "\n",
      "  Running RandomizedSearchCV (n_iter=15 for speed)...\n",
      "Fitting 3 folds for each of 15 candidates, totalling 45 fits\n",
      "\n",
      "  Best cross-validation F1 score: 0.9821\n",
      "  Best parameters:\n",
      "    colsample_bytree: 0.8\n",
      "    learning_rate: 0.05\n",
      "    max_depth: 8\n",
      "    min_child_weight: 1\n",
      "    n_estimators: 500\n",
      "    reg_alpha: 1.0\n",
      "    subsample: 0.9\n",
      "\n",
      "  Validation set F1 score (best model): 0.9800\n",
      "  Validation set F1 score (default model): 0.9697\n",
      "  Improvement: +0.0103 (+1.06%)\n",
      "\n",
      "  ✓ Hyperparameter tuning complete for LS\n",
      "\n",
      "================================================================================\n",
      "✓ XGBoost hyperparameter tuning complete for all edges\n",
      "================================================================================\n",
      "\n",
      "Tuning Summary:\n",
      "--------------------------------------------------------------------------------\n",
      "LR:\n",
      "  Default F1: 1.0000\n",
      "  Tuned F1:   1.0000\n",
      "  Improvement: +0.0000 (+0.00%)\n",
      "LS:\n",
      "  Default F1: 0.9697\n",
      "  Tuned F1:   0.9800\n",
      "  Improvement: +0.0103 (+1.06%)\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning for XGBoostClassifier using validation set\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Define parameter distribution for RandomizedSearchCV\n",
    "xgb_param_dist = {\n",
    "    'max_depth': [4, 5, 6, 7, 8],\n",
    "    'learning_rate': [0.05, 0.1, 0.15],\n",
    "    'n_estimators': [300, 400, 500],\n",
    "    'reg_alpha': [0, 0.1, 0.5, 1.0],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "# F1 score scorer for optimization\n",
    "f1_scorer = make_scorer(f1_score)\n",
    "\n",
    "# Dictionary to store tuned classifiers\n",
    "xgb_tuned_classifiers = {}\n",
    "xgb_tuning_results = {}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"XGBoost Hyperparameter Tuning (Using Validation Set)\")\n",
    "print(\"=\"*80)\n",
    "print(\"Using validation set for tuning. Test set will be used only for final evaluation.\\n\")\n",
    "\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{edge_name} Edge: Hyperparameter Tuning\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    left_df, right_df = source_tables[edge_name]\n",
    "    \n",
    "    # Prepare training data\n",
    "    train_df = splits[edge_name]['train'][['id1', 'id2', 'label']].copy()\n",
    "    train_df['label'] = train_df['label'].astype(str).str.strip().str.upper()\n",
    "    train_df['label_binary'] = (train_df['label'] == 'TRUE').astype(int)\n",
    "    \n",
    "    # Prepare validation data\n",
    "    val_df = splits[edge_name]['val'][['id1', 'id2', 'label']].copy()\n",
    "    val_df['label'] = val_df['label'].astype(str).str.strip().str.upper()\n",
    "    val_df['label_binary'] = (val_df['label'] == 'TRUE').astype(int)\n",
    "    \n",
    "    print(f\"  Training pairs: {len(train_df)}\")\n",
    "    print(f\"  Validation pairs: {len(val_df)}\")\n",
    "    \n",
    "    # Extract features for training\n",
    "    train_features = ml_feature_extractor.create_features(\n",
    "        df_left=left_df,\n",
    "        df_right=right_df,\n",
    "        pairs=train_df[['id1', 'id2']],\n",
    "        labels=train_df['label_binary'],\n",
    "        id_column='_rid'\n",
    "    )\n",
    "    \n",
    "    # Extract features for validation\n",
    "    val_features = ml_feature_extractor.create_features(\n",
    "        df_left=left_df,\n",
    "        df_right=right_df,\n",
    "        pairs=val_df[['id1', 'id2']],\n",
    "        labels=val_df['label_binary'],\n",
    "        id_column='_rid'\n",
    "    )\n",
    "    \n",
    "    feature_columns = [col for col in train_features.columns if col not in ['id1', 'id2', 'label']]\n",
    "    X_train = train_features[feature_columns]\n",
    "    y_train = train_features['label']\n",
    "    X_val = val_features[feature_columns]\n",
    "    y_val = val_features['label']\n",
    "    \n",
    "    print(f\"  Feature dimensions: {X_train.shape[1]}\")\n",
    "    print(f\"  Training: {len(X_train)} samples\")\n",
    "    print(f\"  Validation: {len(X_val)} samples\")\n",
    "    \n",
    "    # Base XGBoost classifier\n",
    "    base_clf = XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='logloss',\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # RandomizedSearchCV with limited iterations for speed\n",
    "    print(f\"\\n  Running RandomizedSearchCV (n_iter=15 for speed)...\")\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=base_clf,\n",
    "        param_distributions=xgb_param_dist,\n",
    "        n_iter=15,  # Limited iterations for faster execution\n",
    "        scoring=f1_scorer,\n",
    "        cv=3,  # 3-fold cross-validation on training set\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Fit on training data\n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get best parameters\n",
    "    best_params = random_search.best_params_\n",
    "    best_score = random_search.best_score_\n",
    "    \n",
    "    print(f\"\\n  Best cross-validation F1 score: {best_score:.4f}\")\n",
    "    print(f\"  Best parameters:\")\n",
    "    for param, value in sorted(best_params.items()):\n",
    "        print(f\"    {param}: {value}\")\n",
    "    \n",
    "    # Evaluate on validation set with best model\n",
    "    best_clf = random_search.best_estimator_\n",
    "    y_val_pred = best_clf.predict(X_val)\n",
    "    val_f1 = f1_score(y_val, y_val_pred)\n",
    "    \n",
    "    print(f\"\\n  Validation set F1 score (best model): {val_f1:.4f}\")\n",
    "    \n",
    "    # Compare with default model\n",
    "    default_clf = XGBClassifier(\n",
    "        n_estimators=400,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        reg_lambda=1.0,\n",
    "        reg_alpha=0.0,\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='logloss',\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    default_clf.fit(X_train, y_train)\n",
    "    y_val_pred_default = default_clf.predict(X_val)\n",
    "    val_f1_default = f1_score(y_val, y_val_pred_default)\n",
    "    \n",
    "    print(f\"  Validation set F1 score (default model): {val_f1_default:.4f}\")\n",
    "    improvement = val_f1 - val_f1_default\n",
    "    print(f\"  Improvement: {improvement:+.4f} ({improvement/val_f1_default*100:+.2f}%)\")\n",
    "    \n",
    "    # Store results (separate from default model)\n",
    "    xgb_tuned_classifiers[edge_name] = best_clf\n",
    "    xgb_tuning_results[edge_name] = {\n",
    "        'best_params': best_params,\n",
    "        'best_cv_score': best_score,\n",
    "        'val_f1': val_f1,\n",
    "        'val_f1_default': val_f1_default,\n",
    "        'improvement': improvement\n",
    "    }\n",
    "    \n",
    "    # Note: Do NOT update xgb_classifiers here - keep default model separate\n",
    "    \n",
    "    print(f\"\\n  ✓ Hyperparameter tuning complete for {edge_name}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"✓ XGBoost hyperparameter tuning complete for all edges\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nTuning Summary:\")\n",
    "print(\"-\" * 80)\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    results = xgb_tuning_results[edge_name]\n",
    "    print(f\"{edge_name}:\")\n",
    "    print(f\"  Default F1: {results['val_f1_default']:.4f}\")\n",
    "    print(f\"  Tuned F1:   {results['val_f1']:.4f}\")\n",
    "    print(f\"  Improvement: {results['improvement']:+.4f} ({results['improvement']/results['val_f1_default']*100:+.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41848ead",
   "metadata": {},
   "source": [
    "#### 5.5.3 Apply XGBoostClassifier Matcher (Default)\n",
    "\n",
    "Filter by the same season_year constraint, then score candidate pairs with the default XGBoost model (from 5.5.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a5b0aed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LR: XGBoostClassifier Matching ===\n",
      "  Filtering candidate pairs by season_year constraint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Starting Entity Matching\n",
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Blocking 106553 x 15215 elements\n",
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Matching 106553 x 15215 elements after 0:00:0.000; 135944 blocked pairs (reduction ratio: 0.9999161462661056)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  After season_year filter: 135,944 candidate pairs (from 5,994,373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Entity Matching finished after 0:00:49.250; found 15429 correspondences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 15,429 matched pairs\n",
      "  Score range: [1.000, 1.000]\n",
      "  Mean score: 1.000\n",
      "=== LS: XGBoostClassifier Matching ===\n",
      "  Filtering candidate pairs by season_year constraint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Starting Entity Matching\n",
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Blocking 106553 x 6743 elements\n",
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Matching 106553 x 6743 elements after 0:00:0.000; 9729 blocked pairs (reduction ratio: 0.9999864590429076)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  After season_year filter: 9,729 candidate pairs (from 227,185)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Entity Matching finished after 0:00:3.244; found 6717 correspondences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 6,717 matched pairs\n",
      "  Score range: [1.000, 1.000]\n",
      "  Mean score: 1.000\n",
      "XGBoostClassifier matching complete for all edges\n"
     ]
    }
   ],
   "source": [
    "# Apply XGBoostClassifier-based matcher to candidate pairs\n",
    "xgb_matching_results = {}\n",
    "\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    print(f\"=== {edge_name}: XGBoostClassifier Matching ===\")\n",
    "    \n",
    "    left_df, right_df = source_tables[edge_name]\n",
    "    cand_df = candidates[edge_name].copy()\n",
    "    matcher = xgb_matchers[edge_name]\n",
    "    clf = xgb_classifiers[edge_name]\n",
    "    \n",
    "    print(\"  Filtering candidate pairs by season_year constraint...\")\n",
    "    cand_with_seasons = cand_df.merge(\n",
    "        left_df[['_rid', 'season_year']],\n",
    "        left_on='id1',\n",
    "        right_on='_rid',\n",
    "        how='left'\n",
    "    ).merge(\n",
    "        right_df[['_rid', 'season_year']],\n",
    "        left_on='id2',\n",
    "        right_on='_rid',\n",
    "        how='left',\n",
    "        suffixes=('', '_right')\n",
    "    )\n",
    "    \n",
    "    cand_df_filtered = cand_with_seasons[\n",
    "        (cand_with_seasons['season_year'].notna()) &\n",
    "        (cand_with_seasons['season_year_right'].notna()) &\n",
    "        (cand_with_seasons['season_year'] == cand_with_seasons['season_year_right'])\n",
    "    ][['id1', 'id2']].copy()\n",
    "    \n",
    "    print(f\"  After season_year filter: {len(cand_df_filtered):,} candidate pairs (from {len(cand_df):,})\")\n",
    "    correspondences = matcher.match(\n",
    "        df_left=left_df,\n",
    "        df_right=right_df,\n",
    "        candidates=cand_df_filtered,\n",
    "        id_column='_rid',\n",
    "        trained_classifier=clf\n",
    "    )\n",
    "    \n",
    "    xgb_matching_results[edge_name] = correspondences\n",
    "    \n",
    "    print(f\"  Generated {len(correspondences):,} matched pairs\")\n",
    "    if 'score' in correspondences.columns:\n",
    "        print(f\"  Score range: [{correspondences['score'].min():.3f}, {correspondences['score'].max():.3f}]\")\n",
    "        print(f\"  Mean score: {correspondences['score'].mean():.3f}\")\n",
    "\n",
    "print(\"XGBoostClassifier matching complete for all edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bf51f3",
   "metadata": {},
   "source": [
    "#### 5.5.4 Evaluate XGBoostClassifier Matching (Default)\n",
    "\n",
    "Assess validation performance of the default XGBoost model with the PyDI evaluator, falling back to manual metrics if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "52e51e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: XGBoostClassifier Evaluation (Validation Set) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Confusion Matrix:\n",
      "[INFO ] root -   True Positives:  30\n",
      "[INFO ] root -   True Negatives:  66\n",
      "[INFO ] root -   False Positives: 0\n",
      "[INFO ] root -   False Negatives: 0\n",
      "[INFO ] root - Performance Metrics:\n",
      "[INFO ] root -   Accuracy:  1.000\n",
      "[INFO ] root -   Precision: 1.000\n",
      "[INFO ] root -   Recall:    1.000\n",
      "[INFO ] root -   F1-Score:  1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Precision: 1.000\n",
      "  Recall:    1.000\n",
      "  F1-Score:  1.000\n",
      "  TP: 30\n",
      "  FP: 0\n",
      "  FN: 0\n",
      "LS: XGBoostClassifier Evaluation (Validation Set) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Confusion Matrix:\n",
      "[INFO ] root -   True Positives:  48\n",
      "[INFO ] root -   True Negatives:  45\n",
      "[INFO ] root -   False Positives: 2\n",
      "[INFO ] root -   False Negatives: 1\n",
      "[INFO ] root - Performance Metrics:\n",
      "[INFO ] root -   Accuracy:  0.969\n",
      "[INFO ] root -   Precision: 0.960\n",
      "[INFO ] root -   Recall:    0.980\n",
      "[INFO ] root -   F1-Score:  0.970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Precision: 0.960\n",
      "  Recall:    0.980\n",
      "  F1-Score:  0.970\n",
      "  TP: 48\n",
      "  FP: 2\n",
      "  FN: 1\n",
      "XGBoostClassifier evaluation complete\n"
     ]
    }
   ],
   "source": [
    "# Evaluate XGBoostClassifier matching on validation set\n",
    "xgb_matching_metrics_val = {}\n",
    "\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    print(f\"{edge_name}: XGBoostClassifier Evaluation (Validation Set) ===\")\n",
    "    \n",
    "    correspondences = xgb_matching_results[edge_name]\n",
    "    val_df = splits[edge_name]['val'][['id1', 'id2', 'label']].copy()\n",
    "    \n",
    "    correspondences_for_eval = correspondences.copy()\n",
    "    if 'score' not in correspondences_for_eval.columns and 'sim' in correspondences_for_eval.columns:\n",
    "        correspondences_for_eval = correspondences_for_eval.rename(columns={'sim': 'score'})\n",
    "    \n",
    "    try:\n",
    "        eval_results = EntityMatchingEvaluator.evaluate_matching(\n",
    "            correspondences=correspondences_for_eval,\n",
    "            test_pairs=val_df,\n",
    "            out_dir=OUTPUT_DIR / 'matching-evaluation-xgb',\n",
    "            matcher_instance=xgb_matchers[edge_name]\n",
    "        )\n",
    "        xgb_matching_metrics_val[edge_name] = eval_results\n",
    "        \n",
    "        print(f\"  Precision: {eval_results.get('precision', 0.0):.3f}\")\n",
    "        print(f\"  Recall:    {eval_results.get('recall', 0.0):.3f}\")\n",
    "        print(f\"  F1-Score:  {eval_results.get('f1', 0.0):.3f}\")\n",
    "        print(f\"  TP: {eval_results.get('true_positives', 0)}\")\n",
    "        print(f\"  FP: {eval_results.get('false_positives', 0)}\")\n",
    "        print(f\"  FN: {eval_results.get('false_negatives', 0)}\")\n",
    "    except Exception as exc:\n",
    "        print(f\"  PyDI evaluator failed: {exc}\")\n",
    "        true_matches = val_df[val_df['label'].astype(str).str.strip().str.upper() == 'TRUE']\n",
    "        true_set = set(zip(true_matches['id1'], true_matches['id2']))\n",
    "        pred_set = set(zip(correspondences['id1'], correspondences['id2']))\n",
    "        tp = len(true_set & pred_set)\n",
    "        fp = len(pred_set - true_set)\n",
    "        fn = len(true_set - pred_set)\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        xgb_matching_metrics_val[edge_name] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'true_positives': tp,\n",
    "            'false_positives': fp,\n",
    "            'false_negatives': fn\n",
    "        }\n",
    "        print(f\"  Precision: {precision:.3f}\")\n",
    "        print(f\"  Recall:    {recall:.3f}\")\n",
    "        print(f\"  F1-Score:  {f1:.3f}\")\n",
    "        print(f\"  TP: {tp}\")\n",
    "        print(f\"  FP: {fp}\")\n",
    "        print(f\"  FN: {fn}\")\n",
    "\n",
    "print(\"XGBoostClassifier evaluation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccb8acb",
   "metadata": {},
   "source": [
    "#### 5.5.6 Apply Tuned XGBoostClassifier Matcher\n",
    "\n",
    "Apply the tuned XGBoostClassifier model to candidate pairs. This allows comparison with the default model results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "74ec1b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LR: Tuned XGBoostClassifier Matching ===\n",
      "  Filtering candidate pairs by season_year constraint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Starting Entity Matching\n",
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Blocking 106553 x 15215 elements\n",
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Matching 106553 x 15215 elements after 0:00:0.001; 135944 blocked pairs (reduction ratio: 0.9999161462661056)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  After season_year filter: 135,944 candidate pairs (from 5,994,373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Entity Matching finished after 0:00:53.862; found 15551 correspondences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 15,551 matched pairs\n",
      "  Score range: [1.000, 1.000]\n",
      "  Mean score: 1.000\n",
      "=== LS: Tuned XGBoostClassifier Matching ===\n",
      "  Filtering candidate pairs by season_year constraint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Starting Entity Matching\n",
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Blocking 106553 x 6743 elements\n",
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Matching 106553 x 6743 elements after 0:00:0.000; 9729 blocked pairs (reduction ratio: 0.9999864590429076)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  After season_year filter: 9,729 candidate pairs (from 227,185)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Entity Matching finished after 0:00:3.816; found 6741 correspondences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 6,741 matched pairs\n",
      "  Score range: [1.000, 1.000]\n",
      "  Mean score: 1.000\n",
      "\n",
      "✓ Tuned XGBoostClassifier matching complete for all edges\n"
     ]
    }
   ],
   "source": [
    "# Apply tuned XGBoostClassifier to candidate pairs\n",
    "xgb_tuned_matching_results = {}\n",
    "xgb_tuned_matchers = {}\n",
    "\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    print(f\"=== {edge_name}: Tuned XGBoostClassifier Matching ===\")\n",
    "    \n",
    "    left_df, right_df = source_tables[edge_name]\n",
    "    cand_df = candidates[edge_name].copy()\n",
    "    clf = xgb_tuned_classifiers[edge_name]\n",
    "    \n",
    "    # Create matcher with tuned classifier\n",
    "    matcher = MLBasedMatcher(ml_feature_extractor)\n",
    "    xgb_tuned_matchers[edge_name] = matcher\n",
    "    \n",
    "    print(\"  Filtering candidate pairs by season_year constraint...\")\n",
    "    cand_with_seasons = cand_df.merge(\n",
    "        left_df[['_rid', 'season_year']],\n",
    "        left_on='id1', right_on='_rid', how='left'\n",
    "    ).merge(\n",
    "        right_df[['_rid', 'season_year']],\n",
    "        left_on='id2', right_on='_rid', how='left', suffixes=('', '_right')\n",
    "    )\n",
    "    cand_df_filtered = cand_with_seasons[\n",
    "        (cand_with_seasons['season_year'].notna()) & \n",
    "        (cand_with_seasons['season_year_right'].notna()) &\n",
    "        (cand_with_seasons['season_year'] == cand_with_seasons['season_year_right'])\n",
    "    ][['id1', 'id2']].copy()\n",
    "    \n",
    "    print(f\"  After season_year filter: {len(cand_df_filtered):,} candidate pairs (from {len(cand_df):,})\")\n",
    "    \n",
    "    correspondences = matcher.match(\n",
    "        df_left=left_df,\n",
    "        df_right=right_df,\n",
    "        candidates=cand_df_filtered,\n",
    "        id_column='_rid',\n",
    "        trained_classifier=clf\n",
    "    )\n",
    "    \n",
    "    xgb_tuned_matching_results[edge_name] = correspondences\n",
    "    \n",
    "    print(f\"  Generated {len(correspondences):,} matched pairs\")\n",
    "    if 'score' in correspondences.columns:\n",
    "        print(f\"  Score range: [{correspondences['score'].min():.3f}, {correspondences['score'].max():.3f}]\")\n",
    "        print(f\"  Mean score: {correspondences['score'].mean():.3f}\")\n",
    "\n",
    "print(\"\\n✓ Tuned XGBoostClassifier matching complete for all edges\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0f3dd7",
   "metadata": {},
   "source": [
    "#### 5.5.7 Evaluate Tuned XGBoostClassifier Matching\n",
    "\n",
    "Evaluate the tuned model performance on validation set and compare with default model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c3278d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Tuned XGBoostClassifier Evaluation (Validation Set)\n",
      "================================================================================\n",
      "\n",
      "=== LR: Tuned XGBoostClassifier Evaluation ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Confusion Matrix:\n",
      "[INFO ] root -   True Positives:  30\n",
      "[INFO ] root -   True Negatives:  66\n",
      "[INFO ] root -   False Positives: 0\n",
      "[INFO ] root -   False Negatives: 0\n",
      "[INFO ] root - Performance Metrics:\n",
      "[INFO ] root -   Accuracy:  1.000\n",
      "[INFO ] root -   Precision: 1.000\n",
      "[INFO ] root -   Recall:    1.000\n",
      "[INFO ] root -   F1-Score:  1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Precision: 1.000\n",
      "  Recall:    1.000\n",
      "  F1-Score:  1.000\n",
      "  TP: 30\n",
      "  FP: 0\n",
      "  FN: 0\n",
      "\n",
      "  Comparison with Default Model:\n",
      "    Default F1: 1.000\n",
      "    Tuned F1:   1.000\n",
      "    Improvement: +0.0000\n",
      "\n",
      "=== LS: Tuned XGBoostClassifier Evaluation ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Confusion Matrix:\n",
      "[INFO ] root -   True Positives:  49\n",
      "[INFO ] root -   True Negatives:  45\n",
      "[INFO ] root -   False Positives: 2\n",
      "[INFO ] root -   False Negatives: 0\n",
      "[INFO ] root - Performance Metrics:\n",
      "[INFO ] root -   Accuracy:  0.979\n",
      "[INFO ] root -   Precision: 0.961\n",
      "[INFO ] root -   Recall:    1.000\n",
      "[INFO ] root -   F1-Score:  0.980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Precision: 0.961\n",
      "  Recall:    1.000\n",
      "  F1-Score:  0.980\n",
      "  TP: 49\n",
      "  FP: 2\n",
      "  FN: 0\n",
      "\n",
      "  Comparison with Default Model:\n",
      "    Default F1: 0.970\n",
      "    Tuned F1:   0.980\n",
      "    Improvement: +0.0103\n",
      "    Relative improvement: +1.06%\n",
      "\n",
      "✓ Tuned XGBoostClassifier evaluation complete\n"
     ]
    }
   ],
   "source": [
    "# Evaluate tuned XGBoostClassifier matching\n",
    "xgb_tuned_matching_metrics_val = {}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Tuned XGBoostClassifier Evaluation (Validation Set)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    print(f\"\\n=== {edge_name}: Tuned XGBoostClassifier Evaluation ===\")\n",
    "    \n",
    "    correspondences = xgb_tuned_matching_results[edge_name]\n",
    "    val_df = splits[edge_name]['val'][['id1', 'id2', 'label']].copy()\n",
    "    \n",
    "    correspondences_for_eval = correspondences.copy()\n",
    "    if 'score' not in correspondences_for_eval.columns and 'sim' in correspondences_for_eval.columns:\n",
    "        correspondences_for_eval = correspondences_for_eval.rename(columns={'sim': 'score'})\n",
    "    \n",
    "    try:\n",
    "        eval_results = EntityMatchingEvaluator.evaluate_matching(\n",
    "            correspondences=correspondences_for_eval,\n",
    "            test_pairs=val_df,\n",
    "            out_dir=OUTPUT_DIR / 'matching-evaluation-xgb-tuned',\n",
    "            matcher_instance=xgb_tuned_matchers[edge_name]\n",
    "        )\n",
    "        xgb_tuned_matching_metrics_val[edge_name] = eval_results\n",
    "        \n",
    "        print(f\"  Precision: {eval_results.get('precision', 0.0):.3f}\")\n",
    "        print(f\"  Recall:    {eval_results.get('recall', 0.0):.3f}\")\n",
    "        print(f\"  F1-Score:  {eval_results.get('f1', 0.0):.3f}\")\n",
    "        print(f\"  TP: {eval_results.get('true_positives', 0)}\")\n",
    "        print(f\"  FP: {eval_results.get('false_positives', 0)}\")\n",
    "        print(f\"  FN: {eval_results.get('false_negatives', 0)}\")\n",
    "        \n",
    "        # Compare with default model\n",
    "        default_metrics = xgb_matching_metrics_val.get(edge_name, {})\n",
    "        if default_metrics:\n",
    "            print(f\"\\n  Comparison with Default Model:\")\n",
    "            print(f\"    Default F1: {default_metrics.get('f1', 0.0):.3f}\")\n",
    "            print(f\"    Tuned F1:   {eval_results.get('f1', 0.0):.3f}\")\n",
    "            improvement = eval_results.get('f1', 0.0) - default_metrics.get('f1', 0.0)\n",
    "            print(f\"    Improvement: {improvement:+.4f}\")\n",
    "            if improvement > 0:\n",
    "                print(f\"    Relative improvement: {improvement/default_metrics.get('f1', 0.0)*100:+.2f}%\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Evaluation failed: {e}\")\n",
    "        # Fallback to manual calculation\n",
    "        true_matches = val_df[val_df['label'].astype(str).str.strip().str.upper() == 'TRUE']\n",
    "        true_set = set(zip(true_matches['id1'], true_matches['id2']))\n",
    "        pred_set = set(zip(correspondences['id1'], correspondences['id2']))\n",
    "        tp = len(true_set & pred_set)\n",
    "        fp = len(pred_set - true_set)\n",
    "        fn = len(true_set - pred_set)\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        xgb_tuned_matching_metrics_val[edge_name] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'true_positives': tp,\n",
    "            'false_positives': fp,\n",
    "            'false_negatives': fn\n",
    "        }\n",
    "        print(f\"  Precision: {precision:.3f}\")\n",
    "        print(f\"  Recall:    {recall:.3f}\")\n",
    "        print(f\"  F1-Score:  {f1:.3f}\")\n",
    "        print(f\"  TP: {tp}\")\n",
    "        print(f\"  FP: {fp}\")\n",
    "        print(f\"  FN: {fn}\")\n",
    "\n",
    "print(\"\\n✓ Tuned XGBoostClassifier evaluation complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ed0c92",
   "metadata": {},
   "source": [
    "#### 5.5.5 Analyze XGBoostClassifier Error Cases\n",
    "\n",
    "Reuse the earlier diagnostic template to inspect false negatives / positives for the XGBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e538d03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LR: XGBoostClassifier Error Cases Analysis\n",
      "================================================================================\n",
      "[FALSE NEGATIVES] (0 cases):\n",
      "  No false negatives found!\n",
      "[FALSE POSITIVES] (0 cases):\n",
      "  No false positives found!\n",
      "================================================================================\n",
      "LS: XGBoostClassifier Error Cases Analysis\n",
      "================================================================================\n",
      "[FALSE NEGATIVES] (1 cases):\n",
      "4762702015|2015|L <-> 4762702015|2015|S\n",
      "Left:  'steven tolleson' | Season: 2015 | Birth: 1983\n",
      "Right: 'steve tolleson' | Season: 2015 | Birth: 1984\n",
      "In candidates: True\n",
      "[FALSE POSITIVES] (2 cases):\n",
      " 5167142015|2015|L <-> 6458482015|2015|S (score: 1.000)\n",
      " Left:  'dario alvarez' | Season: 2015 | Birth: 1989\n",
      " Right: 'dariel alvarez' | Season: 2015 | Birth: 1989\n",
      " 5437062017|2017|L <-> 6210022017|2017|S (score: 1.000)\n",
      " Left:  'dan robertson' | Season: 2017 | Birth: 1985\n",
      " Right: 'daniel robertson' | Season: 2017 | Birth: 1994\n",
      "XGBoostClassifier error analysis complete\n"
     ]
    }
   ],
   "source": [
    "# Analyze error cases for XGBoostClassifier\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"{edge_name}: XGBoostClassifier Error Cases Analysis\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    left_df, right_df = source_tables[edge_name]\n",
    "    val_df = splits[edge_name]['val'][['id1', 'id2', 'label']].copy()\n",
    "    correspondences = xgb_matching_results[edge_name]\n",
    "    \n",
    "    true_matches = val_df[val_df['label'].astype(str).str.strip().str.upper() == 'TRUE']\n",
    "    false_matches = val_df[val_df['label'].astype(str).str.strip().str.upper() == 'FALSE']\n",
    "    true_set = set(zip(true_matches['id1'], true_matches['id2']))\n",
    "    false_set = set(zip(false_matches['id1'], false_matches['id2']))\n",
    "    pred_set = set(zip(correspondences['id1'], correspondences['id2']))\n",
    "    val_set = set(zip(val_df['id1'], val_df['id2']))\n",
    "    \n",
    "    fn_pairs = true_set - pred_set\n",
    "    print(f\"[FALSE NEGATIVES] ({len(fn_pairs)} cases):\")\n",
    "    if fn_pairs:\n",
    "        for id1, id2 in fn_pairs:\n",
    "            left_rec = left_df[left_df['_rid'] == id1].iloc[0] if len(left_df[left_df['_rid'] == id1]) > 0 else None\n",
    "            right_rec = right_df[right_df['_rid'] == id2].iloc[0] if len(right_df[right_df['_rid'] == id2]) > 0 else None\n",
    "            if left_rec is not None and right_rec is not None:\n",
    "                print(f\"{id1} <-> {id2}\")\n",
    "                print(f\"Left:  '{left_rec.get('full_name', 'N/A')}' | Season: {left_rec.get('season_year', 'N/A')} | Birth: {left_rec.get('birth_year', 'N/A')}\")\n",
    "                print(f\"Right: '{right_rec.get('full_name', 'N/A')}' | Season: {right_rec.get('season_year', 'N/A')} | Birth: {right_rec.get('birth_year', 'N/A')}\")\n",
    "                in_candidates = len(candidates[edge_name][(candidates[edge_name]['id1'] == id1) & (candidates[edge_name]['id2'] == id2)]) > 0\n",
    "                print(f\"In candidates: {in_candidates}\")\n",
    "                score_row = correspondences[(correspondences['id1'] == id1) & (correspondences['id2'] == id2)]\n",
    "                if len(score_row) > 0 and 'score' in score_row.columns:\n",
    "                    print(f\"    XGBoost Score: {score_row['score'].iloc[0]:.3f}\")\n",
    "    else:\n",
    "        print(\"  No false negatives found!\")\n",
    "    \n",
    "    fp_pairs = (pred_set & val_set) & false_set\n",
    "    print(f\"[FALSE POSITIVES] ({len(fp_pairs)} cases):\")\n",
    "    if fp_pairs:\n",
    "        for id1, id2 in fp_pairs:\n",
    "            left_rec = left_df[left_df['_rid'] == id1].iloc[0] if len(left_df[left_df['_rid'] == id1]) > 0 else None\n",
    "            right_rec = right_df[right_df['_rid'] == id2].iloc[0] if len(right_df[right_df['_rid'] == id2]) > 0 else None\n",
    "            score_row = correspondences[(correspondences['id1'] == id1) & (correspondences['id2'] == id2)]\n",
    "            score = score_row['score'].iloc[0] if len(score_row) > 0 and 'score' in score_row.columns else None\n",
    "            if left_rec is not None and right_rec is not None:\n",
    "                score_str = f\"{score:.3f}\" if score is not None else 'N/A'\n",
    "                print(f\" {id1} <-> {id2} (score: {score_str})\")\n",
    "                print(f\" Left:  '{left_rec.get('full_name', 'N/A')}' | Season: {left_rec.get('season_year', 'N/A')} | Birth: {left_rec.get('birth_year', 'N/A')}\")\n",
    "                print(f\" Right: '{right_rec.get('full_name', 'N/A')}' | Season: {right_rec.get('season_year', 'N/A')} | Birth: {right_rec.get('birth_year', 'N/A')}\")\n",
    "    else:\n",
    "        print(\"  No false positives found!\")\n",
    "\n",
    "print(\"XGBoostClassifier error analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ea4fb8",
   "metadata": {},
   "source": [
    "## 6. Post-Processing and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f61c41c",
   "metadata": {},
   "source": [
    "### 6.1 Global Matching (One-to-One Constraint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a0dd443b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Filtered correspondences: 15533 -> 15533 (threshold=0.0)\n",
      "[INFO ] root - Greedy matching: 15533 -> 15160 correspondences (30320 entities matched)\n",
      "[INFO ] root - GreedyOneToOneMatchingAlgorithm: 15533 -> 15160 correspondences\n",
      "[INFO ] root - GreedyOneToOneMatchingAlgorithm: 30502 -> 30320 entities\n",
      "[INFO ] root - Filtered correspondences: 6692 -> 6692 (threshold=0.0)\n",
      "[INFO ] root - Greedy matching: 6692 -> 6685 correspondences (13370 entities matched)\n",
      "[INFO ] root - GreedyOneToOneMatchingAlgorithm: 6692 -> 6685 correspondences\n",
      "[INFO ] root - GreedyOneToOneMatchingAlgorithm: 13372 -> 13370 entities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: Greedy matching 15533 -> 15160 pairs\n",
      "LS: Greedy matching 6692 -> 6685 pairs\n"
     ]
    }
   ],
   "source": [
    "from PyDI.entitymatching import GreedyOneToOneMatchingAlgorithm\n",
    "gb_global_matches = {}\n",
    "for edge_name in ['LR','LS']:\n",
    "    matcher = GreedyOneToOneMatchingAlgorithm()\n",
    "    correspondences = gb_matching_results[edge_name].rename(columns={'score':'score'})\n",
    "    gb_global_matches[edge_name] = matcher.cluster(correspondences)\n",
    "    print(f\"{edge_name}: Greedy matching {len(correspondences)} -> {len(gb_global_matches[edge_name])} pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c20563e",
   "metadata": {},
   "source": [
    "### 6.2 Cluster Consistency Analysis\n",
    "\n",
    "Analyze the cluster structure to identify any inconsistencies that our evaluation set may miss. The `EntityMatchingEvaluator` offers the `create_cluster_size_distribution` method for this purpose.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b68cd794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster analysis – LR (GB, global-matched)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.evaluation - Cluster Size Distribution of 15160 clusters:\n",
      "[INFO ] PyDI.entitymatching.evaluation - \tCluster Size\t| Frequency\t| Percentage\n",
      "[INFO ] PyDI.entitymatching.evaluation - \t──────────────────────────────────────────────────\n",
      "[INFO ] PyDI.entitymatching.evaluation - \t\t2\t|\t15160\t|\t100.00%\n",
      "[INFO ] root - Cluster size distribution written to /Users/zhangzihan/Desktop/WBI_project/Schema_Mapped_Datasets/data/output/matching/cluster_analysis/gb_global/cluster_size_distribution.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " cluster_size  frequency  percentage\n",
      "            2      15160       100.0\n",
      "\n",
      "Cluster analysis – LS (GB, global-matched)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.evaluation - Cluster Size Distribution of 6685 clusters:\n",
      "[INFO ] PyDI.entitymatching.evaluation - \tCluster Size\t| Frequency\t| Percentage\n",
      "[INFO ] PyDI.entitymatching.evaluation - \t──────────────────────────────────────────────────\n",
      "[INFO ] PyDI.entitymatching.evaluation - \t\t2\t|\t6685\t|\t100.00%\n",
      "[INFO ] root - Cluster size distribution written to /Users/zhangzihan/Desktop/WBI_project/Schema_Mapped_Datasets/data/output/matching/cluster_analysis/gb_global/cluster_size_distribution.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " cluster_size  frequency  percentage\n",
      "            2       6685       100.0\n"
     ]
    }
   ],
   "source": [
    "from PyDI.entitymatching.evaluation import EntityMatchingEvaluator\n",
    "\n",
    "for edge_name in ['LR','LS']:\n",
    "    print(f\"\\nCluster analysis – {edge_name} (GB, global-matched)\")\n",
    "    dist = EntityMatchingEvaluator.create_cluster_size_distribution(\n",
    "        correspondences=gb_global_matches[edge_name],\n",
    "        out_dir=OUTPUT_DIR / 'cluster_analysis' / 'gb_global'\n",
    "    )\n",
    "    if dist is not None:\n",
    "        print(dist.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "20683ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Cluster Consistency Analysis: GradientBoosting (GB) Matching\n",
      "================================================================================\n",
      "\n",
      "LR Edge:\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.evaluation - Cluster Size Distribution of 15048 clusters:\n",
      "[INFO ] PyDI.entitymatching.evaluation - \tCluster Size\t| Frequency\t| Percentage\n",
      "[INFO ] PyDI.entitymatching.evaluation - \t──────────────────────────────────────────────────\n",
      "[INFO ] PyDI.entitymatching.evaluation - \t\t2\t|\t14825\t|\t98.52%\n",
      "[INFO ] PyDI.entitymatching.evaluation - \t\t3\t|\t99\t|\t0.66%\n",
      "[INFO ] PyDI.entitymatching.evaluation - \t\t4\t|\t94\t|\t0.62%\n",
      "[INFO ] PyDI.entitymatching.evaluation - \t\t5\t|\t15\t|\t0.10%\n",
      "[INFO ] PyDI.entitymatching.evaluation - \t\t6\t|\t8\t|\t0.05%\n",
      "[INFO ] PyDI.entitymatching.evaluation - \t\t7\t|\t3\t|\t0.02%\n",
      "[INFO ] PyDI.entitymatching.evaluation - \t\t8\t|\t3\t|\t0.02%\n",
      "[INFO ] PyDI.entitymatching.evaluation - \t\t11\t|\t1\t|\t0.01%\n",
      "[INFO ] root - Cluster size distribution written to /Users/zhangzihan/Desktop/WBI_project/Schema_Mapped_Datasets/data/output/matching/cluster_analysis/gb_raw/cluster_size_distribution.csv\n",
      "[INFO ] PyDI.entitymatching.evaluation - Cluster Size Distribution of 6681 clusters:\n",
      "[INFO ] PyDI.entitymatching.evaluation - \tCluster Size\t| Frequency\t| Percentage\n",
      "[INFO ] PyDI.entitymatching.evaluation - \t──────────────────────────────────────────────────\n",
      "[INFO ] PyDI.entitymatching.evaluation - \t\t2\t|\t6675\t|\t99.91%\n",
      "[INFO ] PyDI.entitymatching.evaluation - \t\t3\t|\t2\t|\t0.03%\n",
      "[INFO ] PyDI.entitymatching.evaluation - \t\t4\t|\t4\t|\t0.06%\n",
      "[INFO ] root - Cluster size distribution written to /Users/zhangzihan/Desktop/WBI_project/Schema_Mapped_Datasets/data/output/matching/cluster_analysis/gb_raw/cluster_size_distribution.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster Size Distribution:\n",
      " cluster_size  frequency  percentage\n",
      "            2      14825   98.518075\n",
      "            3         99    0.657895\n",
      "            4         94    0.624668\n",
      "            5         15    0.099681\n",
      "            6          8    0.053163\n",
      "            7          3    0.019936\n",
      "            8          3    0.019936\n",
      "           11          1    0.006645\n",
      "\n",
      "Summary:\n",
      "  Total clusters: 15048\n",
      "  Clusters with size 2: 14825 (98.52%)\n",
      "  Clusters with size > 2: 223 (1.48%)\n",
      "\n",
      "   Warning: Found 223 clusters with size > 2.\n",
      "     This may indicate issues with the evaluation set or data quality.\n",
      "     Consider manually inspecting these clusters.\n",
      "\n",
      "LS Edge:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Cluster Size Distribution:\n",
      " cluster_size  frequency  percentage\n",
      "            2       6675   99.910193\n",
      "            3          2    0.029936\n",
      "            4          4    0.059871\n",
      "\n",
      "Summary:\n",
      "  Total clusters: 6681\n",
      "  Clusters with size 2: 6675 (99.91%)\n",
      "  Clusters with size > 2: 6 (0.09%)\n",
      "\n",
      "   Warning: Found 6 clusters with size > 2.\n",
      "     This may indicate issues with the evaluation set or data quality.\n",
      "     Consider manually inspecting these clusters.\n",
      "\n",
      "✓ GradientBoosting (GB) matching cluster analysis complete\n"
     ]
    }
   ],
   "source": [
    "# Cluster Consistency Analysis for GradientBoosting (GB) Matching\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Cluster Consistency Analysis: GradientBoosting (GB) Matching\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "gb_cluster_distributions = {}\n",
    "\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    print(f\"\\n{edge_name} Edge:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Use GradientBoosting correspondences (before global one-to-one matching)\n",
    "    correspondences = gb_matching_results[edge_name]\n",
    "    \n",
    "    # Create cluster size distribution\n",
    "    cluster_distribution = EntityMatchingEvaluator.create_cluster_size_distribution(\n",
    "        correspondences=correspondences,\n",
    "        out_dir=OUTPUT_DIR / \"cluster_analysis\" / \"gb_raw\"\n",
    "    )\n",
    "    \n",
    "    gb_cluster_distributions[edge_name] = cluster_distribution\n",
    "    \n",
    "    print(\"\\nCluster Size Distribution:\")\n",
    "    if cluster_distribution is not None and len(cluster_distribution) > 0:\n",
    "        print(cluster_distribution.to_string(index=False))\n",
    "        \n",
    "        # Calculate summary statistics\n",
    "        # Note: PyDI returns column names in lowercase: 'cluster_size', 'frequency', 'percentage'\n",
    "        total_clusters = cluster_distribution['frequency'].sum()\n",
    "        clusters_size_2 = cluster_distribution[cluster_distribution['cluster_size'] == 2]['frequency'].sum() if len(cluster_distribution[cluster_distribution['cluster_size'] == 2]) > 0 else 0\n",
    "        clusters_size_gt_2 = total_clusters - clusters_size_2\n",
    "        \n",
    "        print(\"\\nSummary:\")\n",
    "        print(f\"  Total clusters: {total_clusters}\")\n",
    "        print(f\"  Clusters with size 2: {clusters_size_2} ({clusters_size_2/total_clusters*100:.2f}%)\")\n",
    "        print(f\"  Clusters with size > 2: {clusters_size_gt_2} ({clusters_size_gt_2/total_clusters*100:.2f}%)\")\n",
    "        \n",
    "        if clusters_size_gt_2 > 0:\n",
    "            print(f\"\\n   Warning: Found {clusters_size_gt_2} clusters with size > 2.\")\n",
    "            print(\"     This may indicate issues with the evaluation set or data quality.\")\n",
    "            print(\"     Consider manually inspecting these clusters.\")\n",
    "    else:\n",
    "        print(\"  No cluster distribution data available\")\n",
    "\n",
    "print(\"\\n✓ GradientBoosting (GB) matching cluster analysis complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118f25ad",
   "metadata": {},
   "source": [
    "### 6.3 Compare All Matching Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e7f3ab53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Matching Performance Comparison: All Methods\n",
      "================================================================================\n",
      "\n",
      "LR Edge:\n",
      "--------------------------------------------------------------------------------\n",
      "  Metric       | Rule            | Optimized       | LogReg          | RandomForest    | GB (Default)    | GB (Tuned)      | XGB (Default)   | XGB (Tuned)     | Best\n",
      "  -------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "  Precision    |           0.962 |           1.000 |           0.909 |           1.000 |           1.000 |           1.000 |           1.000 |           1.000 | Optimized\n",
      "  Recall       |           0.833 |           0.933 |           1.000 |           1.000 |           1.000 |           1.000 |           1.000 |           1.000 | LogReg\n",
      "  F1-Score     |           0.893 |           0.966 |           0.952 |           1.000 |           1.000 |           1.000 |           1.000 |           1.000 | RandomForest\n",
      "\n",
      "LS Edge:\n",
      "--------------------------------------------------------------------------------\n",
      "  Metric       | Rule            | Optimized       | LogReg          | RandomForest    | GB (Default)    | GB (Tuned)      | XGB (Default)   | XGB (Tuned)     | Best\n",
      "  -------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "  Precision    |           0.953 |           1.000 |           0.961 |           0.961 |           0.980 |           0.980 |           0.960 |           0.961 | Optimized\n",
      "  Recall       |           0.837 |           0.939 |           1.000 |           1.000 |           0.980 |           0.980 |           0.980 |           1.000 | LogReg\n",
      "  F1-Score     |           0.891 |           0.968 |           0.980 |           0.980 |           0.980 |           0.980 |           0.970 |           0.980 | LogReg\n",
      "\n",
      "================================================================================\n",
      "Summary:\n",
      "================================================================================\n",
      "  Methods compared: Rule, Optimized, LogReg, RandomForest, GB (Default), GB (Tuned), XGB (Default), XGB (Tuned)\n",
      "  Results saved to: /Users/zhangzihan/Desktop/WBI_project/Schema_Mapped_Datasets/data/output/matching/matching-comparison-all-methods.csv\n"
     ]
    }
   ],
   "source": [
    "# Compare all matching methods: Rule-Based, Optimized, LogisticRegression, RandomForest, GradientBoosting, and XGBoost\n",
    "# Note: ensure each model section has been executed so the corresponding metrics dictionaries exist.\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Matching Performance Comparison: All Methods\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "methods = [\n",
    "    (\"Rule\", matching_metrics_val),\n",
    "    (\"Optimized\", optimized_matching_metrics_val),\n",
    "    (\"LogReg\", logreg_matching_metrics_val),\n",
    "    (\"RandomForest\", ml_matching_metrics_val),\n",
    "    (\"GB (Default)\", gb_matching_metrics_val if 'gb_matching_metrics_val' in globals() else {}),\n",
    "    (\"GB (Tuned)\", gb_tuned_matching_metrics_val if 'gb_tuned_matching_metrics_val' in globals() else {}),\n",
    "    (\"XGB (Default)\", xgb_matching_metrics_val if 'xgb_matching_metrics_val' in globals() else {}),\n",
    "    (\"XGB (Tuned)\", xgb_tuned_matching_metrics_val if 'xgb_tuned_matching_metrics_val' in globals() else {}),\n",
    "]\n",
    "method_labels = [label for label, _ in methods]\n",
    "comparison_data_all = []\n",
    "\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    print(f\"\\n{edge_name} Edge:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    precision_values = []\n",
    "    recall_values = []\n",
    "    f1_values = []\n",
    "    row_entry = {'edge': edge_name}\n",
    "    \n",
    "    for label, metrics_dict in methods:\n",
    "        metrics = metrics_dict.get(edge_name, {}) or {}\n",
    "        precision = metrics.get('precision', 0.0)\n",
    "        recall = metrics.get('recall', 0.0)\n",
    "        f1 = metrics.get('f1', metrics.get('f1_score', 0.0))\n",
    "        precision_values.append(precision)\n",
    "        recall_values.append(recall)\n",
    "        f1_values.append(f1)\n",
    "        row_entry[f\"{label.lower()}_precision\"] = precision\n",
    "        row_entry[f\"{label.lower()}_recall\"] = recall\n",
    "        row_entry[f\"{label.lower()}_f1\"] = f1\n",
    "    \n",
    "    header = \"  Metric       | \" + \" | \".join(f\"{label:<15}\" for label in method_labels) + \" | Best\"\n",
    "    divider = \"  \" + \"-\" * (len(header) - 2)\n",
    "    print(header)\n",
    "    print(divider)\n",
    "    \n",
    "    def best_label(values):\n",
    "        max_value = max(values)\n",
    "        idx = values.index(max_value)\n",
    "        return method_labels[idx]\n",
    "    \n",
    "    precision_line = \"  Precision    | \" + \" | \".join(f\"{val:15.3f}\" for val in precision_values) + f\" | {best_label(precision_values)}\"\n",
    "    recall_line = \"  Recall       | \" + \" | \".join(f\"{val:15.3f}\" for val in recall_values) + f\" | {best_label(recall_values)}\"\n",
    "    f1_line = \"  F1-Score     | \" + \" | \".join(f\"{val:15.3f}\" for val in f1_values) + f\" | {best_label(f1_values)}\"\n",
    "    print(precision_line)\n",
    "    print(recall_line)\n",
    "    print(f1_line)\n",
    "    \n",
    "    comparison_data_all.append(row_entry)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Summary:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"  Methods compared:\", \", \".join(method_labels))\n",
    "\n",
    "comparison_df_all = pd.DataFrame(comparison_data_all)\n",
    "comparison_df_all.to_csv(OUTPUT_DIR / 'matching-comparison-all-methods.csv', index=False)\n",
    "print(f\"  Results saved to: {OUTPUT_DIR / 'matching-comparison-all-methods.csv'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c55687c",
   "metadata": {},
   "source": [
    "# Error Analysis Summary: From Rule-Based to GradientBoosting Matching\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "This analysis documents the error reduction from baseline rule-based matching to the final GradientBoosting classifier, showing a 93.3% reduction in false negatives and 50% reduction in false positives.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Baseline Rule-Based Matching Errors\n",
    "\n",
    "**Error Statistics:**\n",
    "- **LR Edge**: 7 False Negatives (FN), 0 False Positives (FP)\n",
    "- **LS Edge**: 8 False Negatives (FN), 2 False Positives (FP)\n",
    "- **Total**: 15 FN, 2 FP\n",
    "\n",
    "**Error Patterns:**\n",
    "\n",
    "**False Negatives:** All 15 cases involved name variants (e.g., `dan/daniel`, `matt/matthew`, `jon/jonathon`, `phil/phillip`, `rafael/raffy`). Root cause: string similarity insufficient for variant detection; threshold 0.7 too strict.\n",
    "\n",
    "**False Positives:** 2 cases with phonetically similar names (`dario/dariel alvarez`, `jose/josh rojas`). Root cause: over-reliance on string similarity; no birth year constraint.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Improvement Measures Implemented\n",
    "\n",
    "### 2.1 Name Variant Dictionary\n",
    "- Created `NAME_VARIANTS` dictionary with 38 common name variants\n",
    "- Applied in optimized rule-based matching with score boosting\n",
    "- Added `is_name_variant` binary feature for ML models\n",
    "\n",
    "### 2.2 Birth Year Constraints\n",
    "- Soft constraint in rule-based matching (penalty 0.2 when difference > 1 year)\n",
    "- `DateComparator` and `birth_year_diff` numeric feature for ML models\n",
    "- Strong penalty when names match but birth years differ ≥ 2 years\n",
    "\n",
    "### 2.3 Feature Engineering\n",
    "- Enhanced feature set: Levenshtein distance, Jaccard similarity, birth year comparator, birth year difference, name variant flag, Soundex phonetic match\n",
    "\n",
    "### 2.4 Error Case Collection and Training Set Reconstruction\n",
    "- Collected error cases from rule-based and optimized matching evaluations\n",
    "- Exported error cases to `data/output/gt/manual_cases/` for ground truth augmentation\n",
    "- Reconstructed training, validation, and test sets to include corner cases\n",
    "- Ensured error patterns are represented in training data for better model learning\n",
    "\n",
    "### 2.5 Name Normalization\n",
    "- Centralized normalization in `name_utils.py`\n",
    "- Handles UTF-8 encoding, Unicode normalization, punctuation, and suffixes\n",
    "\n",
    "### 2.6 Threshold Adjustments\n",
    "- Edge-specific thresholds for rule-based matching\n",
    "- Classifier probability thresholds tuned per edge\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Final GradientBoosting Performance\n",
    "\n",
    "**Error Statistics:**\n",
    "- **LR Edge**: 0 False Negatives, 0 False Positives\n",
    "- **LS Edge**: 1 False Negative, 1 False Positive\n",
    "- **Total**: 1 FN, 1 FP (93.3% reduction in FN, 50% reduction in FP)\n",
    "\n",
    "**Remaining Errors:**\n",
    "- **False Negative**: `steven tolleson` vs `steve tolleson` - variant not in dictionary\n",
    "- **False Positive**: `dario alvarez` vs `dariel alvarez` - model overconfidence on highly similar names\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Errors Eliminated\n",
    "\n",
    "**All Name Variant False Negatives (14 cases):**\n",
    "- `dan/daniel` (5 cases), `matt/matthew` (1 case), `cal/calvin` (1 case), `jon/jonathon` (2 cases), `phil/phillip` (2 cases), `danny/daniel` (1 case), `rafael/raffy` (2 cases)\n",
    "\n",
    "**Phonetically Similar False Positives (1 case):**\n",
    "- `jose rojas` vs `josh rojas` - fixed by birth year constraints\n",
    "\n",
    "**All LR Edge Errors (7 FN):**\n",
    "- Perfect precision and recall achieved on LR edge\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Error Reduction Summary\n",
    "\n",
    "| Error Type | Baseline | GradientBoosting | Reduction |\n",
    "|------------|----------|------------------|-----------|\n",
    "| FN (LR) | 7 | 0 | 100% |\n",
    "| FN (LS) | 8 | 1 | 87.5% |\n",
    "| FP (LS) | 2 | 1 | 50% |\n",
    "| **Total FN** | **15** | **1** | **93.3%** |\n",
    "| **Total FP** | **2** | **1** | **50%** |\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Key Success Factors\n",
    "\n",
    "1. **Name Variant Dictionary**: Eliminated 14/15 FN cases (93.3%)\n",
    "2. **Error Case Collection and Training Set Reconstruction**: Improved model learning by including corner cases in training data\n",
    "3. **Birth Year Constraints**: Prevented over-merging based on name similarity alone\n",
    "4. **Feature Engineering**: ML models learned complex patterns beyond string similarity\n",
    "5. **Phonetic Features**: Better handling of pronunciation variants\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Recommendations\n",
    "\n",
    "1. Expand name variant dictionary: add `steven/steve`\n",
    "2. Strengthen birth year penalties for perfect name matches\n",
    "3. Add post-processing rules for high-confidence matches with name differences\n",
    "4. Consider ensemble methods to reduce overconfidence\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The transition from rule-based to GradientBoosting matching achieved significant error reduction through name variant handling, birth year constraints, feature engineering, and training set reconstruction with error cases. The remaining errors are edge cases requiring dictionary expansion and stronger constraints for high-confidence matches with subtle name differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83ce158",
   "metadata": {},
   "source": [
    "### 6.4 Final Test Set Evaluation\n",
    "\n",
    "**IMPORTANT**: This section evaluates all models on the **test set** for final performance reporting.\n",
    "\n",
    "**Guidelines**:\n",
    "- Only run this section **ONCE** after all model selection and hyperparameter tuning is complete\n",
    "- Do **NOT** use test set results to make further adjustments to models\n",
    "- Test set evaluation provides unbiased estimates of model performance\n",
    "- These metrics should be used for final reporting and comparison\n",
    "\n",
    "**Models Evaluated**:\n",
    "- Rule-Based Matching\n",
    "- Optimized Rule-Based Matching  \n",
    "- LogisticRegression\n",
    "- RandomForest\n",
    "- GradientBoosting (final selected model)\n",
    "- XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "77208da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FINAL EVALUATION ON TEST SET\n",
      "================================================================================\n",
      " This is the final evaluation. Do not use these results for further tuning.\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Rule-Based - Test Set Evaluation\n",
      "================================================================================\n",
      "\n",
      "  === LR Edge ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Confusion Matrix:\n",
      "[INFO ] root -   True Positives:  23\n",
      "[INFO ] root -   True Negatives:  64\n",
      "[INFO ] root -   False Positives: 3\n",
      "[INFO ] root -   False Negatives: 7\n",
      "[INFO ] root - Performance Metrics:\n",
      "[INFO ] root -   Accuracy:  0.897\n",
      "[INFO ] root -   Precision: 0.885\n",
      "[INFO ] root -   Recall:    0.767\n",
      "[INFO ] root -   F1-Score:  0.821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Precision: 0.885\n",
      "    Recall:    0.767\n",
      "    F1-Score:  0.821\n",
      "    TP: 23\n",
      "    FP: 3\n",
      "    FN: 7\n",
      "\n",
      "  === LS Edge ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Confusion Matrix:\n",
      "[INFO ] root -   True Positives:  43\n",
      "[INFO ] root -   True Negatives:  50\n",
      "[INFO ] root -   False Positives: 3\n",
      "[INFO ] root -   False Negatives: 4\n",
      "[INFO ] root - Performance Metrics:\n",
      "[INFO ] root -   Accuracy:  0.930\n",
      "[INFO ] root -   Precision: 0.935\n",
      "[INFO ] root -   Recall:    0.915\n",
      "[INFO ] root -   F1-Score:  0.925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Precision: 0.935\n",
      "    Recall:    0.915\n",
      "    F1-Score:  0.925\n",
      "    TP: 43\n",
      "    FP: 3\n",
      "    FN: 4\n",
      "\n",
      "================================================================================\n",
      "Optimized - Test Set Evaluation\n",
      "================================================================================\n",
      "\n",
      "  === LR Edge ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Confusion Matrix:\n",
      "[INFO ] root -   True Positives:  26\n",
      "[INFO ] root -   True Negatives:  67\n",
      "[INFO ] root -   False Positives: 0\n",
      "[INFO ] root -   False Negatives: 4\n",
      "[INFO ] root - Performance Metrics:\n",
      "[INFO ] root -   Accuracy:  0.959\n",
      "[INFO ] root -   Precision: 1.000\n",
      "[INFO ] root -   Recall:    0.867\n",
      "[INFO ] root -   F1-Score:  0.929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Precision: 1.000\n",
      "    Recall:    0.867\n",
      "    F1-Score:  0.929\n",
      "    TP: 26\n",
      "    FP: 0\n",
      "    FN: 4\n",
      "\n",
      "  === LS Edge ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Confusion Matrix:\n",
      "[INFO ] root -   True Positives:  41\n",
      "[INFO ] root -   True Negatives:  53\n",
      "[INFO ] root -   False Positives: 0\n",
      "[INFO ] root -   False Negatives: 6\n",
      "[INFO ] root - Performance Metrics:\n",
      "[INFO ] root -   Accuracy:  0.940\n",
      "[INFO ] root -   Precision: 1.000\n",
      "[INFO ] root -   Recall:    0.872\n",
      "[INFO ] root -   F1-Score:  0.932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Precision: 1.000\n",
      "    Recall:    0.872\n",
      "    F1-Score:  0.932\n",
      "    TP: 41\n",
      "    FP: 0\n",
      "    FN: 6\n",
      "\n",
      "================================================================================\n",
      "LogisticRegression - Test Set Evaluation\n",
      "================================================================================\n",
      "\n",
      "  === LR Edge ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Confusion Matrix:\n",
      "[INFO ] root -   True Positives:  30\n",
      "[INFO ] root -   True Negatives:  63\n",
      "[INFO ] root -   False Positives: 4\n",
      "[INFO ] root -   False Negatives: 0\n",
      "[INFO ] root - Performance Metrics:\n",
      "[INFO ] root -   Accuracy:  0.959\n",
      "[INFO ] root -   Precision: 0.882\n",
      "[INFO ] root -   Recall:    1.000\n",
      "[INFO ] root -   F1-Score:  0.938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Precision: 0.882\n",
      "    Recall:    1.000\n",
      "    F1-Score:  0.938\n",
      "    TP: 30\n",
      "    FP: 4\n",
      "    FN: 0\n",
      "\n",
      "  === LS Edge ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Confusion Matrix:\n",
      "[INFO ] root -   True Positives:  47\n",
      "[INFO ] root -   True Negatives:  50\n",
      "[INFO ] root -   False Positives: 3\n",
      "[INFO ] root -   False Negatives: 0\n",
      "[INFO ] root - Performance Metrics:\n",
      "[INFO ] root -   Accuracy:  0.970\n",
      "[INFO ] root -   Precision: 0.940\n",
      "[INFO ] root -   Recall:    1.000\n",
      "[INFO ] root -   F1-Score:  0.969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Precision: 0.940\n",
      "    Recall:    1.000\n",
      "    F1-Score:  0.969\n",
      "    TP: 47\n",
      "    FP: 3\n",
      "    FN: 0\n",
      "\n",
      "================================================================================\n",
      "RandomForest - Test Set Evaluation\n",
      "================================================================================\n",
      "\n",
      "  === LR Edge ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Confusion Matrix:\n",
      "[INFO ] root -   True Positives:  30\n",
      "[INFO ] root -   True Negatives:  66\n",
      "[INFO ] root -   False Positives: 1\n",
      "[INFO ] root -   False Negatives: 0\n",
      "[INFO ] root - Performance Metrics:\n",
      "[INFO ] root -   Accuracy:  0.990\n",
      "[INFO ] root -   Precision: 0.968\n",
      "[INFO ] root -   Recall:    1.000\n",
      "[INFO ] root -   F1-Score:  0.984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Precision: 0.968\n",
      "    Recall:    1.000\n",
      "    F1-Score:  0.984\n",
      "    TP: 30\n",
      "    FP: 1\n",
      "    FN: 0\n",
      "\n",
      "  === LS Edge ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Confusion Matrix:\n",
      "[INFO ] root -   True Positives:  47\n",
      "[INFO ] root -   True Negatives:  51\n",
      "[INFO ] root -   False Positives: 2\n",
      "[INFO ] root -   False Negatives: 0\n",
      "[INFO ] root - Performance Metrics:\n",
      "[INFO ] root -   Accuracy:  0.980\n",
      "[INFO ] root -   Precision: 0.959\n",
      "[INFO ] root -   Recall:    1.000\n",
      "[INFO ] root -   F1-Score:  0.979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Precision: 0.959\n",
      "    Recall:    1.000\n",
      "    F1-Score:  0.979\n",
      "    TP: 47\n",
      "    FP: 2\n",
      "    FN: 0\n",
      "\n",
      "================================================================================\n",
      "GB (Default) - Test Set Evaluation\n",
      "================================================================================\n",
      "\n",
      "  === LR Edge ===\n",
      "    Using global-matched correspondences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Confusion Matrix:\n",
      "[INFO ] root -   True Positives:  30\n",
      "[INFO ] root -   True Negatives:  67\n",
      "[INFO ] root -   False Positives: 0\n",
      "[INFO ] root -   False Negatives: 0\n",
      "[INFO ] root - Performance Metrics:\n",
      "[INFO ] root -   Accuracy:  1.000\n",
      "[INFO ] root -   Precision: 1.000\n",
      "[INFO ] root -   Recall:    1.000\n",
      "[INFO ] root -   F1-Score:  1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Precision: 1.000\n",
      "    Recall:    1.000\n",
      "    F1-Score:  1.000\n",
      "    TP: 30\n",
      "    FP: 0\n",
      "    FN: 0\n",
      "\n",
      "  === LS Edge ===\n",
      "    Using global-matched correspondences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Confusion Matrix:\n",
      "[INFO ] root -   True Positives:  45\n",
      "[INFO ] root -   True Negatives:  53\n",
      "[INFO ] root -   False Positives: 0\n",
      "[INFO ] root -   False Negatives: 2\n",
      "[INFO ] root - Performance Metrics:\n",
      "[INFO ] root -   Accuracy:  0.980\n",
      "[INFO ] root -   Precision: 1.000\n",
      "[INFO ] root -   Recall:    0.957\n",
      "[INFO ] root -   F1-Score:  0.978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Precision: 1.000\n",
      "    Recall:    0.957\n",
      "    F1-Score:  0.978\n",
      "    TP: 45\n",
      "    FP: 0\n",
      "    FN: 2\n",
      "\n",
      "================================================================================\n",
      "GB (Tuned) - Test Set Evaluation\n",
      "================================================================================\n",
      "\n",
      "  === LR Edge ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Confusion Matrix:\n",
      "[INFO ] root -   True Positives:  30\n",
      "[INFO ] root -   True Negatives:  66\n",
      "[INFO ] root -   False Positives: 1\n",
      "[INFO ] root -   False Negatives: 0\n",
      "[INFO ] root - Performance Metrics:\n",
      "[INFO ] root -   Accuracy:  0.990\n",
      "[INFO ] root -   Precision: 0.968\n",
      "[INFO ] root -   Recall:    1.000\n",
      "[INFO ] root -   F1-Score:  0.984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Precision: 0.968\n",
      "    Recall:    1.000\n",
      "    F1-Score:  0.984\n",
      "    TP: 30\n",
      "    FP: 1\n",
      "    FN: 0\n",
      "\n",
      "  === LS Edge ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Confusion Matrix:\n",
      "[INFO ] root -   True Positives:  45\n",
      "[INFO ] root -   True Negatives:  53\n",
      "[INFO ] root -   False Positives: 0\n",
      "[INFO ] root -   False Negatives: 2\n",
      "[INFO ] root - Performance Metrics:\n",
      "[INFO ] root -   Accuracy:  0.980\n",
      "[INFO ] root -   Precision: 1.000\n",
      "[INFO ] root -   Recall:    0.957\n",
      "[INFO ] root -   F1-Score:  0.978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Precision: 1.000\n",
      "    Recall:    0.957\n",
      "    F1-Score:  0.978\n",
      "    TP: 45\n",
      "    FP: 0\n",
      "    FN: 2\n",
      "\n",
      "================================================================================\n",
      "XGB (Default) - Test Set Evaluation\n",
      "================================================================================\n",
      "\n",
      "  === LR Edge ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Confusion Matrix:\n",
      "[INFO ] root -   True Positives:  30\n",
      "[INFO ] root -   True Negatives:  66\n",
      "[INFO ] root -   False Positives: 1\n",
      "[INFO ] root -   False Negatives: 0\n",
      "[INFO ] root - Performance Metrics:\n",
      "[INFO ] root -   Accuracy:  0.990\n",
      "[INFO ] root -   Precision: 0.968\n",
      "[INFO ] root -   Recall:    1.000\n",
      "[INFO ] root -   F1-Score:  0.984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Precision: 0.968\n",
      "    Recall:    1.000\n",
      "    F1-Score:  0.984\n",
      "    TP: 30\n",
      "    FP: 1\n",
      "    FN: 0\n",
      "\n",
      "  === LS Edge ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Confusion Matrix:\n",
      "[INFO ] root -   True Positives:  45\n",
      "[INFO ] root -   True Negatives:  53\n",
      "[INFO ] root -   False Positives: 0\n",
      "[INFO ] root -   False Negatives: 2\n",
      "[INFO ] root - Performance Metrics:\n",
      "[INFO ] root -   Accuracy:  0.980\n",
      "[INFO ] root -   Precision: 1.000\n",
      "[INFO ] root -   Recall:    0.957\n",
      "[INFO ] root -   F1-Score:  0.978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Precision: 1.000\n",
      "    Recall:    0.957\n",
      "    F1-Score:  0.978\n",
      "    TP: 45\n",
      "    FP: 0\n",
      "    FN: 2\n",
      "\n",
      "================================================================================\n",
      "XGB (Tuned) - Test Set Evaluation\n",
      "================================================================================\n",
      "\n",
      "  === LR Edge ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Confusion Matrix:\n",
      "[INFO ] root -   True Positives:  30\n",
      "[INFO ] root -   True Negatives:  66\n",
      "[INFO ] root -   False Positives: 1\n",
      "[INFO ] root -   False Negatives: 0\n",
      "[INFO ] root - Performance Metrics:\n",
      "[INFO ] root -   Accuracy:  0.990\n",
      "[INFO ] root -   Precision: 0.968\n",
      "[INFO ] root -   Recall:    1.000\n",
      "[INFO ] root -   F1-Score:  0.984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Precision: 0.968\n",
      "    Recall:    1.000\n",
      "    F1-Score:  0.984\n",
      "    TP: 30\n",
      "    FP: 1\n",
      "    FN: 0\n",
      "\n",
      "  === LS Edge ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Confusion Matrix:\n",
      "[INFO ] root -   True Positives:  47\n",
      "[INFO ] root -   True Negatives:  51\n",
      "[INFO ] root -   False Positives: 2\n",
      "[INFO ] root -   False Negatives: 0\n",
      "[INFO ] root - Performance Metrics:\n",
      "[INFO ] root -   Accuracy:  0.980\n",
      "[INFO ] root -   Precision: 0.959\n",
      "[INFO ] root -   Recall:    1.000\n",
      "[INFO ] root -   F1-Score:  0.979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Precision: 0.959\n",
      "    Recall:    1.000\n",
      "    F1-Score:  0.979\n",
      "    TP: 47\n",
      "    FP: 2\n",
      "    FN: 0\n",
      "\n",
      "================================================================================\n",
      "Test Set Evaluation Summary\n",
      "================================================================================\n",
      "\n",
      "             Model Edge  Precision   Recall  F1-Score  TP  FP  FN\n",
      "        Rule-Based   LR   0.884615 0.766667  0.821429  23   3   7\n",
      "        Rule-Based   LS   0.934783 0.914894  0.924731  43   3   4\n",
      "         Optimized   LR   1.000000 0.866667  0.928571  26   0   4\n",
      "         Optimized   LS   1.000000 0.872340  0.931818  41   0   6\n",
      "LogisticRegression   LR   0.882353 1.000000  0.937500  30   4   0\n",
      "LogisticRegression   LS   0.940000 1.000000  0.969072  47   3   0\n",
      "      RandomForest   LR   0.967742 1.000000  0.983607  30   1   0\n",
      "      RandomForest   LS   0.959184 1.000000  0.979167  47   2   0\n",
      "      GB (Default)   LR   1.000000 1.000000  1.000000  30   0   0\n",
      "      GB (Default)   LS   1.000000 0.957447  0.978261  45   0   2\n",
      "        GB (Tuned)   LR   0.967742 1.000000  0.983607  30   1   0\n",
      "        GB (Tuned)   LS   1.000000 0.957447  0.978261  45   0   2\n",
      "     XGB (Default)   LR   0.967742 1.000000  0.983607  30   1   0\n",
      "     XGB (Default)   LS   1.000000 0.957447  0.978261  45   0   2\n",
      "       XGB (Tuned)   LR   0.967742 1.000000  0.983607  30   1   0\n",
      "       XGB (Tuned)   LS   0.959184 1.000000  0.979167  47   2   0\n",
      "\n",
      "✓ Test set evaluation summary saved to: /Users/zhangzihan/Desktop/WBI_project/Schema_Mapped_Datasets/data/output/matching/matching-test-evaluation-summary.csv\n",
      "\n",
      "================================================================================\n",
      "✓ Final test set evaluation complete\n",
      "These are your final performance metrics for reporting.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Final Evaluation on Test Set\n",
    "# IMPORTANT: Only run this ONCE after all tuning is complete\n",
    "# Do NOT use test set results for further model adjustments\n",
    "\n",
    "from PyDI.entitymatching.evaluation import EntityMatchingEvaluator\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FINAL EVALUATION ON TEST SET\")\n",
    "print(\"=\"*80)\n",
    "print(\" This is the final evaluation. Do not use these results for further tuning.\\n\")\n",
    "\n",
    "# Dictionary to store all test set metrics\n",
    "test_metrics_all = {}\n",
    "\n",
    "# List of models to evaluate (in order of evaluation)\n",
    "models_to_evaluate = [\n",
    "    ('Rule-Based', matching_results, 'matching'),\n",
    "    ('Optimized', optimized_matching_results, 'optimized'),\n",
    "    ('LogisticRegression', logreg_matching_results, 'logreg'),\n",
    "    ('RandomForest', ml_matching_results, 'ml'),\n",
    "    ('GB (Default)', gb_matching_results, 'gb'),\n",
    "    ('GB (Tuned)', gb_tuned_matching_results if 'gb_tuned_matching_results' in globals() else None, 'gb-tuned'),\n",
    "    ('XGB (Default)', xgb_matching_results if 'xgb_matching_results' in globals() else None, 'xgb'),\n",
    "    ('XGB (Tuned)', xgb_tuned_matching_results if 'xgb_tuned_matching_results' in globals() else None, 'xgb-tuned'),\n",
    "]\n",
    "\n",
    "for model_name, results_dict, model_key in models_to_evaluate:\n",
    "    if results_dict is None:\n",
    "        print(f\"\\n Skipping {model_name}: results not available\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{model_name} - Test Set Evaluation\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    test_metrics_model = {}\n",
    "    \n",
    "    for edge_name in ['LR', 'LS']:\n",
    "        if edge_name not in results_dict:\n",
    "            print(f\"  {edge_name}: results not available\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n  === {edge_name} Edge ===\")\n",
    "        \n",
    "        # Get correspondences (use global matched version if available for default GradientBoosting)\n",
    "        if model_name == 'GB (Default)' and 'gb_global_matches' in globals() and edge_name in gb_global_matches:\n",
    "            correspondences = gb_global_matches[edge_name]\n",
    "            print(f\"    Using global-matched correspondences\")\n",
    "        else:\n",
    "            correspondences = results_dict[edge_name]\n",
    "        \n",
    "        # Get test set\n",
    "        test_df = splits[edge_name]['test'][['id1', 'id2', 'label']].copy()\n",
    "        \n",
    "        # Prepare correspondences for evaluation\n",
    "        correspondences_for_eval = correspondences.copy()\n",
    "        if 'score' not in correspondences_for_eval.columns and 'sim' in correspondences_for_eval.columns:\n",
    "            correspondences_for_eval = correspondences_for_eval.rename(columns={'sim': 'score'})\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        try:\n",
    "            eval_results = EntityMatchingEvaluator.evaluate_matching(\n",
    "                correspondences=correspondences_for_eval,\n",
    "                test_pairs=test_df,\n",
    "                out_dir=OUTPUT_DIR / f'matching-evaluation-test-{model_key}',\n",
    "                matcher_instance=None  # Not needed for evaluation\n",
    "            )\n",
    "            \n",
    "            test_metrics_model[edge_name] = eval_results\n",
    "            \n",
    "            print(f\"    Precision: {eval_results.get('precision', 0.0):.3f}\")\n",
    "            print(f\"    Recall:    {eval_results.get('recall', 0.0):.3f}\")\n",
    "            print(f\"    F1-Score:  {eval_results.get('f1', 0.0):.3f}\")\n",
    "            print(f\"    TP: {eval_results.get('true_positives', 0)}\")\n",
    "            print(f\"    FP: {eval_results.get('false_positives', 0)}\")\n",
    "            print(f\"    FN: {eval_results.get('false_negatives', 0)}\")\n",
    "            \n",
    "        except Exception as exc:\n",
    "            print(f\" Evaluation failed: {exc}\")\n",
    "            test_metrics_model[edge_name] = None\n",
    "    \n",
    "    test_metrics_all[model_name] = test_metrics_model\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Test Set Evaluation Summary\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Create summary table\n",
    "summary_data = []\n",
    "for model_name, metrics_dict in test_metrics_all.items():\n",
    "    for edge_name in ['LR', 'LS']:\n",
    "        if edge_name in metrics_dict and metrics_dict[edge_name] is not None:\n",
    "            metrics = metrics_dict[edge_name]\n",
    "            summary_data.append({\n",
    "                'Model': model_name,\n",
    "                'Edge': edge_name,\n",
    "                'Precision': metrics.get('precision', 0.0),\n",
    "                'Recall': metrics.get('recall', 0.0),\n",
    "                'F1-Score': metrics.get('f1', 0.0),\n",
    "                'TP': metrics.get('true_positives', 0),\n",
    "                'FP': metrics.get('false_positives', 0),\n",
    "                'FN': metrics.get('false_negatives', 0),\n",
    "            })\n",
    "\n",
    "if summary_data:\n",
    "    import pandas as pd\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    print(summary_df.to_string(index=False))\n",
    "    \n",
    "    # Save summary\n",
    "    summary_df.to_csv(OUTPUT_DIR / 'matching-test-evaluation-summary.csv', index=False)\n",
    "    print(f\"\\n✓ Test set evaluation summary saved to: {OUTPUT_DIR / 'matching-test-evaluation-summary.csv'}\")\n",
    "else:\n",
    "    print(\" No test set evaluation results available\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"✓ Final test set evaluation complete\")\n",
    "print(\"These are your final performance metrics for reporting.\")\n",
    "print(f\"{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7711d4e5",
   "metadata": {},
   "source": [
    "## 7. Export Results\n",
    "  ### 7.1 Output Correspondences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "bc301bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "correspondences_output_dir = OUTPUT_DIR / \"correspondences\"\n",
    "correspondences_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# GradientBoosting LR edge\n",
    "gb_global_matches['LR'].to_pickle(correspondences_output_dir / \"gb_global_LR.pkl\")\n",
    "gb_global_matches['LR'].to_csv(correspondences_output_dir / \"gb_global_LR.csv\",index=False)\n",
    "\n",
    "# GradientBoosting LS edge\n",
    "gb_global_matches['LS'].to_pickle(correspondences_output_dir / \"gb_global_LS.pkl\")\n",
    "gb_global_matches['LS'].to_csv(correspondences_output_dir / \"gb_global_LS.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
