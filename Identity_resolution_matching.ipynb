{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "592da263",
   "metadata": {},
   "source": [
    "# Identity Resolution: Matching Phase\n",
    "\n",
    "This notebook implements the matching phase of identity resolution, using candidate pairs generated by the hybrid blocking strategy from `Identity_resolution_workflow.ipynb`.\n",
    "\n",
    "## Workflow Overview\n",
    "\n",
    "1. **Setup**: Import libraries, define paths, load source data tables\n",
    "2. **Load Data**: Load source tables, candidate pairs, and ground truth splits\n",
    "3. **Name Normalization**: Apply consistent name normalization (reusable)\n",
    "4. **Common Matching Infrastructure**: Define reusable matching framework\n",
    "5. **Matching Methods**: Implement different matching strategies\n",
    "   - Rule-Based Matching\n",
    "   - Optimized Matching (name variants + birth year constraint)\n",
    "   - ML-Based Matching (RandomForestClassifier)\n",
    "   - [Future: LLM-based matching, etc.]\n",
    "6. **Evaluation**: Assess matching quality (reusable)\n",
    "7. **Export Results**: Save final matches and metrics (reusable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee371ad",
   "metadata": {},
   "source": [
    "## 0. Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c0624b",
   "metadata": {},
   "source": [
    "### 0.1 Import Libraries and Setup Logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "d66cd730",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Matching phase logging enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Logging setup complete\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# Setup logging\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='[%(levelname)-5s] %(name)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('logs/matching.log'),\n",
    "        logging.StreamHandler()\n",
    "    ],\n",
    "    force=True\n",
    ")\n",
    "logging.getLogger().info('Matching phase logging enabled')\n",
    "\n",
    "print(\"✓ Logging setup complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01649e83",
   "metadata": {},
   "source": [
    "### 0.2 Define Paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f40666de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths configured:\n",
      "  Candidates LR: True\n",
      "  Candidates LS: True\n",
      "  Output dir: /Users/zhangzihan/Desktop/WBI_project/Schema_Mapped_Datasets/data/output/matching\n"
     ]
    }
   ],
   "source": [
    "# Project base directory\n",
    "BASE_DIR = Path('/Users/zhangzihan/Desktop/WBI_project/Schema_Mapped_Datasets')\n",
    "\n",
    "# Input: Candidate pairs from blocking phase\n",
    "CANDIDATES_DIR = BASE_DIR / 'data' / 'output' / 'workflow'\n",
    "CANDIDATES_LR = CANDIDATES_DIR / 'candidates_hybrid_LR.csv'\n",
    "CANDIDATES_LS = CANDIDATES_DIR / 'candidates_hybrid_LS.csv'\n",
    "\n",
    "# Input: Source data tables\n",
    "CLEAN_DIR = BASE_DIR / 'data' / 'output' / 'clean'\n",
    "LAHMAN_PATH = (CLEAN_DIR / 'Lahman_Mapped_dedup.xml'\n",
    "               if (CLEAN_DIR / 'Lahman_Mapped_dedup.xml').exists()\n",
    "               else BASE_DIR / 'Lahman_Mapped.xml')\n",
    "REFERENCE_PATH = (CLEAN_DIR / 'Reference_Mapped_dedup.xml'\n",
    "                  if (CLEAN_DIR / 'Reference_Mapped_dedup.xml').exists()\n",
    "                  else BASE_DIR / 'Reference_Mapped.xml')\n",
    "SAVANT_PATH = (CLEAN_DIR / 'Savant_Mapped_dedup.xml'\n",
    "               if (CLEAN_DIR / 'Savant_Mapped_dedup.xml').exists()\n",
    "               else BASE_DIR / 'Savant_Mapped.xml')\n",
    "\n",
    "# Input: Ground truth splits for evaluation\n",
    "SPLITS_DIR = BASE_DIR / 'data' / 'output' / 'gt' / 'splits'\n",
    "LR_TRAIN = SPLITS_DIR / 'gt_LR_train.csv'\n",
    "LR_VAL = SPLITS_DIR / 'gt_LR_val.csv'\n",
    "LR_TEST = SPLITS_DIR / 'gt_LR_test.csv'\n",
    "LS_TRAIN = SPLITS_DIR / 'gt_LS_train.csv'\n",
    "LS_VAL = SPLITS_DIR / 'gt_LS_val.csv'\n",
    "LS_TEST = SPLITS_DIR / 'gt_LS_test.csv'\n",
    "\n",
    "# Output: Matching results\n",
    "OUTPUT_DIR = BASE_DIR / 'data' / 'output' / 'matching'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Paths configured:\")\n",
    "print(f\"  Candidates LR: {CANDIDATES_LR.exists()}\")\n",
    "print(f\"  Candidates LS: {CANDIDATES_LS.exists()}\")\n",
    "print(f\"  Output dir: {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13510b4",
   "metadata": {},
   "source": [
    "### 0.3 Import PyDI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "bbdc4ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ PyDI imported successfully\n",
      "✓ ML libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import importlib\n",
    "import PyDI  # noqa: F401\n",
    "\n",
    "from PyDI.io import load_xml\n",
    "from PyDI.entitymatching import RuleBasedMatcher, GreedyOneToOneMatchingAlgorithm, MLBasedMatcher, FeatureExtractor\n",
    "from PyDI.entitymatching.comparators import StringComparator, DateComparator\n",
    "from PyDI.entitymatching.evaluation import EntityMatchingEvaluator\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "print(\"✓ PyDI imported successfully\")\n",
    "print(\"✓ ML libraries imported successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2354ac3",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "\n",
    "### 1.1 Load Source Data Tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "10faac85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading source data tables...\n",
      "  L_full: 106,553 records\n",
      "  R_full: 15,215 records\n",
      "  S_full: 6,743 records\n",
      "✓ Source tables loaded\n"
     ]
    }
   ],
   "source": [
    "# Load source data tables\n",
    "print(\"Loading source data tables...\")\n",
    "L_full = load_xml(LAHMAN_PATH).convert_dtypes().reset_index(drop=True)\n",
    "R_full = load_xml(REFERENCE_PATH).convert_dtypes().reset_index(drop=True)\n",
    "S_full = load_xml(SAVANT_PATH).convert_dtypes().reset_index(drop=True)\n",
    "\n",
    "# Create _rid column for matching (same format as blocking phase)\n",
    "for df, tag in [(L_full, 'L'), (R_full, 'R'), (S_full, 'S')]:\n",
    "    if {'player_id', 'season_year'} <= set(df.columns):\n",
    "        pid = df['player_id'].astype('string').fillna('NA')\n",
    "        season = df['season_year'].astype('Int64').astype('string').fillna('NA')\n",
    "        df['_rid'] = pid + '|' + season + f'|{tag}'\n",
    "    else:\n",
    "        df['_rid'] = df.index.map(lambda i: f\"{tag}{i:06d}\")\n",
    "\n",
    "print(f\"  L_full: {len(L_full):,} records\")\n",
    "print(f\"  R_full: {len(R_full):,} records\")\n",
    "print(f\"  S_full: {len(S_full):,} records\")\n",
    "print(\"✓ Source tables loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4035d22",
   "metadata": {},
   "source": [
    "### 1.2 Load Candidate Pairs from Hybrid Blocking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "950f903a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading candidate pairs from hybrid blocking...\n",
      "  LR candidates: 5,733,797 pairs\n",
      "  LS candidates: 215,708 pairs\n",
      "  Total candidates: 5,949,505 pairs\n",
      "✓ Candidate pairs loaded\n"
     ]
    }
   ],
   "source": [
    "# Load candidate pairs generated by hybrid blocking strategy\n",
    "print(\"Loading candidate pairs from hybrid blocking...\")\n",
    "candidates_lr = pd.read_csv(CANDIDATES_LR)\n",
    "candidates_ls = pd.read_csv(CANDIDATES_LS)\n",
    "\n",
    "print(f\"  LR candidates: {len(candidates_lr):,} pairs\")\n",
    "print(f\"  LS candidates: {len(candidates_ls):,} pairs\")\n",
    "print(f\"  Total candidates: {len(candidates_lr) + len(candidates_ls):,} pairs\")\n",
    "print(\"✓ Candidate pairs loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f5a819",
   "metadata": {},
   "source": [
    "### 1.3 Load Ground Truth Splits for Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "81c9e570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ground truth splits...\n",
      "✓ Ground truth splits loaded\n",
      "  LR: train=302, val=99, test=99\n",
      "  LS: train=299, val=96, test=105\n"
     ]
    }
   ],
   "source": [
    "# Load ground truth splits\n",
    "print(\"Loading ground truth splits...\")\n",
    "lr_train_df = pd.read_csv(LR_TRAIN)\n",
    "lr_val_df = pd.read_csv(LR_VAL)\n",
    "lr_test_df = pd.read_csv(LR_TEST)\n",
    "\n",
    "ls_train_df = pd.read_csv(LS_TRAIN)\n",
    "ls_val_df = pd.read_csv(LS_VAL)\n",
    "ls_test_df = pd.read_csv(LS_TEST)\n",
    "\n",
    "# Organize splits by edge\n",
    "splits = {\n",
    "    'LR': {'train': lr_train_df, 'val': lr_val_df, 'test': lr_test_df},\n",
    "    'LS': {'train': ls_train_df, 'val': ls_val_df, 'test': ls_test_df}\n",
    "}\n",
    "\n",
    "# Organize source tables by edge\n",
    "source_tables = {\n",
    "    'LR': (L_full, R_full),\n",
    "    'LS': (L_full, S_full)\n",
    "}\n",
    "\n",
    "# Organize candidate pairs by edge\n",
    "candidates = {\n",
    "    'LR': candidates_lr,\n",
    "    'LS': candidates_ls\n",
    "}\n",
    "\n",
    "print(\"✓ Ground truth splits loaded\")\n",
    "print(f\"  LR: train={len(lr_train_df)}, val={len(lr_val_df)}, test={len(lr_test_df)}\")\n",
    "print(f\"  LS: train={len(ls_train_df)}, val={len(ls_val_df)}, test={len(ls_test_df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869cc8e4",
   "metadata": {},
   "source": [
    "## 2. Name Normalization (Reusable)\n",
    "\n",
    "Apply consistent name normalization across all matching methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3884db79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  L_full: Created 'full_name_normalized' column\n",
      "  R_full: Created 'full_name_normalized' column\n",
      "  S_full: Created 'full_name_normalized' column\n",
      "✓ Name normalization complete\n"
     ]
    }
   ],
   "source": [
    "# Check if normalized names exist, if not, apply normalization\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def normalize_name_for_blocking(text: str) -> str:\n",
    "    r\"\"\"Normalize name for consistent matching (same as workflow notebook)\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    \n",
    "    # Decode literal backslash-x-hex patterns\n",
    "    def decode_literal_hex_sequence(match):\n",
    "        hex_bytes = []\n",
    "        for i in range(1, len(match.groups()) + 1):\n",
    "            hex_str = match.group(i)\n",
    "            try:\n",
    "                hex_bytes.append(int(hex_str, 16))\n",
    "            except ValueError:\n",
    "                return match.group(0)\n",
    "        try:\n",
    "            decoded = bytes(hex_bytes).decode('utf-8')\n",
    "            return decoded\n",
    "        except (UnicodeDecodeError, ValueError):\n",
    "            return match.group(0)\n",
    "    \n",
    "    text = re.sub(r'\\\\x([0-9a-fA-F]{2})\\\\x([0-9a-fA-F]{2})', decode_literal_hex_sequence, text)\n",
    "    text = re.sub(r'\\\\x([0-9a-fA-F]{2})\\\\x([0-9a-fA-F]{2})\\\\x([0-9a-fA-F]{2})', decode_literal_hex_sequence, text)\n",
    "    \n",
    "    def decode_single_hex(match):\n",
    "        hex_str = match.group(1)\n",
    "        try:\n",
    "            return chr(int(hex_str, 16))\n",
    "        except (ValueError, OverflowError):\n",
    "            return match.group(0)\n",
    "    text = re.sub(r'\\\\x([0-9a-fA-F]{2})', decode_single_hex, text)\n",
    "    \n",
    "    # Unicode normalization\n",
    "    text = unicodedata.normalize('NFD', text)\n",
    "    text = ''.join(c for c in text if unicodedata.category(c) != 'Mn')\n",
    "    \n",
    "    # Lowercase and strip\n",
    "    text = text.lower().strip()\n",
    "    \n",
    "    # Handle backslash escapes\n",
    "    text = text.replace('\\\\ ', ' ').replace('\\\\', ' ')\n",
    "    \n",
    "    # Standardize punctuation\n",
    "    text = text.replace('.', '').replace(',', '').replace('-', ' ').replace(\"'\", '')\n",
    "    \n",
    "    # Normalize spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove common suffixes\n",
    "    for suffix in [' jr', ' sr', ' ii', ' iii', ' iv', ' v']:\n",
    "        text = text.replace(suffix, '')\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply normalization if needed\n",
    "for name, df in [('L_full', L_full), ('R_full', R_full), ('S_full', S_full)]:\n",
    "    if 'full_name_normalized' not in df.columns and 'full_name' in df.columns:\n",
    "        df['full_name_normalized'] = df['full_name'].astype('string').map(normalize_name_for_blocking)\n",
    "        print(f\"  {name}: Created 'full_name_normalized' column\")\n",
    "    elif 'full_name_normalized' in df.columns:\n",
    "        print(f\"  {name}: 'full_name_normalized' column already exists\")\n",
    "\n",
    "print(\"✓ Name normalization complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02602ec",
   "metadata": {},
   "source": [
    "## 3. Common Matching Infrastructure (Reusable)\n",
    "\n",
    "This section defines reusable functions and frameworks for all matching methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a2cb66",
   "metadata": {},
   "source": [
    "### 3.1 Matching Function Interface\n",
    "\n",
    "Define a standard interface for matching functions. All matching methods should follow this pattern.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "4d011031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Matching function interface defined\n"
     ]
    }
   ],
   "source": [
    "# Matching function interface\n",
    "# All matching methods should return a DataFrame with columns: ['id1', 'id2', 'sim']\n",
    "# where 'sim' is the similarity score (0.0 to 1.0)\n",
    "\n",
    "def apply_matching_method(\n",
    "    edge_name: str,\n",
    "    matching_func,\n",
    "    *args,\n",
    "    **kwargs\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply a matching method to candidate pairs for a given edge.\n",
    "    \n",
    "    Args:\n",
    "        edge_name: Edge name ('LR' or 'LS')\n",
    "        matching_func: Function that takes (left_df, right_df, candidates_df, *args, **kwargs)\n",
    "                       and returns DataFrame with ['id1', 'id2', 'sim']\n",
    "        *args, **kwargs: Additional arguments to pass to matching_func\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns ['id1', 'id2', 'sim']\n",
    "    \"\"\"\n",
    "    left_df, right_df = source_tables[edge_name]\n",
    "    cand_df = candidates[edge_name]\n",
    "    \n",
    "    print(f\"\\n=== {edge_name}: Applying matching method ===\")\n",
    "    print(f\"  Candidate pairs: {len(cand_df):,}\")\n",
    "    \n",
    "    result = matching_func(left_df, right_df, cand_df, *args, **kwargs)\n",
    "    \n",
    "    # Validate result format\n",
    "    required_cols = ['id1', 'id2', 'sim']\n",
    "    if not all(col in result.columns for col in required_cols):\n",
    "        raise ValueError(f\"Matching function must return DataFrame with columns: {required_cols}\")\n",
    "    \n",
    "    print(f\"  Generated {len(result):,} scored pairs\")\n",
    "    print(f\"  Similarity range: [{result['sim'].min():.3f}, {result['sim'].max():.3f}]\")\n",
    "    print(f\"  Mean similarity: {result['sim'].mean():.3f}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"✓ Matching function interface defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae14ddb",
   "metadata": {},
   "source": [
    "### 3.2 Evaluation Functions (Reusable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "d851fe04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Evaluation functions defined\n"
     ]
    }
   ],
   "source": [
    "# Reusable evaluation functions\n",
    "from PyDI.entitymatching.evaluation import EntityMatchingEvaluator\n",
    "\n",
    "def evaluate_matching_thresholds(\n",
    "    scored_pairs: pd.DataFrame,\n",
    "    val_df: pd.DataFrame,\n",
    "    thresholds: list = [0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "    output_dir: Path = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate matching performance across different similarity thresholds.\n",
    "    \n",
    "    Args:\n",
    "        scored_pairs: DataFrame with columns ['id1', 'id2', 'sim']\n",
    "        val_df: Validation ground truth with columns ['id1', 'id2', 'label']\n",
    "        thresholds: List of similarity thresholds to evaluate\n",
    "        output_dir: Directory for evaluation outputs\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with best_threshold, best_f1, and detailed metrics\n",
    "    \"\"\"\n",
    "    best_threshold = None\n",
    "    best_f1 = 0.0\n",
    "    threshold_metrics = {}\n",
    "    \n",
    "    print(\"\\nThreshold Analysis:\")\n",
    "    print(f\"{'Threshold':<12} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'TP':<8} {'FP':<8} {'FN':<8}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        # Filter by threshold\n",
    "        matched_pairs = scored_pairs[scored_pairs['sim'] >= threshold][['id1', 'id2']].copy()\n",
    "        \n",
    "        # Evaluate\n",
    "        metrics = EntityMatchingEvaluator.evaluate_matching(\n",
    "            predicted_pairs=matched_pairs,\n",
    "            test_pairs=val_df,\n",
    "            out_dir=output_dir\n",
    "        )\n",
    "        \n",
    "        precision = metrics.get('precision', 0.0)\n",
    "        recall = metrics.get('recall', 0.0)\n",
    "        f1 = metrics.get('f1_score', 0.0)\n",
    "        tp = metrics.get('true_positives', 0)\n",
    "        fp = metrics.get('false_positives', 0)\n",
    "        fn = metrics.get('false_negatives', 0)\n",
    "        \n",
    "        threshold_metrics[threshold] = metrics\n",
    "        print(f\"{threshold:<12.1f} {precision:<12.3f} {recall:<12.3f} {f1:<12.3f} {tp:<8} {fp:<8} {fn:<8}\")\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    return {\n",
    "        'best_threshold': best_threshold,\n",
    "        'best_f1': best_f1,\n",
    "        'thresholds': thresholds,\n",
    "        'threshold_metrics': threshold_metrics\n",
    "    }\n",
    "\n",
    "def apply_season_year_constraint(\n",
    "    scored_pairs: pd.DataFrame,\n",
    "    left_df: pd.DataFrame,\n",
    "    right_df: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter scored pairs to ensure season_year matches exactly.\n",
    "    \n",
    "    Args:\n",
    "        scored_pairs: DataFrame with columns ['id1', 'id2', 'sim']\n",
    "        left_df: Left source table with '_rid' and 'season_year' columns\n",
    "        right_df: Right source table with '_rid' and 'season_year' columns\n",
    "    \n",
    "    Returns:\n",
    "        Filtered DataFrame with same columns\n",
    "    \"\"\"\n",
    "    # Merge to get season_year\n",
    "    matches = scored_pairs.merge(\n",
    "        left_df[['_rid', 'season_year']],\n",
    "        left_on='id1',\n",
    "        right_on='_rid',\n",
    "        suffixes=('', '_left')\n",
    "    ).merge(\n",
    "        right_df[['_rid', 'season_year']],\n",
    "        left_on='id2',\n",
    "        right_on='_rid',\n",
    "        suffixes=('', '_right')\n",
    "    )\n",
    "    \n",
    "    # Filter: season_year must match exactly\n",
    "    matches = matches[matches['season_year'] == matches['season_year_right']]\n",
    "    \n",
    "    # Return original columns\n",
    "    return matches[['id1', 'id2', 'sim']].copy()\n",
    "\n",
    "def apply_global_matching(\n",
    "    scored_pairs: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply greedy one-to-one matching to resolve conflicts.\n",
    "    \n",
    "    Args:\n",
    "        scored_pairs: DataFrame with columns ['id1', 'id2', 'sim']\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with one-to-one matches (same columns)\n",
    "    \"\"\"\n",
    "    # Convert 'sim' to 'score' if needed (PyDI expects 'score' column)\n",
    "    correspondences = scored_pairs.copy()\n",
    "    if 'sim' in correspondences.columns and 'score' not in correspondences.columns:\n",
    "        correspondences = correspondences.rename(columns={'sim': 'score'})\n",
    "    \n",
    "    # Apply greedy one-to-one matching using .cluster() method\n",
    "    global_matcher = GreedyOneToOneMatchingAlgorithm()\n",
    "    global_matched = global_matcher.cluster(correspondences)\n",
    "    \n",
    "    # Convert 'score' back to 'sim' if original had 'sim'\n",
    "    if 'sim' in scored_pairs.columns and 'score' in global_matched.columns:\n",
    "        global_matched = global_matched.rename(columns={'score': 'sim'})\n",
    "    \n",
    "    return global_matched\n",
    "\n",
    "print(\"✓ Evaluation functions defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b09db14",
   "metadata": {},
   "source": [
    "## 4. Matching Methods\n",
    "\n",
    "### 4.1 Rule-Based Matching\n",
    "\n",
    "Use similarity comparators to compute matching scores:\n",
    "- **Name similarity**: Levenshtein distance (weight: 0.7) and Jaccard similarity (weight: 0.3)\n",
    "- **Season year**: Must match exactly (hard constraint - if not matched, similarity = 0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "31b58bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule-based comparators configured:\n",
      "  Name comparators: Levenshtein (0.7), Jaccard (0.3)\n",
      "  Hard constraint: season_year must match exactly\n",
      "  Total comparators: 2\n"
     ]
    }
   ],
   "source": [
    "# Configure comparators for rule-based matching\n",
    "from PyDI.entitymatching.comparators import StringComparator, DateComparator\n",
    "\n",
    "# Name comparators (using normalized names)\n",
    "name_comparators = [\n",
    "    StringComparator(\n",
    "        column=\"full_name_normalized\" if 'full_name_normalized' in L_full.columns else \"full_name\",\n",
    "        similarity_function=\"levenshtein\",\n",
    "        preprocess=str.lower\n",
    "    ),\n",
    "    StringComparator(\n",
    "        column=\"full_name_normalized\" if 'full_name_normalized' in L_full.columns else \"full_name\",\n",
    "        similarity_function=\"jaccard\",\n",
    "        tokenization=\"word\",\n",
    "        preprocess=str.lower\n",
    "    )\n",
    "]\n",
    "name_weights = [0.7, 0.3]  # Emphasize Levenshtein for name matching\n",
    "\n",
    "# Matching strategy: name similarity + season_year hard constraint\n",
    "# season_year is checked as a hard constraint (must match exactly) before computing similarity\n",
    "# Only name comparators are used for similarity scoring\n",
    "\n",
    "# Use only name comparators (season_year is handled as hard constraint)\n",
    "rule_based_comparators = name_comparators\n",
    "rule_based_weights = name_weights  # Levenshtein (0.7), Jaccard (0.3)\n",
    "\n",
    "print(\"Rule-based comparators configured:\")\n",
    "print(f\"  Name comparators: Levenshtein (0.7), Jaccard (0.3)\")\n",
    "print(f\"  Hard constraint: season_year must match exactly\")\n",
    "print(f\"  Total comparators: {len(rule_based_comparators)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caedfb8",
   "metadata": {},
   "source": [
    "### 4.1.1 Define Rule-Based Matching Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f99b46f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Rule-based matching function defined\n"
     ]
    }
   ],
   "source": [
    "def rule_based_matching(\n",
    "    left_df: pd.DataFrame,\n",
    "    right_df: pd.DataFrame,\n",
    "    cand_df: pd.DataFrame,\n",
    "    comparators: list = None,\n",
    "    weights: list = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Rule-based matching using similarity comparators.\n",
    "    \n",
    "    Args:\n",
    "        left_df: Left source table\n",
    "        right_df: Right source table\n",
    "        cand_df: Candidate pairs DataFrame with columns ['id1', 'id2']\n",
    "        comparators: List of comparator objects (default: rule_based_comparators)\n",
    "        weights: List of weights for each comparator (default: rule_based_weights)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns ['id1', 'id2', 'sim']\n",
    "    \"\"\"\n",
    "    if comparators is None:\n",
    "        comparators = rule_based_comparators\n",
    "    if weights is None:\n",
    "        weights = rule_based_weights\n",
    "    \n",
    "    # Merge candidate pairs with source data\n",
    "    cand_with_data = cand_df.merge(\n",
    "        left_df,\n",
    "        left_on='id1',\n",
    "        right_on='_rid',\n",
    "        how='left',\n",
    "        suffixes=('', '_left')\n",
    "    ).merge(\n",
    "        right_df,\n",
    "        left_on='id2',\n",
    "        right_on='_rid',\n",
    "        how='left',\n",
    "        suffixes=('', '_right')\n",
    "    )\n",
    "    \n",
    "    # Compute similarity scores\n",
    "    scores_list = []\n",
    "    \n",
    "    for idx, row in cand_with_data.iterrows():\n",
    "        # Get left and right records\n",
    "        # Left record: columns from left_df (no suffix added if no conflict)\n",
    "        left_record = {col: row[col] for col in left_df.columns if col in row}\n",
    "        \n",
    "        # Right record: handle merge suffixes correctly\n",
    "        # When merging with suffixes=('', '_right'), right_df columns get '_right' suffix only if there's a conflict\n",
    "        # We need to check both the original column name and the suffixed version\n",
    "        right_record = {}\n",
    "        for col in right_df.columns:\n",
    "            # First check if column with _right suffix exists (conflict case)\n",
    "            if f\"{col}_right\" in row:\n",
    "                right_record[col] = row[f\"{col}_right\"]\n",
    "            # Then check if original column name exists (no conflict case)\n",
    "            elif col in row:\n",
    "                right_record[col] = row[col]\n",
    "            else:\n",
    "                # Column not found, use None\n",
    "                right_record[col] = None\n",
    "        \n",
    "        # Hard constraint: season_year must match exactly\n",
    "        # If season_year doesn't match, similarity is 0.0\n",
    "        left_season = left_record.get('season_year')\n",
    "        right_season = right_record.get('season_year')\n",
    "        if left_season is None or right_season is None or left_season != right_season:\n",
    "            scores_list.append(0.0)\n",
    "            continue\n",
    "        \n",
    "        # Compute weighted similarity score (only name similarity, since season_year is already checked)\n",
    "        total_score = 0.0\n",
    "        total_weight = 0.0\n",
    "        \n",
    "        for comparator, weight in zip(comparators, weights):\n",
    "            try:\n",
    "                col_name = comparator.column\n",
    "                # Get values, handling None and missing columns\n",
    "                left_val = left_record.get(col_name) or ''\n",
    "                right_val = right_record.get(col_name) or ''\n",
    "                # Convert to string and handle None\n",
    "                left_val = str(left_val) if left_val is not None else ''\n",
    "                right_val = str(right_val) if right_val is not None else ''\n",
    "                \n",
    "                # Compute similarity using comparator\n",
    "                if isinstance(comparator, StringComparator):\n",
    "                    # Apply preprocessing\n",
    "                    if comparator.preprocess:\n",
    "                        left_val = comparator.preprocess(str(left_val))\n",
    "                        right_val = comparator.preprocess(str(right_val))\n",
    "                    \n",
    "                    # Compute similarity\n",
    "                    if comparator.similarity_function == 'levenshtein':\n",
    "                        try:\n",
    "                            from Levenshtein import ratio\n",
    "                            sim = ratio(left_val, right_val)\n",
    "                        except ImportError:\n",
    "                            # Fallback to simple string comparison\n",
    "                            sim = 1.0 if left_val == right_val else 0.0\n",
    "                    elif comparator.similarity_function == 'jaccard':\n",
    "                        if comparator.tokenization == 'word':\n",
    "                            left_tokens = set(left_val.split())\n",
    "                            right_tokens = set(right_val.split())\n",
    "                            if len(left_tokens | right_tokens) == 0:\n",
    "                                sim = 1.0\n",
    "                            else:\n",
    "                                sim = len(left_tokens & right_tokens) / len(left_tokens | right_tokens)\n",
    "                        else:\n",
    "                            sim = 0.0\n",
    "                    else:\n",
    "                        sim = 0.0\n",
    "                elif isinstance(comparator, DateComparator):\n",
    "                    # Date comparison with tolerance\n",
    "                    # Note: DateComparator uses max_days_difference (in days), convert to years for year comparison\n",
    "                    try:\n",
    "                        left_year = int(float(left_val)) if left_val else None\n",
    "                        right_year = int(float(right_val)) if right_val else None\n",
    "                        if left_year is not None and right_year is not None:\n",
    "                            diff = abs(left_year - right_year)\n",
    "                            # Convert max_days_difference to years (365 days = 1 year)\n",
    "                            year_tolerance = getattr(comparator, 'max_days_difference', 365) / 365.0\n",
    "                            if diff <= year_tolerance:\n",
    "                                sim = 1.0 - (diff / (year_tolerance + 1))\n",
    "                            else:\n",
    "                                sim = 0.0\n",
    "                        else:\n",
    "                            sim = 0.0\n",
    "                    except (ValueError, TypeError):\n",
    "                        sim = 0.0\n",
    "                else:\n",
    "                    sim = 0.0\n",
    "                \n",
    "                total_score += sim * weight\n",
    "                total_weight += weight\n",
    "            except Exception:\n",
    "                # If comparator fails, skip it\n",
    "                continue\n",
    "        \n",
    "        # Normalize by total weight\n",
    "        final_score = total_score / total_weight if total_weight > 0 else 0.0\n",
    "        scores_list.append(final_score)\n",
    "    \n",
    "    # Create result DataFrame\n",
    "    result = cand_df[['id1', 'id2']].copy()\n",
    "    result['sim'] = scores_list\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"✓ Rule-based matching function defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c479cdd",
   "metadata": {},
   "source": [
    "### 4.1.2 Apply Rule-Based Matching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c66fd9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LR: Applying matching method ===\n",
      "  Candidate pairs: 5,733,797\n",
      "  Generated 5,733,797 scored pairs\n",
      "  Similarity range: [0.000, 1.000]\n",
      "  Mean similarity: 0.013\n",
      "\n",
      "=== LS: Applying matching method ===\n",
      "  Candidate pairs: 215,708\n",
      "  Generated 215,708 scored pairs\n",
      "  Similarity range: [0.000, 1.000]\n",
      "  Mean similarity: 0.037\n",
      "\n",
      "✓ Rule-based matching complete for all edges\n"
     ]
    }
   ],
   "source": [
    "# Apply rule-based matching to all edges\n",
    "matching_results_rule_based = {}\n",
    "\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    matching_results_rule_based[edge_name] = apply_matching_method(\n",
    "        edge_name,\n",
    "        rule_based_matching,\n",
    "        comparators=rule_based_comparators,\n",
    "        weights=rule_based_weights\n",
    "    )\n",
    "\n",
    "print(\"\\n✓ Rule-based matching complete for all edges\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e140d1",
   "metadata": {},
   "source": [
    "### 2.3 Match Candidate Pairs\n",
    "\n",
    "Apply RuleBasedMatcher to compute similarity scores for all candidate pairs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ee22b335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LR: Rule-Based Matching ===\n",
      "  Filtering candidate pairs by season_year constraint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.rule_based.RuleBasedMatcher - Starting Entity Matching\n",
      "[INFO ] PyDI.entitymatching.rule_based.RuleBasedMatcher - Blocking 106553 x 15215 elements\n",
      "[INFO ] PyDI.entitymatching.rule_based.RuleBasedMatcher - Matching 106553 x 15215 elements after 0:00:0.096; 130895 blocked pairs (reduction ratio: 0.9999192606183567)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  After season_year filter: 130,895 candidate pairs (from 5,733,797)\n",
      "  Computing similarity scores using PyDI RuleBasedMatcher (threshold=0.7)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.rule_based.RuleBasedMatcher - Entity Matching finished after 0:00:19.960; found 15309 correspondences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 15,309 matched pairs (above threshold 0.7)\n",
      "  Similarity score range: [0.700, 1.000]\n",
      "  Mean similarity: 0.998\n",
      "\n",
      "=== LS: Rule-Based Matching ===\n",
      "  Filtering candidate pairs by season_year constraint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.rule_based.RuleBasedMatcher - Starting Entity Matching\n",
      "[INFO ] PyDI.entitymatching.rule_based.RuleBasedMatcher - Blocking 106553 x 6743 elements\n",
      "[INFO ] PyDI.entitymatching.rule_based.RuleBasedMatcher - Matching 106553 x 6743 elements after 0:00:0.078; 9526 blocked pairs (reduction ratio: 0.9999867415811222)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  After season_year filter: 9,526 candidate pairs (from 215,708)\n",
      "  Computing similarity scores using PyDI RuleBasedMatcher (threshold=0.7)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.rule_based.RuleBasedMatcher - Entity Matching finished after 0:00:1.331; found 6725 correspondences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 6,725 matched pairs (above threshold 0.7)\n",
      "  Similarity score range: [0.700, 1.000]\n",
      "  Mean similarity: 0.997\n"
     ]
    }
   ],
   "source": [
    "# Match candidate pairs using PyDI's RuleBasedMatcher (following exercise file pattern)\n",
    "\n",
    "# Define matching threshold\n",
    "threshold = 0.7\n",
    "\n",
    "matching_results = {}\n",
    "matchers = {}\n",
    "\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    print(f\"\\n=== {edge_name}: Rule-Based Matching ===\")\n",
    "    \n",
    "    left_df, right_df = source_tables[edge_name]\n",
    "    cand_df = candidates[edge_name].copy()\n",
    "    \n",
    "    # Apply season_year hard constraint: filter candidate pairs where season_year matches\n",
    "    print(f\"  Filtering candidate pairs by season_year constraint...\")\n",
    "    cand_with_seasons = cand_df.merge(\n",
    "        left_df[['_rid', 'season_year']],\n",
    "        left_on='id1',\n",
    "        right_on='_rid',\n",
    "        how='left'\n",
    "    ).merge(\n",
    "        right_df[['_rid', 'season_year']],\n",
    "        left_on='id2',\n",
    "        right_on='_rid',\n",
    "        how='left',\n",
    "        suffixes=('', '_right')\n",
    "    )\n",
    "    \n",
    "    # Filter: season_year must match exactly\n",
    "    cand_df_filtered = cand_with_seasons[\n",
    "        (cand_with_seasons['season_year'].notna()) & \n",
    "        (cand_with_seasons['season_year_right'].notna()) &\n",
    "        (cand_with_seasons['season_year'] == cand_with_seasons['season_year_right'])\n",
    "    ][['id1', 'id2']].copy()\n",
    "    \n",
    "    print(f\"  After season_year filter: {len(cand_df_filtered):,} candidate pairs (from {len(cand_df):,})\")\n",
    "    \n",
    "    # Initialize matcher (following exercise file pattern)\n",
    "    matcher = RuleBasedMatcher()\n",
    "    \n",
    "    # Match with a single threshold (following exercise file pattern)\n",
    "    print(f\"  Computing similarity scores using PyDI RuleBasedMatcher (threshold={threshold})...\")\n",
    "    correspondences = matcher.match(\n",
    "        df_left=left_df,\n",
    "        df_right=right_df,\n",
    "        candidates=cand_df_filtered,\n",
    "        id_column='_rid',\n",
    "        comparators=rule_based_comparators,\n",
    "        weights=rule_based_weights,\n",
    "        threshold=threshold,\n",
    "        debug=False\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    matching_results[edge_name] = correspondences\n",
    "    matchers[edge_name] = matcher\n",
    "    \n",
    "    print(f\"  Generated {len(correspondences):,} matched pairs (above threshold {threshold})\")\n",
    "    if 'score' in correspondences.columns:\n",
    "        print(f\"  Similarity score range: [{correspondences['score'].min():.3f}, {correspondences['score'].max():.3f}]\")\n",
    "        print(f\"  Mean similarity: {correspondences['score'].mean():.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e4d066",
   "metadata": {},
   "source": [
    "## 3. Evaluate Matching Quality\n",
    "\n",
    "### 3.1 Evaluate on Validation Set\n",
    "\n",
    "Assess matching performance using different similarity thresholds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "3a336e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LR: Matching Evaluation (Validation Set) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Confusion Matrix:\n",
      "[INFO ] root -   True Positives:  70\n",
      "[INFO ] root -   True Negatives:  27\n",
      "[INFO ] root -   False Positives: 0\n",
      "[INFO ] root -   False Negatives: 2\n",
      "[INFO ] root - Performance Metrics:\n",
      "[INFO ] root -   Accuracy:  0.980\n",
      "[INFO ] root -   Precision: 1.000\n",
      "[INFO ] root -   Recall:    0.972\n",
      "[INFO ] root -   F1-Score:  0.986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Precision: 1.000\n",
      "  Recall:    0.972\n",
      "  F1-Score:  0.986\n",
      "  TP: 70\n",
      "  FP: 0\n",
      "  FN: 2\n",
      "\n",
      "=== LS: Matching Evaluation (Validation Set) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Confusion Matrix:\n",
      "[INFO ] root -   True Positives:  52\n",
      "[INFO ] root -   True Negatives:  39\n",
      "[INFO ] root -   False Positives: 2\n",
      "[INFO ] root -   False Negatives: 3\n",
      "[INFO ] root - Performance Metrics:\n",
      "[INFO ] root -   Accuracy:  0.948\n",
      "[INFO ] root -   Precision: 0.963\n",
      "[INFO ] root -   Recall:    0.945\n",
      "[INFO ] root -   F1-Score:  0.954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Precision: 0.963\n",
      "  Recall:    0.945\n",
      "  F1-Score:  0.954\n",
      "  TP: 52\n",
      "  FP: 2\n",
      "  FN: 3\n"
     ]
    }
   ],
   "source": [
    "# Evaluate matching on validation set (following exercise file pattern)\n",
    "# Simple evaluation with a single threshold\n",
    "\n",
    "from PyDI.entitymatching.evaluation import EntityMatchingEvaluator\n",
    "\n",
    "matching_metrics_val = {}\n",
    "\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    print(f\"\\n=== {edge_name}: Matching Evaluation (Validation Set) ===\")\n",
    "    \n",
    "    correspondences = matching_results[edge_name]\n",
    "    val_df = splits[edge_name]['val'][['id1', 'id2', 'label']].copy()\n",
    "    \n",
    "    # Evaluate matching (following exercise file pattern)\n",
    "    eval_results = EntityMatchingEvaluator.evaluate_matching(\n",
    "        correspondences=correspondences,\n",
    "        test_pairs=val_df,\n",
    "        out_dir=OUTPUT_DIR / 'matching-evaluation',\n",
    "        matcher_instance=matchers[edge_name]\n",
    "    )\n",
    "    \n",
    "    matching_metrics_val[edge_name] = eval_results\n",
    "    \n",
    "    print(f\"\\n  Precision: {eval_results.get('precision', 0.0):.3f}\")\n",
    "    print(f\"  Recall:    {eval_results.get('recall', 0.0):.3f}\")\n",
    "    print(f\"  F1-Score:  {eval_results.get('f1', 0.0):.3f}\")\n",
    "    print(f\"  TP: {eval_results.get('true_positives', 0)}\")\n",
    "    print(f\"  FP: {eval_results.get('false_positives', 0)}\")\n",
    "    print(f\"  FN: {eval_results.get('false_negatives', 0)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29886579",
   "metadata": {},
   "source": [
    "### 3.2 Analyze error cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "587a9677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LR: Error Cases Analysis\n",
      "================================================================================\n",
      "\n",
      "[FALSE NEGATIVES] (2 cases):\n",
      "\n",
      "  5961292022|2022|L <-> 5961292022|2022|R\n",
      "    Left:  'dan vogelbach' | Season: 2022 | Birth: 1992\n",
      "    Right: 'daniel vogelbach' | Season: 2022 | Birth: 1993\n",
      "    In candidates: False\n",
      "\n",
      "  5715102017|2017|L <-> 5715102017|2017|R\n",
      "    Left:  'matt boyd' | Season: 2017 | Birth: 1991\n",
      "    Right: 'matthew boyd' | Season: 2017 | Birth: 1991\n",
      "    In candidates: True\n",
      "\n",
      "[FALSE POSITIVES] (0 cases):\n",
      "\n",
      "================================================================================\n",
      "LS: Error Cases Analysis\n",
      "================================================================================\n",
      "\n",
      "[FALSE NEGATIVES] (3 cases):\n",
      "\n",
      "  6404472019|2019|L <-> 6404472019|2019|S\n",
      "    Left:  'phil ervin' | Season: 2019 | Birth: 1992\n",
      "    Right: 'phillip ervin' | Season: 2019 | Birth: 1993\n",
      "    In candidates: True\n",
      "\n",
      "  4931142016|2016|L <-> 4931142016|2016|S\n",
      "    Left:  'nori aoki' | Season: 2016 | Birth: 1982\n",
      "    Right: 'norichika aoki' | Season: 2016 | Birth: 1982\n",
      "    In candidates: True\n",
      "\n",
      "  5470072016|2016|L <-> 5470072016|2016|S\n",
      "    Left:  'robert whalen' | Season: 2016 | Birth: 1994\n",
      "    Right: 'rob whalen' | Season: 2016 | Birth: 1994\n",
      "    In candidates: True\n",
      "\n",
      "[FALSE POSITIVES] (2 cases):\n",
      "\n",
      "  6703512022|2022|L <-> 6689422022|2022|S (score: 0.730)\n",
      "    Left:  'jose rojas' | Season: 2022 | Birth: 1993\n",
      "    Right: 'josh rojas' | Season: 2022 | Birth: 1994\n",
      "\n",
      "  5167142015|2015|L <-> 6458482015|2015|S (score: 0.700)\n",
      "    Left:  'dario alvarez' | Season: 2015 | Birth: 1989\n",
      "    Right: 'dariel alvarez' | Season: 2015 | Birth: 1989\n"
     ]
    }
   ],
   "source": [
    "# Analyze error cases (False Positives and False Negatives) for tuning\n",
    "\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{edge_name}: Error Cases Analysis\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    left_df, right_df = source_tables[edge_name]\n",
    "    val_df = splits[edge_name]['val'][['id1', 'id2', 'label']].copy()\n",
    "    correspondences = matching_results[edge_name]\n",
    "    \n",
    "    # Get true matches and false matches in validation set\n",
    "    true_matches = val_df[val_df['label'].astype(str).str.strip().str.upper() == 'TRUE']\n",
    "    false_matches = val_df[val_df['label'].astype(str).str.strip().str.upper() == 'FALSE']\n",
    "    true_set = set(zip(true_matches['id1'], true_matches['id2']))\n",
    "    false_set = set(zip(false_matches['id1'], false_matches['id2']))\n",
    "    \n",
    "    # Get predicted matches (only those in validation set)\n",
    "    pred_set = set(zip(correspondences['id1'], correspondences['id2']))\n",
    "    val_set = set(zip(val_df['id1'], val_df['id2']))  # All pairs in validation set\n",
    "    \n",
    "    # False Negatives: True matches in validation set that were not predicted\n",
    "    fn_pairs = true_set - pred_set\n",
    "    print(f\"\\n[FALSE NEGATIVES] ({len(fn_pairs)} cases):\")\n",
    "    if len(fn_pairs) > 0:\n",
    "        for id1, id2 in fn_pairs:\n",
    "            left_rec = left_df[left_df['_rid'] == id1].iloc[0] if len(left_df[left_df['_rid'] == id1]) > 0 else None\n",
    "            right_rec = right_df[right_df['_rid'] == id2].iloc[0] if len(right_df[right_df['_rid'] == id2]) > 0 else None\n",
    "            \n",
    "            if left_rec is not None and right_rec is not None:\n",
    "                print(f\"\\n  {id1} <-> {id2}\")\n",
    "                print(f\"    Left:  '{left_rec.get('full_name', 'N/A')}' | Season: {left_rec.get('season_year', 'N/A')} | Birth: {left_rec.get('birth_year', 'N/A')}\")\n",
    "                print(f\"    Right: '{right_rec.get('full_name', 'N/A')}' | Season: {right_rec.get('season_year', 'N/A')} | Birth: {right_rec.get('birth_year', 'N/A')}\")\n",
    "                \n",
    "                # Check if in candidates\n",
    "                in_candidates = len(candidates[edge_name][\n",
    "                    (candidates[edge_name]['id1'] == id1) & \n",
    "                    (candidates[edge_name]['id2'] == id2)\n",
    "                ]) > 0\n",
    "                print(f\"    In candidates: {in_candidates}\")\n",
    "    \n",
    "    # False Positives: Predicted matches that are in validation set but labeled as FALSE\n",
    "    # Only analyze pairs that are in the validation set\n",
    "    fp_pairs = (pred_set & val_set) & false_set\n",
    "    print(f\"\\n[FALSE POSITIVES] ({len(fp_pairs)} cases):\")\n",
    "    if len(fp_pairs) > 0:\n",
    "        for id1, id2 in fp_pairs:\n",
    "            left_rec = left_df[left_df['_rid'] == id1].iloc[0] if len(left_df[left_df['_rid'] == id1]) > 0 else None\n",
    "            right_rec = right_df[right_df['_rid'] == id2].iloc[0] if len(right_df[right_df['_rid'] == id2]) > 0 else None\n",
    "            score_row = correspondences[(correspondences['id1'] == id1) & (correspondences['id2'] == id2)]\n",
    "            score = score_row['score'].iloc[0] if len(score_row) > 0 else None\n",
    "            \n",
    "            if left_rec is not None and right_rec is not None:\n",
    "                score_str = f\"{score:.3f}\" if score is not None else \"N/A\"\n",
    "                print(f\"\\n  {id1} <-> {id2} (score: {score_str})\")\n",
    "                print(f\"    Left:  '{left_rec.get('full_name', 'N/A')}' | Season: {left_rec.get('season_year', 'N/A')} | Birth: {left_rec.get('birth_year', 'N/A')}\")\n",
    "                print(f\"    Right: '{right_rec.get('full_name', 'N/A')}' | Season: {right_rec.get('season_year', 'N/A')} | Birth: {right_rec.get('birth_year', 'N/A')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e713603",
   "metadata": {},
   "source": [
    "### 3.3 Optimized Matching with Name Variants and Birth Year Constraint\n",
    "\n",
    "Based on error analysis, we implement three optimization strategies:\n",
    "\n",
    "1. **Name Variant Handling**: Handle common name variants (dan/daniel, matt/matthew, etc.)\n",
    "2. **Adjusted Threshold**: Lower threshold from 0.7 to 0.65 to capture more true matches\n",
    "3. **Birth Year Soft Constraint**: Apply penalty when birth years differ by more than 1 year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "21c4e226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized matching configuration:\n",
      "  Name variant dictionary: 26 variants\n",
      "  Adjusted threshold: 0.65 (from 0.7)\n",
      "  Birth year constraint: penalty=0.2 for year_diff > 1\n"
     ]
    }
   ],
   "source": [
    "# Optimized matching with name variants and birth year constraint\n",
    "\n",
    "# Strategy 1: Name variant dictionary\n",
    "NAME_VARIANTS = {\n",
    "    'dan': 'daniel',\n",
    "    'daniel': 'dan',\n",
    "    'matt': 'matthew',\n",
    "    'matthew': 'matt',\n",
    "    'jon': 'jonathon',\n",
    "    'jonathon': 'jon',\n",
    "    'phil': 'phillip',\n",
    "    'phillip': 'phil',\n",
    "    'jim': 'james',\n",
    "    'james': 'jim',\n",
    "    'bob': 'robert',\n",
    "    'robert': 'bob',\n",
    "    'bill': 'william',\n",
    "    'william': 'bill',\n",
    "    'mike': 'michael',\n",
    "    'michael': 'mike',\n",
    "    'dave': 'david',\n",
    "    'david': 'dave',\n",
    "    'chris': 'christopher',\n",
    "    'christopher': 'chris',\n",
    "    'tom': 'thomas',\n",
    "    'thomas': 'tom',\n",
    "    'ed': 'edward',\n",
    "    'edward': 'ed',\n",
    "    'rick': 'richard',\n",
    "    'richard': 'rick',\n",
    "}\n",
    "\n",
    "def check_name_variant_match(name1, name2):\n",
    "    \"\"\"\n",
    "    Check if two names are variants of each other.\n",
    "    Returns True if they are variants, False otherwise.\n",
    "    \"\"\"\n",
    "    name1_words = name1.lower().split()\n",
    "    name2_words = name2.lower().split()\n",
    "    \n",
    "    # Must have same number of words\n",
    "    if len(name1_words) != len(name2_words):\n",
    "        return False\n",
    "    \n",
    "    # Check if all words match or are variants\n",
    "    for w1, w2 in zip(name1_words, name2_words):\n",
    "        if w1 != w2:\n",
    "            # Check if they are variants\n",
    "            if not (NAME_VARIANTS.get(w1) == w2 or NAME_VARIANTS.get(w2) == w1):\n",
    "                return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def apply_birth_year_constraint(similarity_score, birth1, birth2, penalty=0.2):\n",
    "    \"\"\"\n",
    "    Apply birth year soft constraint: reduce similarity if birth years differ by more than 1 year.\n",
    "    \n",
    "    Args:\n",
    "        similarity_score: Base similarity score\n",
    "        birth1: Birth year from left record\n",
    "        birth2: Birth year from right record\n",
    "        penalty: Penalty factor (default: 0.2)\n",
    "    \n",
    "    Returns:\n",
    "        Adjusted similarity score\n",
    "    \"\"\"\n",
    "    if birth1 is None or birth2 is None:\n",
    "        return similarity_score  # If missing, don't penalize\n",
    "    \n",
    "    year_diff = abs(birth1 - birth2)\n",
    "    \n",
    "    if year_diff > 1:\n",
    "        # Difference > 1 year: apply full penalty\n",
    "        return similarity_score * (1 - penalty)\n",
    "    elif year_diff == 1:\n",
    "        # Difference = 1 year: apply half penalty\n",
    "        return similarity_score * (1 - penalty * 0.5)\n",
    "    else:\n",
    "        # Same year or difference = 0: no penalty\n",
    "        return similarity_score\n",
    "\n",
    "def compute_enhanced_similarity(left_record, right_record, comparators, weights):\n",
    "    \"\"\"\n",
    "    Compute enhanced similarity with name variant handling.\n",
    "    \n",
    "    Args:\n",
    "        left_record: Left record dictionary\n",
    "        right_record: Right record dictionary\n",
    "        comparators: List of comparator objects\n",
    "        weights: List of weights for comparators\n",
    "    \n",
    "    Returns:\n",
    "        Enhanced similarity score\n",
    "    \"\"\"\n",
    "    # Get names\n",
    "    name1 = str(left_record.get('full_name_normalized', left_record.get('full_name', ''))).lower()\n",
    "    name2 = str(right_record.get('full_name_normalized', right_record.get('full_name', ''))).lower()\n",
    "    \n",
    "    # Compute base similarity using comparators\n",
    "    base_scores = []\n",
    "    for comparator in comparators:\n",
    "        col = comparator.column\n",
    "        left_val = str(left_record.get(col, ''))\n",
    "        right_val = str(right_record.get(col, ''))\n",
    "        \n",
    "        # Compute similarity using comparator\n",
    "        if hasattr(comparator, 'similarity_function'):\n",
    "            if comparator.similarity_function == 'levenshtein':\n",
    "                from difflib import SequenceMatcher\n",
    "                sim = SequenceMatcher(None, left_val.lower(), right_val.lower()).ratio()\n",
    "            elif comparator.similarity_function == 'jaccard':\n",
    "                left_tokens = set(left_val.lower().split())\n",
    "                right_tokens = set(right_val.lower().split())\n",
    "                if len(left_tokens | right_tokens) == 0:\n",
    "                    sim = 1.0\n",
    "                else:\n",
    "                    sim = len(left_tokens & right_tokens) / len(left_tokens | right_tokens)\n",
    "            else:\n",
    "                sim = 0.0\n",
    "        else:\n",
    "            sim = 0.0\n",
    "        \n",
    "        base_scores.append(sim)\n",
    "    \n",
    "    # Weighted average\n",
    "    base_score = sum(s * w for s, w in zip(base_scores, weights))\n",
    "    \n",
    "    # Strategy 1: Check for name variant match\n",
    "    if check_name_variant_match(name1, name2):\n",
    "        # Variant match: boost score (but cap at 1.0)\n",
    "        base_score = min(1.0, base_score + 0.15)\n",
    "    \n",
    "    return base_score\n",
    "\n",
    "# Strategy 2: Adjusted threshold (lower from 0.7 to 0.65)\n",
    "optimized_threshold = 0.65\n",
    "\n",
    "print(\"Optimized matching configuration:\")\n",
    "print(f\"  Name variant dictionary: {len(NAME_VARIANTS)} variants\")\n",
    "print(f\"  Adjusted threshold: {optimized_threshold} (from 0.7)\")\n",
    "print(f\"  Birth year constraint: penalty={0.2} for year_diff > 1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "b3287f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LR: Optimized Rule-Based Matching ===\n",
      "  Filtering candidate pairs by season_year constraint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.rule_based.RuleBasedMatcher - Starting Entity Matching\n",
      "[INFO ] PyDI.entitymatching.rule_based.RuleBasedMatcher - Blocking 106553 x 15215 elements\n",
      "[INFO ] PyDI.entitymatching.rule_based.RuleBasedMatcher - Matching 106553 x 15215 elements after 0:00:0.137; 130895 blocked pairs (reduction ratio: 0.9999192606183567)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  After season_year filter: 130,895 candidate pairs (from 5,733,797)\n",
      "  Computing base similarity scores using PyDI RuleBasedMatcher...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.rule_based.RuleBasedMatcher - Entity Matching finished after 0:00:20.921; found 130895 correspondences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Base correspondences: 130,895\n",
      "  Applying name variant handling and birth year constraint...\n",
      "  Generated 15,296 matched pairs (above threshold 0.65)\n",
      "  Similarity score range: [0.655, 1.000]\n",
      "  Mean similarity: 0.946\n",
      "\n",
      "=== LS: Optimized Rule-Based Matching ===\n",
      "  Filtering candidate pairs by season_year constraint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.rule_based.RuleBasedMatcher - Starting Entity Matching\n",
      "[INFO ] PyDI.entitymatching.rule_based.RuleBasedMatcher - Blocking 106553 x 6743 elements\n",
      "[INFO ] PyDI.entitymatching.rule_based.RuleBasedMatcher - Matching 106553 x 6743 elements after 0:00:0.075; 9526 blocked pairs (reduction ratio: 0.9999867415811222)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  After season_year filter: 9,526 candidate pairs (from 215,708)\n",
      "  Computing base similarity scores using PyDI RuleBasedMatcher...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.rule_based.RuleBasedMatcher - Entity Matching finished after 0:00:1.318; found 9526 correspondences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Base correspondences: 9,526\n",
      "  Applying name variant handling and birth year constraint...\n",
      "  Generated 6,729 matched pairs (above threshold 0.65)\n",
      "  Similarity score range: [0.655, 1.000]\n",
      "  Mean similarity: 0.944\n",
      "\n",
      "✓ Optimized matching complete for all edges\n"
     ]
    }
   ],
   "source": [
    "# Apply optimized matching with name variants and birth year constraint\n",
    "\n",
    "optimized_matching_results = {}\n",
    "optimized_matchers = {}\n",
    "\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    print(f\"\\n=== {edge_name}: Optimized Rule-Based Matching ===\")\n",
    "    \n",
    "    left_df, right_df = source_tables[edge_name]\n",
    "    cand_df = candidates[edge_name].copy()\n",
    "    \n",
    "    # Apply season_year hard constraint: filter candidate pairs where season_year matches\n",
    "    print(f\"  Filtering candidate pairs by season_year constraint...\")\n",
    "    cand_with_seasons = cand_df.merge(\n",
    "        left_df[['_rid', 'season_year', 'birth_year']],\n",
    "        left_on='id1',\n",
    "        right_on='_rid',\n",
    "        how='left'\n",
    "    ).merge(\n",
    "        right_df[['_rid', 'season_year', 'birth_year']],\n",
    "        left_on='id2',\n",
    "        right_on='_rid',\n",
    "        how='left',\n",
    "        suffixes=('', '_right')\n",
    "    )\n",
    "    \n",
    "    # Filter: season_year must match exactly\n",
    "    cand_df_filtered = cand_with_seasons[\n",
    "        (cand_with_seasons['season_year'].notna()) & \n",
    "        (cand_with_seasons['season_year_right'].notna()) &\n",
    "        (cand_with_seasons['season_year'] == cand_with_seasons['season_year_right'])\n",
    "    ][['id1', 'id2', 'birth_year', 'birth_year_right']].copy()\n",
    "    \n",
    "    print(f\"  After season_year filter: {len(cand_df_filtered):,} candidate pairs (from {len(cand_df):,})\")\n",
    "    \n",
    "    # Initialize matcher\n",
    "    matcher = RuleBasedMatcher()\n",
    "    \n",
    "    # First, compute base similarity using PyDI (with original threshold to get all scores)\n",
    "    print(f\"  Computing base similarity scores using PyDI RuleBasedMatcher...\")\n",
    "    base_correspondences = matcher.match(\n",
    "        df_left=left_df,\n",
    "        df_right=right_df,\n",
    "        candidates=cand_df_filtered[['id1', 'id2']],\n",
    "        id_column='_rid',\n",
    "        comparators=rule_based_comparators,\n",
    "        weights=rule_based_weights,\n",
    "        threshold=0.0,  # Get all scores, we'll filter later\n",
    "        debug=False\n",
    "    )\n",
    "    \n",
    "    print(f\"  Base correspondences: {len(base_correspondences):,}\")\n",
    "    \n",
    "    # Apply enhanced similarity computation with name variants and birth year constraint\n",
    "    print(f\"  Applying name variant handling and birth year constraint...\")\n",
    "    \n",
    "    # Merge with source data to get names and birth years\n",
    "    enhanced_correspondences = base_correspondences.merge(\n",
    "        left_df[['_rid', 'full_name_normalized', 'full_name', 'birth_year']],\n",
    "        left_on='id1',\n",
    "        right_on='_rid',\n",
    "        how='left'\n",
    "    ).merge(\n",
    "        right_df[['_rid', 'full_name_normalized', 'full_name', 'birth_year']],\n",
    "        left_on='id2',\n",
    "        right_on='_rid',\n",
    "        how='left',\n",
    "        suffixes=('', '_right')\n",
    "    )\n",
    "    \n",
    "    # Apply name variant enhancement (Strategy 1)\n",
    "    def apply_name_variant_boost(row):\n",
    "        base_score = row['score']\n",
    "        name1 = str(row.get('full_name_normalized', row.get('full_name', ''))).lower()\n",
    "        name2 = str(row.get('full_name_normalized_right', row.get('full_name_right', ''))).lower()\n",
    "        \n",
    "        if check_name_variant_match(name1, name2):\n",
    "            # Variant match: boost score (but cap at 1.0)\n",
    "            return min(1.0, base_score + 0.15)\n",
    "        return base_score\n",
    "    \n",
    "    enhanced_correspondences['enhanced_score'] = enhanced_correspondences.apply(apply_name_variant_boost, axis=1)\n",
    "    \n",
    "    # Apply birth year constraint (Strategy 3)\n",
    "    # Convert birth_year columns to numeric (handle string types)\n",
    "    enhanced_correspondences['birth_year'] = pd.to_numeric(enhanced_correspondences['birth_year'], errors='coerce')\n",
    "    enhanced_correspondences['birth_year_right'] = pd.to_numeric(enhanced_correspondences['birth_year_right'], errors='coerce')\n",
    "    \n",
    "    enhanced_correspondences['final_score'] = enhanced_correspondences.apply(\n",
    "        lambda row: apply_birth_year_constraint(\n",
    "            row['enhanced_score'],\n",
    "            row.get('birth_year'),\n",
    "            row.get('birth_year_right'),\n",
    "            penalty=0.2\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Keep only necessary columns\n",
    "    enhanced_correspondences = enhanced_correspondences[['id1', 'id2', 'final_score']].rename(columns={'final_score': 'score'})\n",
    "    \n",
    "    # Apply optimized threshold (Strategy 2: 0.65 instead of 0.7)\n",
    "    optimized_correspondences = enhanced_correspondences[enhanced_correspondences['score'] >= optimized_threshold].copy()\n",
    "    \n",
    "    # Store results\n",
    "    optimized_matching_results[edge_name] = optimized_correspondences\n",
    "    optimized_matchers[edge_name] = matcher\n",
    "    \n",
    "    print(f\"  Generated {len(optimized_correspondences):,} matched pairs (above threshold {optimized_threshold})\")\n",
    "    if len(optimized_correspondences) > 0:\n",
    "        print(f\"  Similarity score range: [{optimized_correspondences['score'].min():.3f}, {optimized_correspondences['score'].max():.3f}]\")\n",
    "        print(f\"  Mean similarity: {optimized_correspondences['score'].mean():.3f}\")\n",
    "    \n",
    "print(\"\\n✓ Optimized matching complete for all edges\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edaae30",
   "metadata": {},
   "source": [
    "### 3.4 Evaluate Optimized Matching\n",
    "\n",
    "Evaluate the performance of optimized matching on the validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f322d9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LR: Optimized Matching Evaluation (Validation Set) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Confusion Matrix:\n",
      "[INFO ] root -   True Positives:  75\n",
      "[INFO ] root -   True Negatives:  23\n",
      "[INFO ] root -   False Positives: 0\n",
      "[INFO ] root -   False Negatives: 3\n",
      "[INFO ] root - Performance Metrics:\n",
      "[INFO ] root -   Accuracy:  0.970\n",
      "[INFO ] root -   Precision: 1.000\n",
      "[INFO ] root -   Recall:    0.962\n",
      "[INFO ] root -   F1-Score:  0.980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Precision: 1.000\n",
      "  Recall:    0.962\n",
      "  F1-Score:  0.980\n",
      "  TP: 75\n",
      "  FP: 0\n",
      "  FN: 3\n",
      "\n",
      "=== LS: Optimized Matching Evaluation (Validation Set) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Confusion Matrix:\n",
      "[INFO ] root -   True Positives:  55\n",
      "[INFO ] root -   True Negatives:  40\n",
      "[INFO ] root -   False Positives: 2\n",
      "[INFO ] root -   False Negatives: 3\n",
      "[INFO ] root - Performance Metrics:\n",
      "[INFO ] root -   Accuracy:  0.950\n",
      "[INFO ] root -   Precision: 0.965\n",
      "[INFO ] root -   Recall:    0.948\n",
      "[INFO ] root -   F1-Score:  0.957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Precision: 0.965\n",
      "  Recall:    0.948\n",
      "  F1-Score:  0.957\n",
      "  TP: 55\n",
      "  FP: 2\n",
      "  FN: 3\n",
      "\n",
      "✓ Optimized matching evaluation complete\n"
     ]
    }
   ],
   "source": [
    "# Evaluate optimized matching on validation set\n",
    "\n",
    "from PyDI.entitymatching.evaluation import EntityMatchingEvaluator\n",
    "\n",
    "optimized_matching_metrics_val = {}\n",
    "\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    print(f\"\\n=== {edge_name}: Optimized Matching Evaluation (Validation Set) ===\")\n",
    "    \n",
    "    correspondences = optimized_matching_results[edge_name]\n",
    "    val_df = splits[edge_name]['val'][['id1', 'id2', 'label']].copy()\n",
    "    \n",
    "    # Rename 'score' to match PyDI evaluator expectations (if needed)\n",
    "    correspondences_for_eval = correspondences.copy()\n",
    "    if 'score' not in correspondences_for_eval.columns and 'sim' in correspondences_for_eval.columns:\n",
    "        correspondences_for_eval = correspondences_for_eval.rename(columns={'sim': 'score'})\n",
    "    \n",
    "    # Evaluate matching\n",
    "    try:\n",
    "        eval_results = EntityMatchingEvaluator.evaluate_matching(\n",
    "            correspondences=correspondences_for_eval,\n",
    "            test_pairs=val_df,\n",
    "            out_dir=OUTPUT_DIR / 'matching-evaluation-optimized',\n",
    "            matcher_instance=optimized_matchers[edge_name]\n",
    "        )\n",
    "        \n",
    "        optimized_matching_metrics_val[edge_name] = eval_results\n",
    "        \n",
    "        print(f\"\\n  Precision: {eval_results.get('precision', 0.0):.3f}\")\n",
    "        print(f\"  Recall:    {eval_results.get('recall', 0.0):.3f}\")\n",
    "        print(f\"  F1-Score:  {eval_results.get('f1', 0.0):.3f}\")\n",
    "        print(f\"  TP: {eval_results.get('true_positives', 0)}\")\n",
    "        print(f\"  FP: {eval_results.get('false_positives', 0)}\")\n",
    "        print(f\"  FN: {eval_results.get('false_negatives', 0)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  PyDI evaluator failed: {e}\")\n",
    "        # Manual evaluation fallback\n",
    "        true_matches = val_df[val_df['label'].astype(str).str.strip().str.upper() == 'TRUE']\n",
    "        true_set = set(zip(true_matches['id1'], true_matches['id2']))\n",
    "        pred_set = set(zip(correspondences['id1'], correspondences['id2']))\n",
    "        \n",
    "        tp = len(true_set & pred_set)\n",
    "        fp = len(pred_set - true_set)\n",
    "        fn = len(true_set - pred_set)\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        \n",
    "        print(f\"\\n  Precision: {precision:.3f}\")\n",
    "        print(f\"  Recall:    {recall:.3f}\")\n",
    "        print(f\"  F1-Score:  {f1:.3f}\")\n",
    "        print(f\"  TP: {tp}\")\n",
    "        print(f\"  FP: {fp}\")\n",
    "        print(f\"  FN: {fn}\")\n",
    "\n",
    "print(\"\\n✓ Optimized matching evaluation complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2d0e10",
   "metadata": {},
   "source": [
    "### 3.5 Comparison: Original vs Optimized Matching\n",
    "\n",
    "Compare the performance of original matching (threshold=0.7) with optimized matching (name variants + birth year constraint + threshold=0.65).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "a50f06af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Matching Performance Comparison: Original vs Optimized\n",
      "================================================================================\n",
      "\n",
      "LR Edge:\n",
      "--------------------------------------------------------------------------------\n",
      "  Metric              | Original  | Optimized | Improvement\n",
      "  --------------------|-----------|-----------|------------\n",
      "  Precision           |   1.000  |   1.000  |  +0.000\n",
      "  Recall              |   0.972  |   0.962  |  -0.011\n",
      "  F1-Score            |   0.986  |   0.980  |  -0.006\n",
      "  True Positives      |      70  |      75  |      +5\n",
      "  False Positives     |       0  |       0  |      +0\n",
      "  False Negatives     |       2  |       3  |      +1\n",
      "\n",
      "LS Edge:\n",
      "--------------------------------------------------------------------------------\n",
      "  Metric              | Original  | Optimized | Improvement\n",
      "  --------------------|-----------|-----------|------------\n",
      "  Precision           |   0.963  |   0.965  |  +0.002\n",
      "  Recall              |   0.945  |   0.948  |  +0.003\n",
      "  F1-Score            |   0.954  |   0.957  |  +0.002\n",
      "  True Positives      |      52  |      55  |      +3\n",
      "  False Positives     |       2  |       2  |      +0\n",
      "  False Negatives     |       3  |       3  |      +0\n",
      "\n",
      "================================================================================\n",
      "Summary:\n",
      "================================================================================\n",
      "  Optimizations applied:\n",
      "    1. Name variant handling (dan/daniel, matt/matthew, etc.)\n",
      "    2. Lowered threshold from 0.7 to 0.65\n",
      "    3. Birth year soft constraint (penalty for year_diff > 1)\n",
      "\n",
      "  Comparison results saved to: /Users/zhangzihan/Desktop/WBI_project/Schema_Mapped_Datasets/data/output/matching/matching-comparison.csv\n"
     ]
    }
   ],
   "source": [
    "# Compare original vs optimized matching results\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Matching Performance Comparison: Original vs Optimized\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison_data = []\n",
    "\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    print(f\"\\n{edge_name} Edge:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Original results\n",
    "    orig_metrics = matching_metrics_val.get(edge_name, {})\n",
    "    orig_precision = orig_metrics.get('precision', 0.0)\n",
    "    orig_recall = orig_metrics.get('recall', 0.0)\n",
    "    orig_f1 = orig_metrics.get('f1', 0.0)\n",
    "    orig_tp = orig_metrics.get('true_positives', 0)\n",
    "    orig_fp = orig_metrics.get('false_positives', 0)\n",
    "    orig_fn = orig_metrics.get('false_negatives', 0)\n",
    "    \n",
    "    # Optimized results\n",
    "    opt_metrics = optimized_matching_metrics_val.get(edge_name, {})\n",
    "    opt_precision = opt_metrics.get('precision', 0.0)\n",
    "    opt_recall = opt_metrics.get('recall', 0.0)\n",
    "    opt_f1 = opt_metrics.get('f1', 0.0)\n",
    "    opt_tp = opt_metrics.get('true_positives', 0)\n",
    "    opt_fp = opt_metrics.get('false_positives', 0)\n",
    "    opt_fn = opt_metrics.get('false_negatives', 0)\n",
    "    \n",
    "    # Calculate improvements\n",
    "    precision_improvement = opt_precision - orig_precision\n",
    "    recall_improvement = opt_recall - orig_recall\n",
    "    f1_improvement = opt_f1 - orig_f1\n",
    "    tp_improvement = opt_tp - orig_tp\n",
    "    fp_improvement = opt_fp - orig_fp\n",
    "    fn_improvement = opt_fn - orig_fn\n",
    "    \n",
    "    print(f\"  Metric              | Original  | Optimized | Improvement\")\n",
    "    print(f\"  --------------------|-----------|-----------|------------\")\n",
    "    print(f\"  Precision           | {orig_precision:7.3f}  | {opt_precision:7.3f}  | {precision_improvement:+7.3f}\")\n",
    "    print(f\"  Recall              | {orig_recall:7.3f}  | {opt_recall:7.3f}  | {recall_improvement:+7.3f}\")\n",
    "    print(f\"  F1-Score            | {orig_f1:7.3f}  | {opt_f1:7.3f}  | {f1_improvement:+7.3f}\")\n",
    "    print(f\"  True Positives      | {orig_tp:7d}  | {opt_tp:7d}  | {tp_improvement:+7d}\")\n",
    "    print(f\"  False Positives     | {orig_fp:7d}  | {opt_fp:7d}  | {fp_improvement:+7d}\")\n",
    "    print(f\"  False Negatives     | {orig_fn:7d}  | {opt_fn:7d}  | {fn_improvement:+7d}\")\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'edge': edge_name,\n",
    "        'original_precision': orig_precision,\n",
    "        'optimized_precision': opt_precision,\n",
    "        'original_recall': orig_recall,\n",
    "        'optimized_recall': opt_recall,\n",
    "        'original_f1': orig_f1,\n",
    "        'optimized_f1': opt_f1,\n",
    "        'precision_improvement': precision_improvement,\n",
    "        'recall_improvement': recall_improvement,\n",
    "        'f1_improvement': f1_improvement,\n",
    "    })\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Summary:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"  Optimizations applied:\")\n",
    "print(f\"    1. Name variant handling (dan/daniel, matt/matthew, etc.)\")\n",
    "print(f\"    2. Lowered threshold from 0.7 to 0.65\")\n",
    "print(f\"    3. Birth year soft constraint (penalty for year_diff > 1)\")\n",
    "\n",
    "# Save comparison results\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df.to_csv(OUTPUT_DIR / 'matching-comparison.csv', index=False)\n",
    "print(f\"\\n  Comparison results saved to: {OUTPUT_DIR / 'matching-comparison.csv'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a36e70",
   "metadata": {},
   "source": [
    "### 3.6 Cluster Consistency Analysis\n",
    "\n",
    "Analyze the cluster structure to identify any inconsistencies that our evaluation set may miss. The `EntityMatchingEvaluator` offers the `create_cluster_size_distribution` method for this purpose.\n",
    "\n",
    "**Important Notes:**\n",
    "- Your evaluation is only as good as your evaluation set! This is also true for the matching step!\n",
    "- If you see an F1 of 95% and you are happy, but if the evaluation set does not accurately represent your data, the cluster size distribution is one way you can spot this.\n",
    "- Seeing many clusters with a size larger than 2 when you are sure your source datasets are deduplicated should make you question your evaluation set and manually check what is going on!\n",
    "- In this stage of the project, you will iteratively refine your evaluation sets not only based on the metrics you see but also manual inspection of debug logs and cluster size distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "7d1bfefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Cluster Consistency Analysis: Original Matching\n",
      "================================================================================\n",
      "\n",
      "LR Edge:\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.evaluation - Cluster Size Distribution of 15102 clusters:\n",
      "[INFO ] PyDI.entitymatching.evaluation - \tCluster Size\t| Frequency\t| Percentage\n",
      "[INFO ] PyDI.entitymatching.evaluation - \t──────────────────────────────────────────────────\n",
      "[INFO ] PyDI.entitymatching.evaluation - \t\t2\t|\t15000\t|\t99.32%\n",
      "[INFO ] PyDI.entitymatching.evaluation - \t\t3\t|\t52\t|\t0.34%\n",
      "[INFO ] PyDI.entitymatching.evaluation - \t\t4\t|\t47\t|\t0.31%\n",
      "[INFO ] PyDI.entitymatching.evaluation - \t\t5\t|\t1\t|\t0.01%\n",
      "[INFO ] PyDI.entitymatching.evaluation - \t\t6\t|\t1\t|\t0.01%\n",
      "[INFO ] PyDI.entitymatching.evaluation - \t\t7\t|\t1\t|\t0.01%\n",
      "[INFO ] root - Cluster size distribution written to /Users/zhangzihan/Desktop/WBI_project/Schema_Mapped_Datasets/data/output/matching/cluster_analysis/original/cluster_size_distribution.csv\n",
      "[INFO ] PyDI.entitymatching.evaluation - Cluster Size Distribution of 6677 clusters:\n",
      "[INFO ] PyDI.entitymatching.evaluation - \tCluster Size\t| Frequency\t| Percentage\n",
      "[INFO ] PyDI.entitymatching.evaluation - \t──────────────────────────────────────────────────\n",
      "[INFO ] PyDI.entitymatching.evaluation - \t\t2\t|\t6645\t|\t99.52%\n",
      "[INFO ] PyDI.entitymatching.evaluation - \t\t3\t|\t22\t|\t0.33%\n",
      "[INFO ] PyDI.entitymatching.evaluation - \t\t4\t|\t10\t|\t0.15%\n",
      "[INFO ] root - Cluster size distribution written to /Users/zhangzihan/Desktop/WBI_project/Schema_Mapped_Datasets/data/output/matching/cluster_analysis/original/cluster_size_distribution.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster Size Distribution:\n",
      " cluster_size  frequency  percentage\n",
      "            2      15000   99.324593\n",
      "            3         52    0.344325\n",
      "            4         47    0.311217\n",
      "            5          1    0.006622\n",
      "            6          1    0.006622\n",
      "            7          1    0.006622\n",
      "\n",
      "LS Edge:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Cluster Size Distribution:\n",
      " cluster_size  frequency  percentage\n",
      "            2       6645   99.520743\n",
      "            3         22    0.329489\n",
      "            4         10    0.149768\n",
      "\n",
      "✓ Original matching cluster analysis complete\n"
     ]
    }
   ],
   "source": [
    "# Cluster Consistency Analysis for Original Matching\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Cluster Consistency Analysis: Original Matching\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "original_cluster_distributions = {}\n",
    "\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    print(f\"\\n{edge_name} Edge:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    correspondences = matching_results[edge_name]\n",
    "    \n",
    "    # Create cluster size distribution\n",
    "    cluster_distribution = EntityMatchingEvaluator.create_cluster_size_distribution(\n",
    "        correspondences=correspondences,\n",
    "        out_dir=OUTPUT_DIR / \"cluster_analysis\" / \"original\"\n",
    "    )\n",
    "    \n",
    "    original_cluster_distributions[edge_name] = cluster_distribution\n",
    "    \n",
    "    print(f\"\\nCluster Size Distribution:\")\n",
    "    if cluster_distribution is not None and len(cluster_distribution) > 0:\n",
    "        print(cluster_distribution.to_string(index=False))\n",
    "    else:\n",
    "        print(\"  No cluster distribution data available\")\n",
    "\n",
    "print(\"\\n✓ Original matching cluster analysis complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "e8169e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Cluster Consistency Analysis: Optimized Matching\n",
      "================================================================================\n",
      "\n",
      "LR Edge:\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.evaluation - Cluster Size Distribution of 15091 clusters:\n",
      "[INFO ] PyDI.entitymatching.evaluation - \tCluster Size\t| Frequency\t| Percentage\n",
      "[INFO ] PyDI.entitymatching.evaluation - \t──────────────────────────────────────────────────\n",
      "[INFO ] PyDI.entitymatching.evaluation - \t\t2\t|\t14977\t|\t99.24%\n",
      "[INFO ] PyDI.entitymatching.evaluation - \t\t3\t|\t50\t|\t0.33%\n",
      "[INFO ] PyDI.entitymatching.evaluation - \t\t4\t|\t62\t|\t0.41%\n",
      "[INFO ] PyDI.entitymatching.evaluation - \t\t5\t|\t2\t|\t0.01%\n",
      "[INFO ] root - Cluster size distribution written to /Users/zhangzihan/Desktop/WBI_project/Schema_Mapped_Datasets/data/output/matching/cluster_analysis/optimized/cluster_size_distribution.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster Size Distribution:\n",
      " cluster_size  frequency  percentage\n",
      "            2      14977   99.244583\n",
      "            3         50    0.331323\n",
      "            4         62    0.410841\n",
      "            5          2    0.013253\n",
      "\n",
      "Summary:\n",
      "  Total clusters: 15091\n",
      "  Clusters with size 2: 14977 (99.24%)\n",
      "  Clusters with size > 2: 114 (0.76%)\n",
      "\n",
      "   Warning: Found 114 clusters with size > 2.\n",
      "     This may indicate issues with the evaluation set or data quality.\n",
      "     Consider manually inspecting these clusters.\n",
      "\n",
      "LS Edge:\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.evaluation - Cluster Size Distribution of 6674 clusters:\n",
      "[INFO ] PyDI.entitymatching.evaluation - \tCluster Size\t| Frequency\t| Percentage\n",
      "[INFO ] PyDI.entitymatching.evaluation - \t──────────────────────────────────────────────────\n",
      "[INFO ] PyDI.entitymatching.evaluation - \t\t2\t|\t6635\t|\t99.42%\n",
      "[INFO ] PyDI.entitymatching.evaluation - \t\t3\t|\t28\t|\t0.42%\n",
      "[INFO ] PyDI.entitymatching.evaluation - \t\t4\t|\t10\t|\t0.15%\n",
      "[INFO ] PyDI.entitymatching.evaluation - \t\t5\t|\t1\t|\t0.01%\n",
      "[INFO ] root - Cluster size distribution written to /Users/zhangzihan/Desktop/WBI_project/Schema_Mapped_Datasets/data/output/matching/cluster_analysis/optimized/cluster_size_distribution.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster Size Distribution:\n",
      " cluster_size  frequency  percentage\n",
      "            2       6635   99.415643\n",
      "            3         28    0.419539\n",
      "            4         10    0.149835\n",
      "            5          1    0.014984\n",
      "\n",
      "Summary:\n",
      "  Total clusters: 6674\n",
      "  Clusters with size 2: 6635 (99.42%)\n",
      "  Clusters with size > 2: 39 (0.58%)\n",
      "\n",
      "   Warning: Found 39 clusters with size > 2.\n",
      "     This may indicate issues with the evaluation set or data quality.\n",
      "     Consider manually inspecting these clusters.\n",
      "\n",
      "✓ Optimized matching cluster analysis complete\n"
     ]
    }
   ],
   "source": [
    "# Cluster Consistency Analysis for Optimized Matching\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Cluster Consistency Analysis: Optimized Matching\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "optimized_cluster_distributions = {}\n",
    "\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    print(f\"\\n{edge_name} Edge:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    correspondences = optimized_matching_results[edge_name]\n",
    "    \n",
    "    # Create cluster size distribution\n",
    "    cluster_distribution = EntityMatchingEvaluator.create_cluster_size_distribution(\n",
    "        correspondences=correspondences,\n",
    "        out_dir=OUTPUT_DIR / \"cluster_analysis\" / \"optimized\"\n",
    "    )\n",
    "    \n",
    "    optimized_cluster_distributions[edge_name] = cluster_distribution\n",
    "    \n",
    "    print(f\"\\nCluster Size Distribution:\")\n",
    "    if cluster_distribution is not None and len(cluster_distribution) > 0:\n",
    "        print(cluster_distribution.to_string(index=False))\n",
    "        \n",
    "        # Calculate summary statistics\n",
    "        # Note: PyDI returns column names in lowercase: 'cluster_size', 'frequency', 'percentage'\n",
    "        total_clusters = cluster_distribution['frequency'].sum()\n",
    "        clusters_size_2 = cluster_distribution[cluster_distribution['cluster_size'] == 2]['frequency'].sum() if len(cluster_distribution[cluster_distribution['cluster_size'] == 2]) > 0 else 0\n",
    "        clusters_size_gt_2 = total_clusters - clusters_size_2\n",
    "        \n",
    "        print(f\"\\nSummary:\")\n",
    "        print(f\"  Total clusters: {total_clusters}\")\n",
    "        print(f\"  Clusters with size 2: {clusters_size_2} ({clusters_size_2/total_clusters*100:.2f}%)\")\n",
    "        print(f\"  Clusters with size > 2: {clusters_size_gt_2} ({clusters_size_gt_2/total_clusters*100:.2f}%)\")\n",
    "        \n",
    "        if clusters_size_gt_2 > 0:\n",
    "            print(f\"\\n   Warning: Found {clusters_size_gt_2} clusters with size > 2.\")\n",
    "            print(f\"     This may indicate issues with the evaluation set or data quality.\")\n",
    "            print(f\"     Consider manually inspecting these clusters.\")\n",
    "    else:\n",
    "        print(\"  No cluster distribution data available\")\n",
    "\n",
    "print(\"\\n✓ Optimized matching cluster analysis complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f83d8b",
   "metadata": {},
   "source": [
    "### 3.7 Cluster Distribution Comparison\n",
    "\n",
    "Compare cluster size distributions between original and optimized matching results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "857fbba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Cluster Distribution Comparison: Original vs Optimized\n",
      "================================================================================\n",
      "\n",
      "LR Edge:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  Metric                    | Original  | Optimized | Change\n",
      "  --------------------------|-----------|-----------|--------\n",
      "  Total Clusters            |     15102 |     15091 |    -11\n",
      "  Clusters with Size 2       |     15000 |     14977 |    -23\n",
      "  Clusters with Size > 2     |       102 |       114 |    +12\n",
      "  % Clusters with Size 2     |    99.32% |    99.24% |  -0.08%\n",
      "  % Clusters with Size > 2   |     0.68% |     0.76% |  +0.08%\n",
      "\n",
      "  Detailed Cluster Size Distribution:\n",
      "\n",
      "  Original Matching:\n",
      " cluster_size  frequency  percentage\n",
      "            2      15000   99.324593\n",
      "            3         52    0.344325\n",
      "            4         47    0.311217\n",
      "            5          1    0.006622\n",
      "            6          1    0.006622\n",
      "            7          1    0.006622\n",
      "\n",
      "  Optimized Matching:\n",
      " cluster_size  frequency  percentage\n",
      "            2      14977   99.244583\n",
      "            3         50    0.331323\n",
      "            4         62    0.410841\n",
      "            5          2    0.013253\n",
      "\n",
      "LS Edge:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  Metric                    | Original  | Optimized | Change\n",
      "  --------------------------|-----------|-----------|--------\n",
      "  Total Clusters            |      6677 |      6674 |     -3\n",
      "  Clusters with Size 2       |      6645 |      6635 |    -10\n",
      "  Clusters with Size > 2     |        32 |        39 |     +7\n",
      "  % Clusters with Size 2     |    99.52% |    99.42% |  -0.11%\n",
      "  % Clusters with Size > 2   |     0.48% |     0.58% |  +0.11%\n",
      "\n",
      "  Detailed Cluster Size Distribution:\n",
      "\n",
      "  Original Matching:\n",
      " cluster_size  frequency  percentage\n",
      "            2       6645   99.520743\n",
      "            3         22    0.329489\n",
      "            4         10    0.149768\n",
      "\n",
      "  Optimized Matching:\n",
      " cluster_size  frequency  percentage\n",
      "            2       6635   99.415643\n",
      "            3         28    0.419539\n",
      "            4         10    0.149835\n",
      "            5          1    0.014984\n",
      "\n",
      "✓ Cluster distribution comparison complete\n"
     ]
    }
   ],
   "source": [
    "# Compare cluster distributions between original and optimized matching\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Cluster Distribution Comparison: Original vs Optimized\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    print(f\"\\n{edge_name} Edge:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    orig_dist = original_cluster_distributions.get(edge_name)\n",
    "    opt_dist = optimized_cluster_distributions.get(edge_name)\n",
    "    \n",
    "    if orig_dist is not None and opt_dist is not None:\n",
    "        # Calculate total clusters\n",
    "        # Note: PyDI returns column names in lowercase: 'cluster_size', 'frequency', 'percentage'\n",
    "        orig_total = orig_dist['frequency'].sum()\n",
    "        opt_total = opt_dist['frequency'].sum()\n",
    "        \n",
    "        # Calculate clusters with size 2\n",
    "        orig_size_2 = orig_dist[orig_dist['cluster_size'] == 2]['frequency'].sum() if len(orig_dist[orig_dist['cluster_size'] == 2]) > 0 else 0\n",
    "        opt_size_2 = opt_dist[opt_dist['cluster_size'] == 2]['frequency'].sum() if len(opt_dist[opt_dist['cluster_size'] == 2]) > 0 else 0\n",
    "        \n",
    "        # Calculate clusters with size > 2\n",
    "        orig_size_gt_2 = orig_total - orig_size_2\n",
    "        opt_size_gt_2 = opt_total - opt_size_2\n",
    "        \n",
    "        print(f\"\\n  Metric                    | Original  | Optimized | Change\")\n",
    "        print(f\"  --------------------------|-----------|-----------|--------\")\n",
    "        print(f\"  Total Clusters            | {orig_total:9d} | {opt_total:9d} | {opt_total - orig_total:+6d}\")\n",
    "        print(f\"  Clusters with Size 2       | {orig_size_2:9d} | {opt_size_2:9d} | {opt_size_2 - orig_size_2:+6d}\")\n",
    "        print(f\"  Clusters with Size > 2     | {orig_size_gt_2:9d} | {opt_size_gt_2:9d} | {opt_size_gt_2 - orig_size_gt_2:+6d}\")\n",
    "        print(f\"  % Clusters with Size 2     | {orig_size_2/orig_total*100:8.2f}% | {opt_size_2/opt_total*100:8.2f}% | {(opt_size_2/opt_total - orig_size_2/orig_total)*100:+6.2f}%\")\n",
    "        print(f\"  % Clusters with Size > 2   | {orig_size_gt_2/orig_total*100:8.2f}% | {opt_size_gt_2/opt_total*100:8.2f}% | {(opt_size_gt_2/opt_total - orig_size_gt_2/orig_total)*100:+6.2f}%\")\n",
    "        \n",
    "        # Show detailed distribution comparison if there are differences\n",
    "        if orig_size_gt_2 != opt_size_gt_2:\n",
    "            print(f\"\\n  Detailed Cluster Size Distribution:\")\n",
    "            print(f\"\\n  Original Matching:\")\n",
    "            print(orig_dist.to_string(index=False))\n",
    "            print(f\"\\n  Optimized Matching:\")\n",
    "            print(opt_dist.to_string(index=False))\n",
    "    else:\n",
    "        print(\"  Cluster distribution data not available for comparison\")\n",
    "\n",
    "print(\"\\n✓ Cluster distribution comparison complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b796819",
   "metadata": {},
   "source": [
    "## 4. ML-Based Matching\n",
    "\n",
    "For use with scikit-learn classifiers. Comparators are the features. Train on labeled pairs to learn optimal weights.\n",
    "\n",
    "**Feature Extraction**: Feature extraction converts record pairs into feature vectors using the set of comparators. The `FeatureExtractor` class handles this transformation.\n",
    "\n",
    "**Workflow**:\n",
    "1. Define feature extractors (comparators)\n",
    "2. Extract features from training pairs\n",
    "3. Train a classifier (e.g., RandomForestClassifier)\n",
    "4. Apply ML-based matcher to candidate pairs\n",
    "5. Evaluate performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8fce1e",
   "metadata": {},
   "source": [
    "### 4.0 Export Candidate Error Cases\n",
    "\n",
    "Persist currently detected validation errors to `data/output/gt/manual_cases/` so the ground-truth notebook can ingest them (section 5.8). Run this after completing the rule-based/optimized evaluations and before the ML feature extractor setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f3b8dcaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LR] Exported 7 cases to /Users/zhangzihan/Desktop/WBI_project/Schema_Mapped_Datasets/data/output/gt/manual_cases/manual_cases_LR.csv\n",
      "[LS] Exported 10 cases to /Users/zhangzihan/Desktop/WBI_project/Schema_Mapped_Datasets/data/output/gt/manual_cases/manual_cases_LS.csv\n"
     ]
    }
   ],
   "source": [
    "# Export candidate error cases for GT augmentation (section 5.8 in GT notebook)\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "MANUAL_CASES_DIR = BASE_DIR / 'data' / 'output' / 'gt' / 'manual_cases'\n",
    "MANUAL_CASES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "error_records = {'LR': [], 'LS': []}\n",
    "\n",
    "def _collect_errors(result_dict, label: str):\n",
    "    if result_dict is None:\n",
    "        return\n",
    "    for edge_name in ['LR', 'LS']:\n",
    "        if edge_name not in result_dict:\n",
    "            continue\n",
    "        correspondences = result_dict[edge_name]\n",
    "        val_df = splits[edge_name]['val'][['id1', 'id2', 'label']].copy()\n",
    "        val_df['label'] = val_df['label'].astype(str).str.strip().str.upper()\n",
    "        true_set = set(zip(val_df[val_df['label'] == 'TRUE']['id1'], val_df[val_df['label'] == 'TRUE']['id2']))\n",
    "        false_set = set(zip(val_df[val_df['label'] == 'FALSE']['id1'], val_df[val_df['label'] == 'FALSE']['id2']))\n",
    "        pred_set = set(zip(correspondences['id1'], correspondences['id2']))\n",
    "        val_pairs = set(zip(val_df['id1'], val_df['id2']))\n",
    "\n",
    "        fn_pairs = true_set - pred_set\n",
    "        for id1, id2 in fn_pairs:\n",
    "            error_records[edge_name].append({\n",
    "                'id1': id1,\n",
    "                'id2': id2,\n",
    "                'label': 'TRUE',\n",
    "                'edge': edge_name,\n",
    "                'source': label,\n",
    "                'error_type': 'FN'\n",
    "            })\n",
    "\n",
    "        fp_pairs = (pred_set & val_pairs) & false_set\n",
    "        for id1, id2 in fp_pairs:\n",
    "            error_records[edge_name].append({\n",
    "                'id1': id1,\n",
    "                'id2': id2,\n",
    "                'label': 'FALSE',\n",
    "                'edge': edge_name,\n",
    "                'source': label,\n",
    "                'error_type': 'FP'\n",
    "            })\n",
    "\n",
    "_collect_errors(globals().get('matching_results'), 'rule_based')\n",
    "_collect_errors(globals().get('optimized_matching_results'), 'optimized')\n",
    "_collect_errors(globals().get('logreg_matching_results'), 'logreg')\n",
    "_collect_errors(globals().get('ml_matching_results'), 'random_forest')\n",
    "_collect_errors(globals().get('gb_matching_results'), 'gradient_boosting')\n",
    "_collect_errors(globals().get('xgb_matching_results'), 'xgboost')\n",
    "\n",
    "for edge_name, records in error_records.items():\n",
    "    if not records:\n",
    "        continue\n",
    "    df_edge = pd.DataFrame(records)\n",
    "    manual_path = MANUAL_CASES_DIR / f'manual_cases_{edge_name}.csv'\n",
    "    if manual_path.exists():\n",
    "        existing = pd.read_csv(manual_path)\n",
    "        # Align columns\n",
    "        for col in df_edge.columns:\n",
    "            if col not in existing.columns:\n",
    "                existing[col] = ''\n",
    "        for col in existing.columns:\n",
    "            if col not in df_edge.columns:\n",
    "                df_edge[col] = ''\n",
    "        df_edge = pd.concat([existing, df_edge], ignore_index=True)\n",
    "    df_edge = df_edge.drop_duplicates(subset=['id1', 'id2'])\n",
    "    df_edge.to_csv(manual_path, index=False)\n",
    "    print(f\"[{edge_name}] Exported {len(df_edge)} cases to {manual_path}\")\n",
    "\n",
    "if not any(error_records.values()):\n",
    "    print(\"No error cases collected (ensure evaluation cells were executed).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a004cd2",
   "metadata": {},
   "source": [
    "### 4.1 Define Feature Extractors (Comparators)\n",
    "\n",
    "Define feature extractors for ML training. Enhanced with birth year comparator to improve matching accuracy:\n",
    "- **Name similarity**: Levenshtein distance and Jaccard similarity on normalized names\n",
    "- **Birth year**: DateComparator to penalize pairs with different birth years (max_days_difference=365)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "56646c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML-based matching feature extractors configured:\n",
      "  Comparators: 3\n",
      "    - Levenshtein distance on normalized name\n",
      "    - Jaccard similarity on normalized name (word tokenization)\n",
      "    - Birth year comparator (max_days_difference=365)\n",
      "✓ Feature extractor initialized\n"
     ]
    }
   ],
   "source": [
    "# Define feature extractors (comparators) for ML-based matching\n",
    "# Enhanced with birth year comparator to improve matching accuracy\n",
    "\n",
    "from PyDI.entitymatching import MLBasedMatcher, FeatureExtractor, StringComparator\n",
    "from PyDI.entitymatching.comparators import DateComparator\n",
    "from PyDI.io import load_csv\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from PyDI.entitymatching import VectorFeatureExtractor\n",
    "\n",
    "ml_comparators = [\n",
    "    StringComparator(\n",
    "        column=\"full_name_normalized\" if 'full_name_normalized' in L_full.columns else \"full_name\",\n",
    "        similarity_function=\"levenshtein\",\n",
    "        preprocess=str.lower\n",
    "    ),\n",
    "    StringComparator(\n",
    "        column=\"full_name_normalized\" if 'full_name_normalized' in L_full.columns else \"full_name\",\n",
    "        similarity_function=\"jaccard\",\n",
    "        tokenization=\"word\",\n",
    "        preprocess=str.lower\n",
    "    ),\n",
    "    # Enhanced: Add birth year comparator to penalize pairs with different birth years\n",
    "    DateComparator(\n",
    "        column=\"birth_year\",\n",
    "        max_days_difference=365  # Allow 1 year difference (365 days)\n",
    "    )\n",
    "]\n",
    "\n",
    "# Initialize feature extractor\n",
    "ml_feature_extractor = FeatureExtractor(ml_comparators)\n",
    "\n",
    "print(\"ML-based matching feature extractors configured:\")\n",
    "print(f\"  Comparators: {len(ml_comparators)}\")\n",
    "print(f\"    - Levenshtein distance on normalized name\")\n",
    "print(f\"    - Jaccard similarity on normalized name (word tokenization)\")\n",
    "print(f\"    - Birth year comparator (max_days_difference=365)\")\n",
    "print(\"✓ Feature extractor initialized\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e15b00",
   "metadata": {},
   "source": [
    "### 4.2 Train LogisticRegression Matcher\n",
    "\n",
    "Train a baseline LogisticRegression model using the shared feature extractor. This provides a lightweight reference point before the tree-based ensembles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "49b58dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LR: Training LogisticRegression Matcher ===\n",
      "  Training pairs: 302\n",
      "    True matches: 217\n",
      "    False matches: 85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Label distribution: 217 positive, 85 negative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training features shape: (302, 3)\n",
      "  Feature columns: ['StringComparator(full_name_normalized, levenshtein, tokenization=char, list_strategy=None)', 'StringComparator(full_name_normalized, jaccard, tokenization=word, list_strategy=None)', 'DateComparator(birth_year, list_strategy=None)']\n",
      "\n",
      "  Top coefficients (magnitude):\n",
      "                                                                                   feature  coefficient\n",
      "    StringComparator(full_name_normalized, jaccard, tokenization=word, list_strategy=None)     3.518803\n",
      "StringComparator(full_name_normalized, levenshtein, tokenization=char, list_strategy=None)     2.701644\n",
      "                                            DateComparator(birth_year, list_strategy=None)     1.661940\n",
      "  Intercept: -4.6522\n",
      "  ✓ LogisticRegression trained for LR\n",
      "\n",
      "=== LS: Training LogisticRegression Matcher ===\n",
      "  Training pairs: 299\n",
      "    True matches: 164\n",
      "    False matches: 135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Label distribution: 164 positive, 135 negative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training features shape: (299, 3)\n",
      "  Feature columns: ['StringComparator(full_name_normalized, levenshtein, tokenization=char, list_strategy=None)', 'StringComparator(full_name_normalized, jaccard, tokenization=word, list_strategy=None)', 'DateComparator(birth_year, list_strategy=None)']\n",
      "\n",
      "  Top coefficients (magnitude):\n",
      "                                                                                   feature  coefficient\n",
      "StringComparator(full_name_normalized, levenshtein, tokenization=char, list_strategy=None)     4.357054\n",
      "    StringComparator(full_name_normalized, jaccard, tokenization=word, list_strategy=None)     2.490422\n",
      "                                            DateComparator(birth_year, list_strategy=None)     1.440008\n",
      "  Intercept: -4.9249\n",
      "  ✓ LogisticRegression trained for LS\n",
      "\n",
      "✓ LogisticRegression matchers trained for all edges\n"
     ]
    }
   ],
   "source": [
    "# Train LogisticRegression-based matchers for each edge\n",
    "logreg_classifiers = {}\n",
    "logreg_matchers = {}\n",
    "\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    print(f\"\\n=== {edge_name}: Training LogisticRegression Matcher ===\")\n",
    "    \n",
    "    left_df, right_df = source_tables[edge_name]\n",
    "    train_df = splits[edge_name]['train'][['id1', 'id2', 'label']].copy()\n",
    "    \n",
    "    # Normalize labels\n",
    "    train_df['label'] = train_df['label'].astype(str).str.strip().str.upper()\n",
    "    train_df['label_binary'] = (train_df['label'] == 'TRUE').astype(int)\n",
    "    \n",
    "    print(f\"  Training pairs: {len(train_df)}\")\n",
    "    print(f\"    True matches: {train_df['label_binary'].sum()}\")\n",
    "    print(f\"    False matches: {len(train_df) - train_df['label_binary'].sum()}\")\n",
    "    \n",
    "    # Extract shared features\n",
    "    train_features = ml_feature_extractor.create_features(\n",
    "        df_left=left_df,\n",
    "        df_right=right_df,\n",
    "        pairs=train_df[['id1', 'id2']],\n",
    "        labels=train_df['label_binary'],\n",
    "        id_column='_rid'\n",
    "    )\n",
    "    \n",
    "    feature_columns = [col for col in train_features.columns if col not in ['id1', 'id2', 'label']]\n",
    "    X_train = train_features[feature_columns]\n",
    "    y_train = train_features['label']\n",
    "    \n",
    "    print(f\"  Training features shape: {X_train.shape}\")\n",
    "    print(f\"  Feature columns: {feature_columns}\")\n",
    "    \n",
    "    # Configure Logistic Regression\n",
    "    clf = LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        class_weight='balanced',\n",
    "        solver='liblinear'\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    logreg_classifiers[edge_name] = clf\n",
    "    logreg_matchers[edge_name] = MLBasedMatcher(ml_feature_extractor)\n",
    "    \n",
    "    # Inspect coefficients for interpretability\n",
    "    coef_df = pd.DataFrame({\n",
    "        'feature': feature_columns,\n",
    "        'coefficient': clf.coef_[0]\n",
    "    }).sort_values('coefficient', key=lambda s: s.abs(), ascending=False)\n",
    "    print(\"\\n  Top coefficients (magnitude):\")\n",
    "    print(coef_df.to_string(index=False))\n",
    "    print(f\"  Intercept: {clf.intercept_[0]:.4f}\")\n",
    "    print(f\"  ✓ LogisticRegression trained for {edge_name}\")\n",
    "\n",
    "print(\"\\n✓ LogisticRegression matchers trained for all edges\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f687dec",
   "metadata": {},
   "source": [
    "### 4.3 Apply LogisticRegression Matcher to Candidate Pairs\n",
    "\n",
    "Reuse the shared matching pipeline to score candidate pairs with the LogisticRegression classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "3788e984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LR: LogisticRegression Matching ===\n",
      "  Filtering candidate pairs by season_year constraint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Starting Entity Matching\n",
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Blocking 106553 x 15215 elements\n",
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Matching 106553 x 15215 elements after 0:00:0.000; 130895 blocked pairs (reduction ratio: 0.9999192606183567)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  After season_year filter: 130,895 candidate pairs (from 5,733,797)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Entity Matching finished after 0:00:49.062; found 19673 correspondences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 19,673 matched pairs\n",
      "  Score range: [0.401, 0.962]\n",
      "  Mean score: 0.792\n",
      "\n",
      "=== LS: LogisticRegression Matching ===\n",
      "  Filtering candidate pairs by season_year constraint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Starting Entity Matching\n",
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Blocking 106553 x 6743 elements\n",
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Matching 106553 x 6743 elements after 0:00:0.000; 9526 blocked pairs (reduction ratio: 0.9999867415811222)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  After season_year filter: 9,526 candidate pairs (from 215,708)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Entity Matching finished after 0:00:3.768; found 6792 correspondences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 6,792 matched pairs\n",
      "  Score range: [0.408, 0.967]\n",
      "  Mean score: 0.910\n",
      "\n",
      "✓ LogisticRegression matching complete for all edges\n"
     ]
    }
   ],
   "source": [
    "# Apply LogisticRegression-based matcher to candidate pairs\n",
    "logreg_matching_results = {}\n",
    "\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    print(f\"\\n=== {edge_name}: LogisticRegression Matching ===\")\n",
    "    \n",
    "    left_df, right_df = source_tables[edge_name]\n",
    "    cand_df = candidates[edge_name].copy()\n",
    "    matcher = logreg_matchers[edge_name]\n",
    "    clf = logreg_classifiers[edge_name]\n",
    "    \n",
    "    # Apply season_year hard constraint\n",
    "    print(\"  Filtering candidate pairs by season_year constraint...\")\n",
    "    cand_with_seasons = cand_df.merge(\n",
    "        left_df[['_rid', 'season_year']],\n",
    "        left_on='id1',\n",
    "        right_on='_rid',\n",
    "        how='left'\n",
    "    ).merge(\n",
    "        right_df[['_rid', 'season_year']],\n",
    "        left_on='id2',\n",
    "        right_on='_rid',\n",
    "        how='left',\n",
    "        suffixes=('', '_right')\n",
    "    )\n",
    "    \n",
    "    cand_df_filtered = cand_with_seasons[\n",
    "        (cand_with_seasons['season_year'].notna()) &\n",
    "        (cand_with_seasons['season_year_right'].notna()) &\n",
    "        (cand_with_seasons['season_year'] == cand_with_seasons['season_year_right'])\n",
    "    ][['id1', 'id2']].copy()\n",
    "    \n",
    "    print(f\"  After season_year filter: {len(cand_df_filtered):,} candidate pairs (from {len(cand_df):,})\")\n",
    "    \n",
    "    correspondences = matcher.match(\n",
    "        df_left=left_df,\n",
    "        df_right=right_df,\n",
    "        candidates=cand_df_filtered,\n",
    "        id_column='_rid',\n",
    "        use_probabilities=True,\n",
    "        trained_classifier=clf,\n",
    "        threshold=0.4\n",
    "    )\n",
    "    \n",
    "    logreg_matching_results[edge_name] = correspondences\n",
    "    \n",
    "    print(f\"  Generated {len(correspondences):,} matched pairs\")\n",
    "    if 'score' in correspondences.columns:\n",
    "        print(f\"  Score range: [{correspondences['score'].min():.3f}, {correspondences['score'].max():.3f}]\")\n",
    "        print(f\"  Mean score: {correspondences['score'].mean():.3f}\")\n",
    "\n",
    "print(\"\\n✓ LogisticRegression matching complete for all edges\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551b1c1f",
   "metadata": {},
   "source": [
    "### 4.4 Evaluate LogisticRegression Matching\n",
    "\n",
    "Assess LogisticRegression performance on the validation set using the PyDI evaluator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "88945a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LR: LogisticRegression Evaluation (Validation Set) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Confusion Matrix:\n",
      "[INFO ] root -   True Positives:  65\n",
      "[INFO ] root -   True Negatives:  27\n",
      "[INFO ] root -   False Positives: 0\n",
      "[INFO ] root -   False Negatives: 7\n",
      "[INFO ] root - Performance Metrics:\n",
      "[INFO ] root -   Accuracy:  0.929\n",
      "[INFO ] root -   Precision: 1.000\n",
      "[INFO ] root -   Recall:    0.903\n",
      "[INFO ] root -   F1-Score:  0.949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Precision: 1.000\n",
      "  Recall:    0.903\n",
      "  F1-Score:  0.949\n",
      "  TP: 65\n",
      "  FP: 0\n",
      "  FN: 7\n",
      "\n",
      "=== LS: LogisticRegression Evaluation (Validation Set) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Confusion Matrix:\n",
      "[INFO ] root -   True Positives:  54\n",
      "[INFO ] root -   True Negatives:  37\n",
      "[INFO ] root -   False Positives: 4\n",
      "[INFO ] root -   False Negatives: 1\n",
      "[INFO ] root - Performance Metrics:\n",
      "[INFO ] root -   Accuracy:  0.948\n",
      "[INFO ] root -   Precision: 0.931\n",
      "[INFO ] root -   Recall:    0.982\n",
      "[INFO ] root -   F1-Score:  0.956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Precision: 0.931\n",
      "  Recall:    0.982\n",
      "  F1-Score:  0.956\n",
      "  TP: 54\n",
      "  FP: 4\n",
      "  FN: 1\n",
      "\n",
      "✓ LogisticRegression evaluation complete\n"
     ]
    }
   ],
   "source": [
    "# Evaluate LogisticRegression matching on validation set\n",
    "logreg_matching_metrics_val = {}\n",
    "\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    print(f\"\\n=== {edge_name}: LogisticRegression Evaluation (Validation Set) ===\")\n",
    "    \n",
    "    correspondences = logreg_matching_results[edge_name]\n",
    "    val_df = splits[edge_name]['val'][['id1', 'id2', 'label']].copy()\n",
    "    \n",
    "    correspondences_for_eval = correspondences.copy()\n",
    "    if 'score' not in correspondences_for_eval.columns and 'sim' in correspondences_for_eval.columns:\n",
    "        correspondences_for_eval = correspondences_for_eval.rename(columns={'sim': 'score'})\n",
    "    \n",
    "    try:\n",
    "        eval_results = EntityMatchingEvaluator.evaluate_matching(\n",
    "            correspondences=correspondences_for_eval,\n",
    "            test_pairs=val_df,\n",
    "            out_dir=OUTPUT_DIR / 'matching-evaluation-logreg',\n",
    "            matcher_instance=logreg_matchers[edge_name]\n",
    "        )\n",
    "        logreg_matching_metrics_val[edge_name] = eval_results\n",
    "        \n",
    "        print(f\"\\n  Precision: {eval_results.get('precision', 0.0):.3f}\")\n",
    "        print(f\"  Recall:    {eval_results.get('recall', 0.0):.3f}\")\n",
    "        print(f\"  F1-Score:  {eval_results.get('f1', 0.0):.3f}\")\n",
    "        print(f\"  TP: {eval_results.get('true_positives', 0)}\")\n",
    "        print(f\"  FP: {eval_results.get('false_positives', 0)}\")\n",
    "        print(f\"  FN: {eval_results.get('false_negatives', 0)}\")\n",
    "    except Exception as exc:\n",
    "        print(f\"  PyDI evaluator failed: {exc}\")\n",
    "        true_matches = val_df[val_df['label'].astype(str).str.strip().str.upper() == 'TRUE']\n",
    "        true_set = set(zip(true_matches['id1'], true_matches['id2']))\n",
    "        pred_set = set(zip(correspondences['id1'], correspondences['id2']))\n",
    "        \n",
    "        tp = len(true_set & pred_set)\n",
    "        fp = len(pred_set - true_set)\n",
    "        fn = len(true_set - pred_set)\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        \n",
    "        logreg_matching_metrics_val[edge_name] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'true_positives': tp,\n",
    "            'false_positives': fp,\n",
    "            'false_negatives': fn\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n  Precision: {precision:.3f}\")\n",
    "        print(f\"  Recall:    {recall:.3f}\")\n",
    "        print(f\"  F1-Score:  {f1:.3f}\")\n",
    "        print(f\"  TP: {tp}\")\n",
    "        print(f\"  FP: {fp}\")\n",
    "        print(f\"  FN: {fn}\")\n",
    "\n",
    "print(\"\\n✓ LogisticRegression evaluation complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd8dcdc",
   "metadata": {},
   "source": [
    "### 4.4.1 LogisticRegression Error Cases Analysis\n",
    "\n",
    "Investigate False Positives/Negatives for the LogisticRegression matcher using the same diagnostic template.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "39ee7fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LR: LogisticRegression Error Cases Analysis\n",
      "================================================================================\n",
      "\n",
      "[FALSE NEGATIVES] (7 cases):\n",
      "\n",
      "  5948382019|2019|L <-> 5948382019|2019|R\n",
      "    Left:  'philip gosselin' | Season: 2019 | Birth: 1988\n",
      "    Right: 'phil gosselin' | Season: 2019 | Birth: 1989\n",
      "    In candidates: True\n",
      "\n",
      "  5948382015|2015|L <-> 5948382015|2015|R\n",
      "    Left:  'philip gosselin' | Season: 2015 | Birth: 1988\n",
      "    Right: 'phil gosselin' | Season: 2015 | Birth: 1989\n",
      "    In candidates: True\n",
      "\n",
      "  6638452022|2022|L <-> 6638452022|2022|R\n",
      "    Left:  'alfonso rivas' | Season: 2022 | Birth: 1996\n",
      "    Right: 'alfonso rivas iii' | Season: 2022 | Birth: 1997\n",
      "    In candidates: True\n",
      "\n",
      "  5961292022|2022|L <-> 5961292022|2022|R\n",
      "    Left:  'dan vogelbach' | Season: 2022 | Birth: 1992\n",
      "    Right: 'daniel vogelbach' | Season: 2022 | Birth: 1993\n",
      "    In candidates: False\n",
      "\n",
      "  5471702019|2019|L <-> 5471702019|2019|R\n",
      "    Left:  'nick delmonico' | Season: 2019 | Birth: 1992\n",
      "    Right: 'nicky delmonico' | Season: 2019 | Birth: 1993\n",
      "    In candidates: True\n",
      "\n",
      "  5948382021|2021|L <-> 5948382021|2021|R\n",
      "    Left:  'philip gosselin' | Season: 2021 | Birth: 1988\n",
      "    Right: 'phil gosselin' | Season: 2021 | Birth: 1989\n",
      "    In candidates: True\n",
      "\n",
      "  6074732017|2017|L <-> 6074732017|2017|R\n",
      "    Left:  'chase bradford' | Season: 2017 | Birth: 1989\n",
      "    Right: 'chasen bradford' | Season: 2017 | Birth: 1990\n",
      "    In candidates: True\n",
      "\n",
      "[FALSE POSITIVES] (0 cases):\n",
      "  No false positives found!\n",
      "\n",
      "================================================================================\n",
      "LS: LogisticRegression Error Cases Analysis\n",
      "================================================================================\n",
      "\n",
      "[FALSE NEGATIVES] (1 cases):\n",
      "\n",
      "  6404472019|2019|L <-> 6404472019|2019|S\n",
      "    Left:  'phil ervin' | Season: 2019 | Birth: 1992\n",
      "    Right: 'phillip ervin' | Season: 2019 | Birth: 1993\n",
      "    In candidates: True\n",
      "\n",
      "[FALSE POSITIVES] (4 cases):\n",
      "\n",
      "  6703512022|2022|L <-> 6689422022|2022|S (score: 0.457)\n",
      "    Left:  'jose rojas' | Season: 2022 | Birth: 1993\n",
      "    Right: 'josh rojas' | Season: 2022 | Birth: 1994\n",
      "\n",
      "  6072592024|2024|L <-> 6053612024|2024|S (score: 0.737)\n",
      "    Left:  'nick martinez' | Season: 2024 | Birth: 1990\n",
      "    Right: 'nick martini' | Season: 2024 | Birth: 1990\n",
      "\n",
      "  5190082017|2017|L <-> 6080612017|2017|S (score: 0.443)\n",
      "    Left:  't j mcfarland' | Season: 2017 | Birth: 1989\n",
      "    Right: 't j rivera' | Season: 2017 | Birth: 1989\n",
      "\n",
      "  5167142015|2015|L <-> 6458482015|2015|S (score: 0.746)\n",
      "    Left:  'dario alvarez' | Season: 2015 | Birth: 1989\n",
      "    Right: 'dariel alvarez' | Season: 2015 | Birth: 1989\n",
      "\n",
      "✓ LogisticRegression error analysis complete\n"
     ]
    }
   ],
   "source": [
    "# Analyze error cases for LogisticRegression\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{edge_name}: LogisticRegression Error Cases Analysis\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    left_df, right_df = source_tables[edge_name]\n",
    "    val_df = splits[edge_name]['val'][['id1', 'id2', 'label']].copy()\n",
    "    correspondences = logreg_matching_results[edge_name]\n",
    "    \n",
    "    true_matches = val_df[val_df['label'].astype(str).str.strip().str.upper() == 'TRUE']\n",
    "    false_matches = val_df[val_df['label'].astype(str).str.strip().str.upper() == 'FALSE']\n",
    "    true_set = set(zip(true_matches['id1'], true_matches['id2']))\n",
    "    false_set = set(zip(false_matches['id1'], false_matches['id2']))\n",
    "    pred_set = set(zip(correspondences['id1'], correspondences['id2']))\n",
    "    val_set = set(zip(val_df['id1'], val_df['id2']))\n",
    "    \n",
    "    # False negatives\n",
    "    fn_pairs = true_set - pred_set\n",
    "    print(f\"\\n[FALSE NEGATIVES] ({len(fn_pairs)} cases):\")\n",
    "    if fn_pairs:\n",
    "        for id1, id2 in fn_pairs:\n",
    "            left_rec = left_df[left_df['_rid'] == id1].iloc[0] if len(left_df[left_df['_rid'] == id1]) > 0 else None\n",
    "            right_rec = right_df[right_df['_rid'] == id2].iloc[0] if len(right_df[right_df['_rid'] == id2]) > 0 else None\n",
    "            if left_rec is not None and right_rec is not None:\n",
    "                print(f\"\\n  {id1} <-> {id2}\")\n",
    "                print(f\"    Left:  '{left_rec.get('full_name', 'N/A')}' | Season: {left_rec.get('season_year', 'N/A')} | Birth: {left_rec.get('birth_year', 'N/A')}\")\n",
    "                print(f\"    Right: '{right_rec.get('full_name', 'N/A')}' | Season: {right_rec.get('season_year', 'N/A')} | Birth: {right_rec.get('birth_year', 'N/A')}\")\n",
    "                in_candidates = len(candidates[edge_name][\n",
    "                    (candidates[edge_name]['id1'] == id1) &\n",
    "                    (candidates[edge_name]['id2'] == id2)\n",
    "                ]) > 0\n",
    "                print(f\"    In candidates: {in_candidates}\")\n",
    "                score_row = correspondences[(correspondences['id1'] == id1) & (correspondences['id2'] == id2)]\n",
    "                if len(score_row) > 0 and 'score' in score_row.columns:\n",
    "                    print(f\"    LogReg Score: {score_row['score'].iloc[0]:.3f}\")\n",
    "    else:\n",
    "        print(\"  No false negatives found!\")\n",
    "    \n",
    "    # False positives\n",
    "    fp_pairs = (pred_set & val_set) & false_set\n",
    "    print(f\"\\n[FALSE POSITIVES] ({len(fp_pairs)} cases):\")\n",
    "    if fp_pairs:\n",
    "        for id1, id2 in fp_pairs:\n",
    "            left_rec = left_df[left_df['_rid'] == id1].iloc[0] if len(left_df[left_df['_rid'] == id1]) > 0 else None\n",
    "            right_rec = right_df[right_df['_rid'] == id2].iloc[0] if len(right_df[right_df['_rid'] == id2]) > 0 else None\n",
    "            score_row = correspondences[(correspondences['id1'] == id1) & (correspondences['id2'] == id2)]\n",
    "            score = score_row['score'].iloc[0] if len(score_row) > 0 and 'score' in score_row.columns else None\n",
    "            if left_rec is not None and right_rec is not None:\n",
    "                score_str = f\"{score:.3f}\" if score is not None else \"N/A\"\n",
    "                print(f\"\\n  {id1} <-> {id2} (score: {score_str})\")\n",
    "                print(f\"    Left:  '{left_rec.get('full_name', 'N/A')}' | Season: {left_rec.get('season_year', 'N/A')} | Birth: {left_rec.get('birth_year', 'N/A')}\")\n",
    "                print(f\"    Right: '{right_rec.get('full_name', 'N/A')}' | Season: {right_rec.get('season_year', 'N/A')} | Birth: {right_rec.get('birth_year', 'N/A')}\")\n",
    "    else:\n",
    "        print(\"  No false positives found!\")\n",
    "\n",
    "print(\"\\n✓ LogisticRegression error analysis complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f3c7c0",
   "metadata": {},
   "source": [
    "### 4.5 Extract Features and Train RandomForestClassifier\n",
    "\n",
    "Extract features from training pairs and train a RandomForestClassifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "f6440750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LR: Training ML-Based Matcher ===\n",
      "  Training pairs: 302\n",
      "    True matches: 217\n",
      "    False matches: 85\n",
      "  Extracting features from training pairs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Label distribution: 217 positive, 85 negative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Extracted features: 302 pairs\n",
      "  Feature columns: 3\n",
      "  Training features shape: (302, 3)\n",
      "  Training labels distribution: {1: 217, 0: 85}\n",
      "  Training RandomForestClassifier...\n",
      "\n",
      "  Top 5 Feature Importances:\n",
      "    StringComparator(full_name_normalized, levenshtein, tokenization=char, list_strategy=None): 0.5042\n",
      "    StringComparator(full_name_normalized, jaccard, tokenization=word, list_strategy=None): 0.4058\n",
      "    DateComparator(birth_year, list_strategy=None): 0.0900\n",
      "  ✓ Classifier trained for LR\n",
      "\n",
      "=== LS: Training ML-Based Matcher ===\n",
      "  Training pairs: 299\n",
      "    True matches: 164\n",
      "    False matches: 135\n",
      "  Extracting features from training pairs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Label distribution: 164 positive, 135 negative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Extracted features: 299 pairs\n",
      "  Feature columns: 3\n",
      "  Training features shape: (299, 3)\n",
      "  Training labels distribution: {1: 164, 0: 135}\n",
      "  Training RandomForestClassifier...\n",
      "\n",
      "  Top 5 Feature Importances:\n",
      "    StringComparator(full_name_normalized, levenshtein, tokenization=char, list_strategy=None): 0.6069\n",
      "    StringComparator(full_name_normalized, jaccard, tokenization=word, list_strategy=None): 0.3208\n",
      "    DateComparator(birth_year, list_strategy=None): 0.0723\n",
      "  ✓ Classifier trained for LS\n",
      "\n",
      "✓ ML-based matchers trained for all edges\n"
     ]
    }
   ],
   "source": [
    "# Train ML-based matchers for each edge\n",
    "ml_classifiers = {}\n",
    "ml_matchers = {}\n",
    "\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    print(f\"\\n=== {edge_name}: Training ML-Based Matcher ===\")\n",
    "    \n",
    "    left_df, right_df = source_tables[edge_name]\n",
    "    train_df = splits[edge_name]['train'][['id1', 'id2', 'label']].copy()\n",
    "    \n",
    "    # Prepare labels: convert 'TRUE'/'FALSE' to 1/0\n",
    "    train_df['label'] = train_df['label'].astype(str).str.strip().str.upper()\n",
    "    train_df['label_binary'] = (train_df['label'] == 'TRUE').astype(int)\n",
    "    \n",
    "    print(f\"  Training pairs: {len(train_df)}\")\n",
    "    print(f\"    True matches: {train_df['label_binary'].sum()}\")\n",
    "    print(f\"    False matches: {len(train_df) - train_df['label_binary'].sum()}\")\n",
    "    \n",
    "    # Extract features for training pairs\n",
    "    print(f\"  Extracting features from training pairs...\")\n",
    "    train_features = ml_feature_extractor.create_features(\n",
    "        df_left=left_df,\n",
    "        df_right=right_df,\n",
    "        pairs=train_df[['id1', 'id2']],\n",
    "        labels=train_df['label_binary'],\n",
    "        id_column='_rid'\n",
    "    )\n",
    "    \n",
    "    print(f\"  Extracted features: {len(train_features)} pairs\")\n",
    "    print(f\"  Feature columns: {len([col for col in train_features.columns if col not in ['id1', 'id2', 'label']])}\")\n",
    "    \n",
    "    # Prepare training data\n",
    "    feature_columns = [col for col in train_features.columns if col not in ['id1', 'id2', 'label']]\n",
    "    X_train = train_features[feature_columns]\n",
    "    y_train = train_features['label']\n",
    "    \n",
    "    print(f\"  Training features shape: {X_train.shape}\")\n",
    "    print(f\"  Training labels distribution: {y_train.value_counts().to_dict()}\")\n",
    "    \n",
    "    # Train classifier\n",
    "    print(f\"  Training RandomForestClassifier...\")\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Store classifier and create matcher\n",
    "    ml_classifiers[edge_name] = clf\n",
    "    ml_matchers[edge_name] = MLBasedMatcher(ml_feature_extractor)\n",
    "    \n",
    "    # Log feature importance if available\n",
    "    if hasattr(clf, 'feature_importances_'):\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': feature_columns,\n",
    "            'importance': clf.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        print(f\"\\n  Top 5 Feature Importances:\")\n",
    "        for idx, row in feature_importance.head(5).iterrows():\n",
    "            print(f\"    {row['feature']}: {row['importance']:.4f}\")\n",
    "    \n",
    "    print(f\"  ✓ Classifier trained for {edge_name}\")\n",
    "\n",
    "print(\"\\n✓ ML-based matchers trained for all edges\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66828372",
   "metadata": {},
   "source": [
    "### 4.6 Apply RandomForest Matcher to Candidate Pairs\n",
    "\n",
    "Apply the trained RandomForest-based matcher to candidate pairs from the blocking phase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "38bef601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LR: ML-Based Matching ===\n",
      "  Filtering candidate pairs by season_year constraint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Starting Entity Matching\n",
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Blocking 106553 x 15215 elements\n",
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Matching 106553 x 15215 elements after 0:00:0.000; 130895 blocked pairs (reduction ratio: 0.9999192606183567)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  After season_year filter: 130,895 candidate pairs (from 5,733,797)\n",
      "  Applying ML-based matcher...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Entity Matching finished after 0:00:51.452; found 15389 correspondences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 15,389 matched pairs\n",
      "  Score range: [1.000, 1.000]\n",
      "  Mean score: 1.000\n",
      "\n",
      "=== LS: ML-Based Matching ===\n",
      "  Filtering candidate pairs by season_year constraint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Starting Entity Matching\n",
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Blocking 106553 x 6743 elements\n",
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Matching 106553 x 6743 elements after 0:00:0.000; 9526 blocked pairs (reduction ratio: 0.9999867415811222)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  After season_year filter: 9,526 candidate pairs (from 215,708)\n",
      "  Applying ML-based matcher...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Entity Matching finished after 0:00:3.497; found 6785 correspondences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 6,785 matched pairs\n",
      "  Score range: [1.000, 1.000]\n",
      "  Mean score: 1.000\n",
      "\n",
      "✓ ML-based matching complete for all edges\n"
     ]
    }
   ],
   "source": [
    "# Apply ML-based matching to candidate pairs\n",
    "ml_matching_results = {}\n",
    "\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    print(f\"\\n=== {edge_name}: ML-Based Matching ===\")\n",
    "    \n",
    "    left_df, right_df = source_tables[edge_name]\n",
    "    cand_df = candidates[edge_name].copy()\n",
    "    matcher = ml_matchers[edge_name]\n",
    "    clf = ml_classifiers[edge_name]\n",
    "    \n",
    "    # Apply season_year hard constraint: filter candidate pairs where season_year matches\n",
    "    print(f\"  Filtering candidate pairs by season_year constraint...\")\n",
    "    cand_with_seasons = cand_df.merge(\n",
    "        left_df[['_rid', 'season_year']],\n",
    "        left_on='id1',\n",
    "        right_on='_rid',\n",
    "        how='left'\n",
    "    ).merge(\n",
    "        right_df[['_rid', 'season_year']],\n",
    "        left_on='id2',\n",
    "        right_on='_rid',\n",
    "        how='left',\n",
    "        suffixes=('', '_right')\n",
    "    )\n",
    "    \n",
    "    # Filter: season_year must match exactly\n",
    "    cand_df_filtered = cand_with_seasons[\n",
    "        (cand_with_seasons['season_year'].notna()) & \n",
    "        (cand_with_seasons['season_year_right'].notna()) &\n",
    "        (cand_with_seasons['season_year'] == cand_with_seasons['season_year_right'])\n",
    "    ][['id1', 'id2']].copy()\n",
    "    \n",
    "    print(f\"  After season_year filter: {len(cand_df_filtered):,} candidate pairs (from {len(cand_df):,})\")\n",
    "    \n",
    "    # Apply ML-based matcher\n",
    "    print(f\"  Applying ML-based matcher...\")\n",
    "    correspondences = matcher.match(\n",
    "        df_left=left_df,\n",
    "        df_right=right_df,\n",
    "        candidates=cand_df_filtered,\n",
    "        id_column='_rid',\n",
    "        trained_classifier=clf\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    ml_matching_results[edge_name] = correspondences\n",
    "    \n",
    "    print(f\"  Generated {len(correspondences):,} matched pairs\")\n",
    "    if 'score' in correspondences.columns:\n",
    "        print(f\"  Score range: [{correspondences['score'].min():.3f}, {correspondences['score'].max():.3f}]\")\n",
    "        print(f\"  Mean score: {correspondences['score'].mean():.3f}\")\n",
    "\n",
    "print(\"\\n✓ ML-based matching complete for all edges\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3369139",
   "metadata": {},
   "source": [
    "### 4.7 Evaluate RandomForest Matching\n",
    "\n",
    "Evaluate the performance of the RandomForest-based matcher on the validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "095ec0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LR: ML-Based Matching Evaluation (Validation Set) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Confusion Matrix:\n",
      "[INFO ] root -   True Positives:  70\n",
      "[INFO ] root -   True Negatives:  27\n",
      "[INFO ] root -   False Positives: 0\n",
      "[INFO ] root -   False Negatives: 2\n",
      "[INFO ] root - Performance Metrics:\n",
      "[INFO ] root -   Accuracy:  0.980\n",
      "[INFO ] root -   Precision: 1.000\n",
      "[INFO ] root -   Recall:    0.972\n",
      "[INFO ] root -   F1-Score:  0.986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Precision: 1.000\n",
      "  Recall:    0.972\n",
      "  F1-Score:  0.986\n",
      "  TP: 70\n",
      "  FP: 0\n",
      "  FN: 2\n",
      "\n",
      "=== LS: ML-Based Matching Evaluation (Validation Set) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Confusion Matrix:\n",
      "[INFO ] root -   True Positives:  53\n",
      "[INFO ] root -   True Negatives:  38\n",
      "[INFO ] root -   False Positives: 3\n",
      "[INFO ] root -   False Negatives: 2\n",
      "[INFO ] root - Performance Metrics:\n",
      "[INFO ] root -   Accuracy:  0.948\n",
      "[INFO ] root -   Precision: 0.946\n",
      "[INFO ] root -   Recall:    0.964\n",
      "[INFO ] root -   F1-Score:  0.955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Precision: 0.946\n",
      "  Recall:    0.964\n",
      "  F1-Score:  0.955\n",
      "  TP: 53\n",
      "  FP: 3\n",
      "  FN: 2\n",
      "\n",
      "✓ ML-based matching evaluation complete\n"
     ]
    }
   ],
   "source": [
    "# Evaluate ML-based matching on validation set\n",
    "\n",
    "ml_matching_metrics_val = {}\n",
    "\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    print(f\"\\n=== {edge_name}: ML-Based Matching Evaluation (Validation Set) ===\")\n",
    "    \n",
    "    correspondences = ml_matching_results[edge_name]\n",
    "    val_df = splits[edge_name]['val'][['id1', 'id2', 'label']].copy()\n",
    "    \n",
    "    # Rename 'score' to match PyDI evaluator expectations (if needed)\n",
    "    correspondences_for_eval = correspondences.copy()\n",
    "    if 'score' not in correspondences_for_eval.columns and 'sim' in correspondences_for_eval.columns:\n",
    "        correspondences_for_eval = correspondences_for_eval.rename(columns={'sim': 'score'})\n",
    "    \n",
    "    # Evaluate matching\n",
    "    try:\n",
    "        eval_results = EntityMatchingEvaluator.evaluate_matching(\n",
    "            correspondences=correspondences_for_eval,\n",
    "            test_pairs=val_df,\n",
    "            out_dir=OUTPUT_DIR / 'matching-evaluation-ml',\n",
    "            matcher_instance=ml_matchers[edge_name]\n",
    "        )\n",
    "        \n",
    "        ml_matching_metrics_val[edge_name] = eval_results\n",
    "        \n",
    "        print(f\"\\n  Precision: {eval_results.get('precision', 0.0):.3f}\")\n",
    "        print(f\"  Recall:    {eval_results.get('recall', 0.0):.3f}\")\n",
    "        print(f\"  F1-Score:  {eval_results.get('f1', 0.0):.3f}\")\n",
    "        print(f\"  TP: {eval_results.get('true_positives', 0)}\")\n",
    "        print(f\"  FP: {eval_results.get('false_positives', 0)}\")\n",
    "        print(f\"  FN: {eval_results.get('false_negatives', 0)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  PyDI evaluator failed: {e}\")\n",
    "        # Manual evaluation fallback\n",
    "        true_matches = val_df[val_df['label'].astype(str).str.strip().str.upper() == 'TRUE']\n",
    "        true_set = set(zip(true_matches['id1'], true_matches['id2']))\n",
    "        pred_set = set(zip(correspondences['id1'], correspondences['id2']))\n",
    "        \n",
    "        tp = len(true_set & pred_set)\n",
    "        fp = len(pred_set - true_set)\n",
    "        fn = len(true_set - pred_set)\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        \n",
    "        print(f\"\\n  Precision: {precision:.3f}\")\n",
    "        print(f\"  Recall:    {recall:.3f}\")\n",
    "        print(f\"  F1-Score:  {f1:.3f}\")\n",
    "        print(f\"  TP: {tp}\")\n",
    "        print(f\"  FP: {fp}\")\n",
    "        print(f\"  FN: {fn}\")\n",
    "\n",
    "print(\"\\n✓ ML-based matching evaluation complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7621fcf",
   "metadata": {},
   "source": [
    "### 4.7.1 RandomForestClassifier Error Cases Analysis\n",
    "\n",
    "Analyze False Positives and False Negatives for RandomForestClassifier to identify patterns for further improvement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "947b13a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LR: RandomForestClassifier Error Cases Analysis\n",
      "================================================================================\n",
      "\n",
      "[FALSE NEGATIVES] (2 cases):\n",
      "\n",
      "  5961292022|2022|L <-> 5961292022|2022|R\n",
      "    Left:  'dan vogelbach' | Season: 2022 | Birth: 1992\n",
      "    Right: 'daniel vogelbach' | Season: 2022 | Birth: 1993\n",
      "    In candidates: False\n",
      "\n",
      "  5715102017|2017|L <-> 5715102017|2017|R\n",
      "    Left:  'matt boyd' | Season: 2017 | Birth: 1991\n",
      "    Right: 'matthew boyd' | Season: 2017 | Birth: 1991\n",
      "    In candidates: True\n",
      "\n",
      "[FALSE POSITIVES] (0 cases):\n",
      "  No false positives found!\n",
      "\n",
      "================================================================================\n",
      "LS: RandomForestClassifier Error Cases Analysis\n",
      "================================================================================\n",
      "\n",
      "[FALSE NEGATIVES] (2 cases):\n",
      "\n",
      "  4931142016|2016|L <-> 4931142016|2016|S\n",
      "    Left:  'nori aoki' | Season: 2016 | Birth: 1982\n",
      "    Right: 'norichika aoki' | Season: 2016 | Birth: 1982\n",
      "    In candidates: True\n",
      "\n",
      "  5470072016|2016|L <-> 5470072016|2016|S\n",
      "    Left:  'robert whalen' | Season: 2016 | Birth: 1994\n",
      "    Right: 'rob whalen' | Season: 2016 | Birth: 1994\n",
      "    In candidates: True\n",
      "\n",
      "[FALSE POSITIVES] (3 cases):\n",
      "\n",
      "  6703512022|2022|L <-> 6689422022|2022|S (score: 1.000)\n",
      "    Left:  'jose rojas' | Season: 2022 | Birth: 1993\n",
      "    Right: 'josh rojas' | Season: 2022 | Birth: 1994\n",
      "\n",
      "  6072592024|2024|L <-> 6053612024|2024|S (score: 1.000)\n",
      "    Left:  'nick martinez' | Season: 2024 | Birth: 1990\n",
      "    Right: 'nick martini' | Season: 2024 | Birth: 1990\n",
      "\n",
      "  5167142015|2015|L <-> 6458482015|2015|S (score: 1.000)\n",
      "    Left:  'dario alvarez' | Season: 2015 | Birth: 1989\n",
      "    Right: 'dariel alvarez' | Season: 2015 | Birth: 1989\n",
      "\n",
      "✓ RandomForestClassifier error analysis complete\n"
     ]
    }
   ],
   "source": [
    "# Analyze error cases for RandomForestClassifier (reusing code structure from 3.2)\n",
    "\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{edge_name}: RandomForestClassifier Error Cases Analysis\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    left_df, right_df = source_tables[edge_name]\n",
    "    val_df = splits[edge_name]['val'][['id1', 'id2', 'label']].copy()\n",
    "    correspondences = ml_matching_results[edge_name]\n",
    "    \n",
    "    # Get true matches and false matches in validation set\n",
    "    true_matches = val_df[val_df['label'].astype(str).str.strip().str.upper() == 'TRUE']\n",
    "    false_matches = val_df[val_df['label'].astype(str).str.strip().str.upper() == 'FALSE']\n",
    "    true_set = set(zip(true_matches['id1'], true_matches['id2']))\n",
    "    false_set = set(zip(false_matches['id1'], false_matches['id2']))\n",
    "    \n",
    "    # Get predicted matches (only those in validation set)\n",
    "    pred_set = set(zip(correspondences['id1'], correspondences['id2']))\n",
    "    val_set = set(zip(val_df['id1'], val_df['id2']))  # All pairs in validation set\n",
    "    \n",
    "    # False Negatives: True matches in validation set that were not predicted\n",
    "    fn_pairs = true_set - pred_set\n",
    "    print(f\"\\n[FALSE NEGATIVES] ({len(fn_pairs)} cases):\")\n",
    "    if len(fn_pairs) > 0:\n",
    "        for id1, id2 in fn_pairs:\n",
    "            left_rec = left_df[left_df['_rid'] == id1].iloc[0] if len(left_df[left_df['_rid'] == id1]) > 0 else None\n",
    "            right_rec = right_df[right_df['_rid'] == id2].iloc[0] if len(right_df[right_df['_rid'] == id2]) > 0 else None\n",
    "            \n",
    "            if left_rec is not None and right_rec is not None:\n",
    "                print(f\"\\n  {id1} <-> {id2}\")\n",
    "                print(f\"    Left:  '{left_rec.get('full_name', 'N/A')}' | Season: {left_rec.get('season_year', 'N/A')} | Birth: {left_rec.get('birth_year', 'N/A')}\")\n",
    "                print(f\"    Right: '{right_rec.get('full_name', 'N/A')}' | Season: {right_rec.get('season_year', 'N/A')} | Birth: {right_rec.get('birth_year', 'N/A')}\")\n",
    "                \n",
    "                # Check if in candidates\n",
    "                in_candidates = len(candidates[edge_name][\n",
    "                    (candidates[edge_name]['id1'] == id1) & \n",
    "                    (candidates[edge_name]['id2'] == id2)\n",
    "                ]) > 0\n",
    "                print(f\"    In candidates: {in_candidates}\")\n",
    "                \n",
    "                # Check if pair was scored by RF (even if below threshold)\n",
    "                score_row = correspondences[(correspondences['id1'] == id1) & (correspondences['id2'] == id2)]\n",
    "                if len(score_row) > 0:\n",
    "                    score = score_row['score'].iloc[0] if 'score' in score_row.columns else None\n",
    "                    score_str = f\"{score:.3f}\" if score is not None else \"N/A\"\n",
    "                    print(f\"    RF Score: {score_str}\")\n",
    "    else:\n",
    "        print(\"  No false negatives found!\")\n",
    "    \n",
    "    # False Positives: Predicted matches that are in validation set but labeled as FALSE\n",
    "    # Only analyze pairs that are in the validation set\n",
    "    fp_pairs = (pred_set & val_set) & false_set\n",
    "    print(f\"\\n[FALSE POSITIVES] ({len(fp_pairs)} cases):\")\n",
    "    if len(fp_pairs) > 0:\n",
    "        for id1, id2 in fp_pairs:\n",
    "            left_rec = left_df[left_df['_rid'] == id1].iloc[0] if len(left_df[left_df['_rid'] == id1]) > 0 else None\n",
    "            right_rec = right_df[right_df['_rid'] == id2].iloc[0] if len(right_df[right_df['_rid'] == id2]) > 0 else None\n",
    "            score_row = correspondences[(correspondences['id1'] == id1) & (correspondences['id2'] == id2)]\n",
    "            score = score_row['score'].iloc[0] if len(score_row) > 0 and 'score' in score_row.columns else None\n",
    "            \n",
    "            if left_rec is not None and right_rec is not None:\n",
    "                score_str = f\"{score:.3f}\" if score is not None else \"N/A\"\n",
    "                print(f\"\\n  {id1} <-> {id2} (score: {score_str})\")\n",
    "                print(f\"    Left:  '{left_rec.get('full_name', 'N/A')}' | Season: {left_rec.get('season_year', 'N/A')} | Birth: {left_rec.get('birth_year', 'N/A')}\")\n",
    "                print(f\"    Right: '{right_rec.get('full_name', 'N/A')}' | Season: {right_rec.get('season_year', 'N/A')} | Birth: {right_rec.get('birth_year', 'N/A')}\")\n",
    "    else:\n",
    "        print(\"  No false positives found!\")\n",
    "\n",
    "print(\"\\n✓ RandomForestClassifier error analysis complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16864fb",
   "metadata": {},
   "source": [
    "### 4.5 Comparison: All Matching Methods\n",
    "\n",
    "Compare the performance of Rule-Based, Optimized, RandomForest, and GradientBoosting matching approaches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "dcf4cb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Matching Performance Comparison: All Methods\n",
      "================================================================================\n",
      "\n",
      "LR Edge:\n",
      "--------------------------------------------------------------------------------\n",
      "  Metric       | Rule  | Optimized | RF      | GB      | Best\n",
      "  -------------|-------|-----------|---------|---------|------\n",
      "  Precision    | 1.000 |     1.000 |   1.000 |   1.000 | Rule\n",
      "  Recall       | 0.972 |     0.962 |   0.972 |   0.962 | Rule\n",
      "  F1-Score     | 0.986 |     0.980 |   0.986 |   0.980 | Rule\n",
      "\n",
      "LS Edge:\n",
      "--------------------------------------------------------------------------------\n",
      "  Metric       | Rule  | Optimized | RF      | GB      | Best\n",
      "  -------------|-------|-----------|---------|---------|------\n",
      "  Precision    | 0.963 |     0.965 |   0.946 |   0.950 | Opt\n",
      "  Recall       | 0.945 |     0.948 |   0.964 |   0.983 | GB\n",
      "  F1-Score     | 0.954 |     0.957 |   0.955 |   0.966 | GB\n",
      "\n",
      "================================================================================\n",
      "Summary:\n",
      "================================================================================\n",
      "  Methods: Rule-Based, Optimized, RandomForest (RF), GradientBoosting (GB)\n",
      "  Results saved to: /Users/zhangzihan/Desktop/WBI_project/Schema_Mapped_Datasets/data/output/matching/matching-comparison-all-methods.csv\n"
     ]
    }
   ],
   "source": [
    "# Compare all matching methods: Rule-Based, Optimized, RandomForest, and GradientBoosting\n",
    "# Note: Run GradientBoosting cells (4.6) before this comparison for complete results\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Matching Performance Comparison: All Methods\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check if GradientBoosting results are available\n",
    "gb_available = 'gb_matching_metrics_val' in globals()\n",
    "if not gb_available:\n",
    "    print(\"Note: GradientBoosting results not yet computed. Run cells 4.6 first for complete comparison.\\n\")\n",
    "\n",
    "comparison_data_all = []\n",
    "\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    print(f\"\\n{edge_name} Edge:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Get metrics for all methods (with safe defaults if not yet computed)\n",
    "    orig_metrics = matching_metrics_val.get(edge_name, {})\n",
    "    opt_metrics = optimized_matching_metrics_val.get(edge_name, {})\n",
    "    ml_metrics = ml_matching_metrics_val.get(edge_name, {})\n",
    "    gb_metrics = gb_matching_metrics_val.get(edge_name, {}) if 'gb_matching_metrics_val' in globals() else {}\n",
    "    \n",
    "    orig_p, orig_r, orig_f1 = orig_metrics.get('precision', 0.0), orig_metrics.get('recall', 0.0), orig_metrics.get('f1', 0.0)\n",
    "    opt_p, opt_r, opt_f1 = opt_metrics.get('precision', 0.0), opt_metrics.get('recall', 0.0), opt_metrics.get('f1', 0.0)\n",
    "    ml_p, ml_r, ml_f1 = ml_metrics.get('precision', 0.0), ml_metrics.get('recall', 0.0), ml_metrics.get('f1', 0.0)\n",
    "    gb_p, gb_r, gb_f1 = gb_metrics.get('precision', 0.0), gb_metrics.get('recall', 0.0), gb_metrics.get('f1', 0.0)\n",
    "    \n",
    "    # Find best method for each metric\n",
    "    def find_best(values, names):\n",
    "        max_idx = values.index(max(values))\n",
    "        return names[max_idx]\n",
    "    \n",
    "    print(f\"  Metric       | Rule  | Optimized | RF      | GB      | Best\")\n",
    "    print(f\"  -------------|-------|-----------|---------|---------|------\")\n",
    "    \n",
    "    precisions = [orig_p, opt_p, ml_p, gb_p]\n",
    "    recalls = [orig_r, opt_r, ml_r, gb_r]\n",
    "    f1s = [orig_f1, opt_f1, ml_f1, gb_f1]\n",
    "    names = ['Rule', 'Opt', 'RF', 'GB']\n",
    "    \n",
    "    print(f\"  Precision    | {orig_p:5.3f} | {opt_p:9.3f} | {ml_p:7.3f} | {gb_p:7.3f} | {find_best(precisions, names)}\")\n",
    "    print(f\"  Recall       | {orig_r:5.3f} | {opt_r:9.3f} | {ml_r:7.3f} | {gb_r:7.3f} | {find_best(recalls, names)}\")\n",
    "    print(f\"  F1-Score     | {orig_f1:5.3f} | {opt_f1:9.3f} | {ml_f1:7.3f} | {gb_f1:7.3f} | {find_best(f1s, names)}\")\n",
    "    \n",
    "    comparison_data_all.append({\n",
    "        'edge': edge_name,\n",
    "        'rule_based_precision': orig_p, 'rule_based_recall': orig_r, 'rule_based_f1': orig_f1,\n",
    "        'optimized_precision': opt_p, 'optimized_recall': opt_r, 'optimized_f1': opt_f1,\n",
    "        'rf_precision': ml_p, 'rf_recall': ml_r, 'rf_f1': ml_f1,\n",
    "        'gb_precision': gb_p, 'gb_recall': gb_r, 'gb_f1': gb_f1,\n",
    "    })\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Summary:\")\n",
    "print(\"=\"*80)\n",
    "print(\"  Methods: Rule-Based, Optimized, RandomForest (RF), GradientBoosting (GB)\")\n",
    "\n",
    "comparison_df_all = pd.DataFrame(comparison_data_all)\n",
    "comparison_df_all.to_csv(OUTPUT_DIR / 'matching-comparison-all-methods.csv', index=False)\n",
    "print(f\"  Results saved to: {OUTPUT_DIR / 'matching-comparison-all-methods.csv'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b8b4e4",
   "metadata": {},
   "source": [
    "### 4.6 GradientBoostingClassifier Matching\n",
    "\n",
    "Train and apply GradientBoostingClassifier using the same feature extractor for comparison with RandomForest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "3bd5a79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LR: GradientBoostingClassifier ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Label distribution: 217 positive, 85 negative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training GradientBoostingClassifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Starting Entity Matching\n",
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Blocking 106553 x 15215 elements\n",
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Matching 106553 x 15215 elements after 0:00:0.001; 130895 blocked pairs (reduction ratio: 0.9999192606183567)\n",
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Entity Matching finished after 0:00:47.734; found 15336 correspondences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 15,336 matched pairs\n",
      "\n",
      "=== LS: GradientBoostingClassifier ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Label distribution: 164 positive, 135 negative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training GradientBoostingClassifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Starting Entity Matching\n",
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Blocking 106553 x 6743 elements\n",
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Matching 106553 x 6743 elements after 0:00:0.000; 9526 blocked pairs (reduction ratio: 0.9999867415811222)\n",
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Entity Matching finished after 0:00:3.241; found 6786 correspondences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 6,786 matched pairs\n",
      "\n",
      "✓ GradientBoostingClassifier matching complete\n"
     ]
    }
   ],
   "source": [
    "# Train and apply GradientBoostingClassifier (reusing feature extractor from 4.2)\n",
    "\n",
    "# Ensure GradientBoostingClassifier is imported\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_classifiers = {}\n",
    "gb_matchers = {}\n",
    "gb_matching_results = {}\n",
    "\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    print(f\"\\n=== {edge_name}: GradientBoostingClassifier ===\")\n",
    "    \n",
    "    left_df, right_df = source_tables[edge_name]\n",
    "    train_df = splits[edge_name]['train'][['id1', 'id2', 'label']].copy()\n",
    "    train_df['label'] = train_df['label'].astype(str).str.strip().str.upper()\n",
    "    train_df['label_binary'] = (train_df['label'] == 'TRUE').astype(int)\n",
    "    \n",
    "    # Reuse feature extraction from 4.2\n",
    "    train_features = ml_feature_extractor.create_features(\n",
    "        df_left=left_df,\n",
    "        df_right=right_df,\n",
    "        pairs=train_df[['id1', 'id2']],\n",
    "        labels=train_df['label_binary'],\n",
    "        id_column='_rid'\n",
    "    )\n",
    "    \n",
    "    feature_columns = [col for col in train_features.columns if col not in ['id1', 'id2', 'label']]\n",
    "    X_train = train_features[feature_columns]\n",
    "    y_train = train_features['label']\n",
    "    \n",
    "    # Train GradientBoostingClassifier\n",
    "    print(f\"  Training GradientBoostingClassifier...\")\n",
    "    gb_clf = GradientBoostingClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        random_state=42\n",
    "    )\n",
    "    gb_clf.fit(X_train, y_train)\n",
    "    \n",
    "    gb_classifiers[edge_name] = gb_clf\n",
    "    gb_matchers[edge_name] = MLBasedMatcher(ml_feature_extractor)\n",
    "    \n",
    "    # Apply to candidate pairs (with season_year constraint)\n",
    "    cand_df = candidates[edge_name].copy()\n",
    "    cand_with_seasons = cand_df.merge(\n",
    "        left_df[['_rid', 'season_year']],\n",
    "        left_on='id1', right_on='_rid', how='left'\n",
    "    ).merge(\n",
    "        right_df[['_rid', 'season_year']],\n",
    "        left_on='id2', right_on='_rid', how='left', suffixes=('', '_right')\n",
    "    )\n",
    "    cand_df_filtered = cand_with_seasons[\n",
    "        (cand_with_seasons['season_year'].notna()) & \n",
    "        (cand_with_seasons['season_year_right'].notna()) &\n",
    "        (cand_with_seasons['season_year'] == cand_with_seasons['season_year_right'])\n",
    "    ][['id1', 'id2']].copy()\n",
    "    \n",
    "    # Apply matcher\n",
    "    correspondences = gb_matchers[edge_name].match(\n",
    "        df_left=left_df,\n",
    "        df_right=right_df,\n",
    "        candidates=cand_df_filtered,\n",
    "        id_column='_rid',\n",
    "        trained_classifier=gb_clf\n",
    "    )\n",
    "    gb_matching_results[edge_name] = correspondences\n",
    "    \n",
    "    print(f\"  Generated {len(correspondences):,} matched pairs\")\n",
    "\n",
    "print(\"\\n✓ GradientBoostingClassifier matching complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "74463c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LR: GradientBoostingClassifier Evaluation ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Confusion Matrix:\n",
      "[INFO ] root -   True Positives:  70\n",
      "[INFO ] root -   True Negatives:  27\n",
      "[INFO ] root -   False Positives: 0\n",
      "[INFO ] root -   False Negatives: 2\n",
      "[INFO ] root - Performance Metrics:\n",
      "[INFO ] root -   Accuracy:  0.980\n",
      "[INFO ] root -   Precision: 1.000\n",
      "[INFO ] root -   Recall:    0.972\n",
      "[INFO ] root -   F1-Score:  0.986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Precision: 1.000\n",
      "  Recall:    0.972\n",
      "  F1-Score:  0.986\n",
      "\n",
      "=== LS: GradientBoostingClassifier Evaluation ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Confusion Matrix:\n",
      "[INFO ] root -   True Positives:  54\n",
      "[INFO ] root -   True Negatives:  40\n",
      "[INFO ] root -   False Positives: 1\n",
      "[INFO ] root -   False Negatives: 1\n",
      "[INFO ] root - Performance Metrics:\n",
      "[INFO ] root -   Accuracy:  0.979\n",
      "[INFO ] root -   Precision: 0.982\n",
      "[INFO ] root -   Recall:    0.982\n",
      "[INFO ] root -   F1-Score:  0.982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Precision: 0.982\n",
      "  Recall:    0.982\n",
      "  F1-Score:  0.982\n",
      "\n",
      "✓ GradientBoostingClassifier evaluation complete\n"
     ]
    }
   ],
   "source": [
    "# Evaluate GradientBoostingClassifier matching\n",
    "\n",
    "gb_matching_metrics_val = {}\n",
    "\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    print(f\"\\n=== {edge_name}: GradientBoostingClassifier Evaluation ===\")\n",
    "    \n",
    "    correspondences = gb_matching_results[edge_name]\n",
    "    val_df = splits[edge_name]['val'][['id1', 'id2', 'label']].copy()\n",
    "    \n",
    "    correspondences_for_eval = correspondences.copy()\n",
    "    if 'score' not in correspondences_for_eval.columns and 'sim' in correspondences_for_eval.columns:\n",
    "        correspondences_for_eval = correspondences_for_eval.rename(columns={'sim': 'score'})\n",
    "    \n",
    "    try:\n",
    "        eval_results = EntityMatchingEvaluator.evaluate_matching(\n",
    "            correspondences=correspondences_for_eval,\n",
    "            test_pairs=val_df,\n",
    "            out_dir=OUTPUT_DIR / 'matching-evaluation-gb',\n",
    "            matcher_instance=gb_matchers[edge_name]\n",
    "        )\n",
    "        gb_matching_metrics_val[edge_name] = eval_results\n",
    "        print(f\"  Precision: {eval_results.get('precision', 0.0):.3f}\")\n",
    "        print(f\"  Recall:    {eval_results.get('recall', 0.0):.3f}\")\n",
    "        print(f\"  F1-Score:  {eval_results.get('f1', 0.0):.3f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Evaluation failed: {e}\")\n",
    "\n",
    "print(\"\\n✓ GradientBoostingClassifier evaluation complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313ded61",
   "metadata": {},
   "source": [
    "### 4.7 GradientBoostingClassifier Error Cases Analysis\n",
    "\n",
    "Analyze False Positives and False Negatives for GradientBoostingClassifier to identify patterns for further improvement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "e7647603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LR: GradientBoostingClassifier Error Cases Analysis\n",
      "================================================================================\n",
      "\n",
      "[FALSE NEGATIVES] (2 cases):\n",
      "\n",
      "  5961292022|2022|L <-> 5961292022|2022|R\n",
      "    Left:  'dan vogelbach' | Season: 2022 | Birth: 1992\n",
      "    Right: 'daniel vogelbach' | Season: 2022 | Birth: 1993\n",
      "    In candidates: False\n",
      "\n",
      "  5715102017|2017|L <-> 5715102017|2017|R\n",
      "    Left:  'matt boyd' | Season: 2017 | Birth: 1991\n",
      "    Right: 'matthew boyd' | Season: 2017 | Birth: 1991\n",
      "    In candidates: True\n",
      "\n",
      "[FALSE POSITIVES] (0 cases):\n",
      "  No false positives found!\n",
      "\n",
      "================================================================================\n",
      "LS: GradientBoostingClassifier Error Cases Analysis\n",
      "================================================================================\n",
      "\n",
      "[FALSE NEGATIVES] (1 cases):\n",
      "\n",
      "  5470072016|2016|L <-> 5470072016|2016|S\n",
      "    Left:  'robert whalen' | Season: 2016 | Birth: 1994\n",
      "    Right: 'rob whalen' | Season: 2016 | Birth: 1994\n",
      "    In candidates: True\n",
      "\n",
      "[FALSE POSITIVES] (1 cases):\n",
      "\n",
      "  5167142015|2015|L <-> 6458482015|2015|S (score: 1.000)\n",
      "    Left:  'dario alvarez' | Season: 2015 | Birth: 1989\n",
      "    Right: 'dariel alvarez' | Season: 2015 | Birth: 1989\n",
      "\n",
      "✓ GradientBoostingClassifier error analysis complete\n"
     ]
    }
   ],
   "source": [
    "# Analyze error cases for GradientBoostingClassifier (reusing code structure from 3.2)\n",
    "\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{edge_name}: GradientBoostingClassifier Error Cases Analysis\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    left_df, right_df = source_tables[edge_name]\n",
    "    val_df = splits[edge_name]['val'][['id1', 'id2', 'label']].copy()\n",
    "    correspondences = gb_matching_results[edge_name]\n",
    "    \n",
    "    # Get true matches and false matches in validation set\n",
    "    true_matches = val_df[val_df['label'].astype(str).str.strip().str.upper() == 'TRUE']\n",
    "    false_matches = val_df[val_df['label'].astype(str).str.strip().str.upper() == 'FALSE']\n",
    "    true_set = set(zip(true_matches['id1'], true_matches['id2']))\n",
    "    false_set = set(zip(false_matches['id1'], false_matches['id2']))\n",
    "    \n",
    "    # Get predicted matches (only those in validation set)\n",
    "    pred_set = set(zip(correspondences['id1'], correspondences['id2']))\n",
    "    val_set = set(zip(val_df['id1'], val_df['id2']))  # All pairs in validation set\n",
    "    \n",
    "    # False Negatives: True matches in validation set that were not predicted\n",
    "    fn_pairs = true_set - pred_set\n",
    "    print(f\"\\n[FALSE NEGATIVES] ({len(fn_pairs)} cases):\")\n",
    "    if len(fn_pairs) > 0:\n",
    "        for id1, id2 in fn_pairs:\n",
    "            left_rec = left_df[left_df['_rid'] == id1].iloc[0] if len(left_df[left_df['_rid'] == id1]) > 0 else None\n",
    "            right_rec = right_df[right_df['_rid'] == id2].iloc[0] if len(right_df[right_df['_rid'] == id2]) > 0 else None\n",
    "            \n",
    "            if left_rec is not None and right_rec is not None:\n",
    "                print(f\"\\n  {id1} <-> {id2}\")\n",
    "                print(f\"    Left:  '{left_rec.get('full_name', 'N/A')}' | Season: {left_rec.get('season_year', 'N/A')} | Birth: {left_rec.get('birth_year', 'N/A')}\")\n",
    "                print(f\"    Right: '{right_rec.get('full_name', 'N/A')}' | Season: {right_rec.get('season_year', 'N/A')} | Birth: {right_rec.get('birth_year', 'N/A')}\")\n",
    "                \n",
    "                # Check if in candidates\n",
    "                in_candidates = len(candidates[edge_name][\n",
    "                    (candidates[edge_name]['id1'] == id1) & \n",
    "                    (candidates[edge_name]['id2'] == id2)\n",
    "                ]) > 0\n",
    "                print(f\"    In candidates: {in_candidates}\")\n",
    "                \n",
    "                # Check if pair was scored by GB (even if below threshold)\n",
    "                score_row = correspondences[(correspondences['id1'] == id1) & (correspondences['id2'] == id2)]\n",
    "                if len(score_row) > 0:\n",
    "                    score = score_row['score'].iloc[0] if 'score' in score_row.columns else None\n",
    "                    score_str = f\"{score:.3f}\" if score is not None else \"N/A\"\n",
    "                    print(f\"    GB Score: {score_str}\")\n",
    "    else:\n",
    "        print(\"  No false negatives found!\")\n",
    "    \n",
    "    # False Positives: Predicted matches that are in validation set but labeled as FALSE\n",
    "    # Only analyze pairs that are in the validation set\n",
    "    fp_pairs = (pred_set & val_set) & false_set\n",
    "    print(f\"\\n[FALSE POSITIVES] ({len(fp_pairs)} cases):\")\n",
    "    if len(fp_pairs) > 0:\n",
    "        for id1, id2 in fp_pairs:\n",
    "            left_rec = left_df[left_df['_rid'] == id1].iloc[0] if len(left_df[left_df['_rid'] == id1]) > 0 else None\n",
    "            right_rec = right_df[right_df['_rid'] == id2].iloc[0] if len(right_df[right_df['_rid'] == id2]) > 0 else None\n",
    "            score_row = correspondences[(correspondences['id1'] == id1) & (correspondences['id2'] == id2)]\n",
    "            score = score_row['score'].iloc[0] if len(score_row) > 0 and 'score' in score_row.columns else None\n",
    "            \n",
    "            if left_rec is not None and right_rec is not None:\n",
    "                score_str = f\"{score:.3f}\" if score is not None else \"N/A\"\n",
    "                print(f\"\\n  {id1} <-> {id2} (score: {score_str})\")\n",
    "                print(f\"    Left:  '{left_rec.get('full_name', 'N/A')}' | Season: {left_rec.get('season_year', 'N/A')} | Birth: {left_rec.get('birth_year', 'N/A')}\")\n",
    "                print(f\"    Right: '{right_rec.get('full_name', 'N/A')}' | Season: {right_rec.get('season_year', 'N/A')} | Birth: {right_rec.get('birth_year', 'N/A')}\")\n",
    "    else:\n",
    "        print(\"  No false positives found!\")\n",
    "\n",
    "print(\"\\n✓ GradientBoostingClassifier error analysis complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e168fb",
   "metadata": {},
   "source": [
    "### 4.8 Error Analysis Summary and Improvement Plan\n",
    "\n",
    "Based on the error analysis, we identify the following patterns and propose targeted improvements:\n",
    "\n",
    "#### **LR Edge Error Patterns:**\n",
    "\n",
    "1. **Blocking Issues (2 cases):**\n",
    "   - `dan vogelbach` vs `daniel vogelbach` (2 instances) - Not in candidate pairs\n",
    "   - **Root Cause:** The Enhanced TokenBlocker is missing these name variant pairs during blocking phase\n",
    "   - **Impact:** These are true matches that never reach the matching stage\n",
    "\n",
    "2. **Matching Issues (1 case):**\n",
    "   - `jonathon niese` vs `jon niese` - In candidates but missed by GradientBoostingClassifier\n",
    "   - **Root Cause:** ML model not recognizing common name variants despite high name similarity\n",
    "   - **Impact:** True match filtered out by classifier\n",
    "\n",
    "3. **False Positives:** 0 cases (Perfect Precision)\n",
    "\n",
    "#### **LS Edge Error Patterns:**\n",
    "\n",
    "1. **Matching Issues - False Negatives (1 case):**\n",
    "   - `dan robertson` vs `daniel robertson` - In candidates but missed by classifier\n",
    "   - **Root Cause:** Same as LR - name variant not recognized by ML model\n",
    "\n",
    "2. **Matching Issues - False Positives (3 cases, all score=1.000):**\n",
    "   - `josh rojas` vs `jose rojas` (birth years: 1994 vs 1993)\n",
    "   - `kevan smith` vs `kevin smith` (birth years: 1988 vs 1997)\n",
    "   - `matt duffy` vs `matt duffy` (birth years: 1989 vs 1991)\n",
    "   - **Root Cause:** Model overconfident on name similarity, insufficiently penalizing birth year differences\n",
    "   - **Impact:** High-confidence false matches reduce precision\n",
    "\n",
    "#### **Proposed Improvement Strategies:**\n",
    "\n",
    "**Strategy 1: Enhance Feature Engineering for ML Models**\n",
    "- **Add Birth Year Difference Feature:** Calculate `abs(birth_year_left - birth_year_right)` as an explicit feature\n",
    "- **Add Name Variant Indicator:** Create binary feature indicating if names are known variants (using `NAME_VARIANTS` dictionary)\n",
    "- **Action:** Extend `ml_comparators` to include `DateComparator` for birth_year, or add custom feature extraction\n",
    "\n",
    "**Strategy 2: Improve Blocking for Name Variants (LR Edge)**\n",
    "- **Enhance TokenBlocker:** Investigate why `dan`/`daniel` variants are missed in blocking phase\n",
    "- **Action:** Review `normalize_name_for_blocking` function or add name variant expansion to blocking keys\n",
    "\n",
    "**Strategy 3: Hyperparameter Tuning**\n",
    "- **Adjust GradientBoostingClassifier:** Fine-tune parameters to better balance name similarity vs. birth year constraints\n",
    "- **Action:** Grid search or Bayesian optimization for optimal parameters\n",
    "\n",
    "**Strategy 4: Ensemble Methods**\n",
    "- **Combine Models:** Use voting or stacking to combine RandomForest and GradientBoosting predictions\n",
    "- **Action:** Implement ensemble matcher that leverages strengths of both models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40af5094",
   "metadata": {},
   "source": [
    "### 4.11 Train XGBoostClassifier Matcher\n",
    "\n",
    "Train a gradient-boosted tree model (XGBoost) on the same feature set to capture non-linear interactions beyond RandomForest/GradientBoosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "9612ab22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LR: Training XGBoostClassifier ===\n",
      "  Training pairs: 302\n",
      "  True matches: 217\n",
      "  False matches: 85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Label distribution: 217 positive, 85 negative\n",
      "[INFO ] root - Label distribution: 164 positive, 135 negative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top feature importances:\n",
      " StringComparator(full_name_normalized, jaccard, tokenization=word, list_strategy=None): 0.4100\n",
      " StringComparator(full_name_normalized, levenshtein, tokenization=char, list_strategy=None): 0.3639\n",
      " DateComparator(birth_year, list_strategy=None): 0.2260\n",
      " XGBoostClassifier trained for LR\n",
      "=== LS: Training XGBoostClassifier ===\n",
      "  Training pairs: 299\n",
      "  True matches: 164\n",
      "  False matches: 135\n",
      "Top feature importances:\n",
      " StringComparator(full_name_normalized, jaccard, tokenization=word, list_strategy=None): 0.6271\n",
      " StringComparator(full_name_normalized, levenshtein, tokenization=char, list_strategy=None): 0.3093\n",
      " DateComparator(birth_year, list_strategy=None): 0.0636\n",
      " XGBoostClassifier trained for LS\n",
      "XGBoostClassifier training complete for all edges\n"
     ]
    }
   ],
   "source": [
    "# Train XGBoost-based matchers for each edge\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_classifiers = {}\n",
    "xgb_matchers = {}\n",
    "\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    print(f\"=== {edge_name}: Training XGBoostClassifier ===\")\n",
    "    \n",
    "    left_df, right_df = source_tables[edge_name]\n",
    "    train_df = splits[edge_name]['train'][['id1', 'id2', 'label']].copy()\n",
    "    train_df['label'] = train_df['label'].astype(str).str.strip().str.upper()\n",
    "    train_df['label_binary'] = (train_df['label'] == 'TRUE').astype(int)\n",
    "    \n",
    "    print(f\"  Training pairs: {len(train_df)}\")\n",
    "    print(f\"  True matches: {train_df['label_binary'].sum()}\")\n",
    "    print(f\"  False matches: {len(train_df) - train_df['label_binary'].sum()}\")\n",
    "    \n",
    "    train_features = ml_feature_extractor.create_features(\n",
    "        df_left=left_df,\n",
    "        df_right=right_df,\n",
    "        pairs=train_df[['id1', 'id2']],\n",
    "        labels=train_df['label_binary'],\n",
    "        id_column='_rid'\n",
    "    )\n",
    "    \n",
    "    feature_columns = [col for col in train_features.columns if col not in ['id1', 'id2', 'label']]\n",
    "    X_train = train_features[feature_columns]\n",
    "    y_train = train_features['label']\n",
    "    \n",
    "    clf = XGBClassifier(\n",
    "        n_estimators=400,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        reg_lambda=1.0,\n",
    "        reg_alpha=0.0,\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='logloss',\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    xgb_classifiers[edge_name] = clf\n",
    "    xgb_matchers[edge_name] = MLBasedMatcher(ml_feature_extractor)\n",
    "    \n",
    "    importance = sorted(\n",
    "        zip(feature_columns, clf.feature_importances_),\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )\n",
    "    print(\"Top feature importances:\")\n",
    "    for feat, val in importance:\n",
    "        print(f\" {feat}: {val:.4f}\")\n",
    "    print(f\" XGBoostClassifier trained for {edge_name}\")\n",
    "\n",
    "print(\"XGBoostClassifier training complete for all edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41848ead",
   "metadata": {},
   "source": [
    "### 4.12 Apply XGBoostClassifier Matcher to Candidate Pairs\n",
    "\n",
    "Filter by the same season_year constraint, then score candidate pairs with the trained XGBoost models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "a5b0aed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LR: XGBoostClassifier Matching ===\n",
      "  Filtering candidate pairs by season_year constraint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Starting Entity Matching\n",
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Blocking 106553 x 15215 elements\n",
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Matching 106553 x 15215 elements after 0:00:0.000; 130895 blocked pairs (reduction ratio: 0.9999192606183567)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  After season_year filter: 130,895 candidate pairs (from 5,733,797)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Entity Matching finished after 0:00:49.858; found 15334 correspondences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 15,334 matched pairs\n",
      "  Score range: [1.000, 1.000]\n",
      "  Mean score: 1.000\n",
      "=== LS: XGBoostClassifier Matching ===\n",
      "  Filtering candidate pairs by season_year constraint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Starting Entity Matching\n",
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Blocking 106553 x 6743 elements\n",
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Matching 106553 x 6743 elements after 0:00:0.000; 9526 blocked pairs (reduction ratio: 0.9999867415811222)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  After season_year filter: 9,526 candidate pairs (from 215,708)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.ml_based.MLBasedMatcher - Entity Matching finished after 0:00:3.287; found 6789 correspondences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 6,789 matched pairs\n",
      "  Score range: [1.000, 1.000]\n",
      "  Mean score: 1.000\n",
      "XGBoostClassifier matching complete for all edges\n"
     ]
    }
   ],
   "source": [
    "# Apply XGBoostClassifier-based matcher to candidate pairs\n",
    "xgb_matching_results = {}\n",
    "\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    print(f\"=== {edge_name}: XGBoostClassifier Matching ===\")\n",
    "    \n",
    "    left_df, right_df = source_tables[edge_name]\n",
    "    cand_df = candidates[edge_name].copy()\n",
    "    matcher = xgb_matchers[edge_name]\n",
    "    clf = xgb_classifiers[edge_name]\n",
    "    \n",
    "    print(\"  Filtering candidate pairs by season_year constraint...\")\n",
    "    cand_with_seasons = cand_df.merge(\n",
    "        left_df[['_rid', 'season_year']],\n",
    "        left_on='id1',\n",
    "        right_on='_rid',\n",
    "        how='left'\n",
    "    ).merge(\n",
    "        right_df[['_rid', 'season_year']],\n",
    "        left_on='id2',\n",
    "        right_on='_rid',\n",
    "        how='left',\n",
    "        suffixes=('', '_right')\n",
    "    )\n",
    "    \n",
    "    cand_df_filtered = cand_with_seasons[\n",
    "        (cand_with_seasons['season_year'].notna()) &\n",
    "        (cand_with_seasons['season_year_right'].notna()) &\n",
    "        (cand_with_seasons['season_year'] == cand_with_seasons['season_year_right'])\n",
    "    ][['id1', 'id2']].copy()\n",
    "    \n",
    "    print(f\"  After season_year filter: {len(cand_df_filtered):,} candidate pairs (from {len(cand_df):,})\")\n",
    "    correspondences = matcher.match(\n",
    "        df_left=left_df,\n",
    "        df_right=right_df,\n",
    "        candidates=cand_df_filtered,\n",
    "        id_column='_rid',\n",
    "        trained_classifier=clf\n",
    "    )\n",
    "    \n",
    "    xgb_matching_results[edge_name] = correspondences\n",
    "    \n",
    "    print(f\"  Generated {len(correspondences):,} matched pairs\")\n",
    "    if 'score' in correspondences.columns:\n",
    "        print(f\"  Score range: [{correspondences['score'].min():.3f}, {correspondences['score'].max():.3f}]\")\n",
    "        print(f\"  Mean score: {correspondences['score'].mean():.3f}\")\n",
    "\n",
    "print(\"XGBoostClassifier matching complete for all edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bf51f3",
   "metadata": {},
   "source": [
    "### 4.13 Evaluate XGBoostClassifier Matching\n",
    "\n",
    "Assess validation performance with the PyDI evaluator, falling back to manual metrics if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "52e51e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: XGBoostClassifier Evaluation (Validation Set) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Confusion Matrix:\n",
      "[INFO ] root -   True Positives:  69\n",
      "[INFO ] root -   True Negatives:  27\n",
      "[INFO ] root -   False Positives: 0\n",
      "[INFO ] root -   False Negatives: 3\n",
      "[INFO ] root - Performance Metrics:\n",
      "[INFO ] root -   Accuracy:  0.970\n",
      "[INFO ] root -   Precision: 1.000\n",
      "[INFO ] root -   Recall:    0.958\n",
      "[INFO ] root -   F1-Score:  0.979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Precision: 1.000\n",
      "  Recall:    0.958\n",
      "  F1-Score:  0.979\n",
      "  TP: 69\n",
      "  FP: 0\n",
      "  FN: 3\n",
      "LS: XGBoostClassifier Evaluation (Validation Set) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] root - Confusion Matrix:\n",
      "[INFO ] root -   True Positives:  54\n",
      "[INFO ] root -   True Negatives:  39\n",
      "[INFO ] root -   False Positives: 2\n",
      "[INFO ] root -   False Negatives: 1\n",
      "[INFO ] root - Performance Metrics:\n",
      "[INFO ] root -   Accuracy:  0.969\n",
      "[INFO ] root -   Precision: 0.964\n",
      "[INFO ] root -   Recall:    0.982\n",
      "[INFO ] root -   F1-Score:  0.973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Precision: 0.964\n",
      "  Recall:    0.982\n",
      "  F1-Score:  0.973\n",
      "  TP: 54\n",
      "  FP: 2\n",
      "  FN: 1\n",
      "XGBoostClassifier evaluation complete\n"
     ]
    }
   ],
   "source": [
    "# Evaluate XGBoostClassifier matching on validation set\n",
    "xgb_matching_metrics_val = {}\n",
    "\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    print(f\"{edge_name}: XGBoostClassifier Evaluation (Validation Set) ===\")\n",
    "    \n",
    "    correspondences = xgb_matching_results[edge_name]\n",
    "    val_df = splits[edge_name]['val'][['id1', 'id2', 'label']].copy()\n",
    "    \n",
    "    correspondences_for_eval = correspondences.copy()\n",
    "    if 'score' not in correspondences_for_eval.columns and 'sim' in correspondences_for_eval.columns:\n",
    "        correspondences_for_eval = correspondences_for_eval.rename(columns={'sim': 'score'})\n",
    "    \n",
    "    try:\n",
    "        eval_results = EntityMatchingEvaluator.evaluate_matching(\n",
    "            correspondences=correspondences_for_eval,\n",
    "            test_pairs=val_df,\n",
    "            out_dir=OUTPUT_DIR / 'matching-evaluation-xgb',\n",
    "            matcher_instance=xgb_matchers[edge_name]\n",
    "        )\n",
    "        xgb_matching_metrics_val[edge_name] = eval_results\n",
    "        \n",
    "        print(f\"  Precision: {eval_results.get('precision', 0.0):.3f}\")\n",
    "        print(f\"  Recall:    {eval_results.get('recall', 0.0):.3f}\")\n",
    "        print(f\"  F1-Score:  {eval_results.get('f1', 0.0):.3f}\")\n",
    "        print(f\"  TP: {eval_results.get('true_positives', 0)}\")\n",
    "        print(f\"  FP: {eval_results.get('false_positives', 0)}\")\n",
    "        print(f\"  FN: {eval_results.get('false_negatives', 0)}\")\n",
    "    except Exception as exc:\n",
    "        print(f\"  PyDI evaluator failed: {exc}\")\n",
    "        true_matches = val_df[val_df['label'].astype(str).str.strip().str.upper() == 'TRUE']\n",
    "        true_set = set(zip(true_matches['id1'], true_matches['id2']))\n",
    "        pred_set = set(zip(correspondences['id1'], correspondences['id2']))\n",
    "        tp = len(true_set & pred_set)\n",
    "        fp = len(pred_set - true_set)\n",
    "        fn = len(true_set - pred_set)\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        xgb_matching_metrics_val[edge_name] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'true_positives': tp,\n",
    "            'false_positives': fp,\n",
    "            'false_negatives': fn\n",
    "        }\n",
    "        print(f\"  Precision: {precision:.3f}\")\n",
    "        print(f\"  Recall:    {recall:.3f}\")\n",
    "        print(f\"  F1-Score:  {f1:.3f}\")\n",
    "        print(f\"  TP: {tp}\")\n",
    "        print(f\"  FP: {fp}\")\n",
    "        print(f\"  FN: {fn}\")\n",
    "\n",
    "print(\"XGBoostClassifier evaluation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ed0c92",
   "metadata": {},
   "source": [
    "### 4.13.1 XGBoostClassifier Error Cases Analysis\n",
    "\n",
    "Reuse the earlier diagnostic template to inspect false negatives / positives for the XGBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "e538d03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LR: XGBoostClassifier Error Cases Analysis\n",
      "================================================================================\n",
      "[FALSE NEGATIVES] (3 cases):\n",
      "5961292022|2022|L <-> 5961292022|2022|R\n",
      "Left:  'dan vogelbach' | Season: 2022 | Birth: 1992\n",
      "Right: 'daniel vogelbach' | Season: 2022 | Birth: 1993\n",
      "In candidates: False\n",
      "5715102017|2017|L <-> 5715102017|2017|R\n",
      "Left:  'matt boyd' | Season: 2017 | Birth: 1991\n",
      "Right: 'matthew boyd' | Season: 2017 | Birth: 1991\n",
      "In candidates: True\n",
      "6638452022|2022|L <-> 6638452022|2022|R\n",
      "Left:  'alfonso rivas' | Season: 2022 | Birth: 1996\n",
      "Right: 'alfonso rivas iii' | Season: 2022 | Birth: 1997\n",
      "In candidates: True\n",
      "[FALSE POSITIVES] (0 cases):\n",
      "  No false positives found!\n",
      "================================================================================\n",
      "LS: XGBoostClassifier Error Cases Analysis\n",
      "================================================================================\n",
      "[FALSE NEGATIVES] (1 cases):\n",
      "4931142016|2016|L <-> 4931142016|2016|S\n",
      "Left:  'nori aoki' | Season: 2016 | Birth: 1982\n",
      "Right: 'norichika aoki' | Season: 2016 | Birth: 1982\n",
      "In candidates: True\n",
      "[FALSE POSITIVES] (2 cases):\n",
      " 6703512022|2022|L <-> 6689422022|2022|S (score: 1.000)\n",
      " Left:  'jose rojas' | Season: 2022 | Birth: 1993\n",
      " Right: 'josh rojas' | Season: 2022 | Birth: 1994\n",
      " 4444362015|2015|L <-> 5439012015|2015|S (score: 1.000)\n",
      " Left:  'ryan webb' | Season: 2015 | Birth: 1986\n",
      " Right: 'ryan weber' | Season: 2015 | Birth: 1991\n",
      "XGBoostClassifier error analysis complete\n"
     ]
    }
   ],
   "source": [
    "# Analyze error cases for XGBoostClassifier\n",
    "for edge_name in ['LR', 'LS']:\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"{edge_name}: XGBoostClassifier Error Cases Analysis\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    left_df, right_df = source_tables[edge_name]\n",
    "    val_df = splits[edge_name]['val'][['id1', 'id2', 'label']].copy()\n",
    "    correspondences = xgb_matching_results[edge_name]\n",
    "    \n",
    "    true_matches = val_df[val_df['label'].astype(str).str.strip().str.upper() == 'TRUE']\n",
    "    false_matches = val_df[val_df['label'].astype(str).str.strip().str.upper() == 'FALSE']\n",
    "    true_set = set(zip(true_matches['id1'], true_matches['id2']))\n",
    "    false_set = set(zip(false_matches['id1'], false_matches['id2']))\n",
    "    pred_set = set(zip(correspondences['id1'], correspondences['id2']))\n",
    "    val_set = set(zip(val_df['id1'], val_df['id2']))\n",
    "    \n",
    "    fn_pairs = true_set - pred_set\n",
    "    print(f\"[FALSE NEGATIVES] ({len(fn_pairs)} cases):\")\n",
    "    if fn_pairs:\n",
    "        for id1, id2 in fn_pairs:\n",
    "            left_rec = left_df[left_df['_rid'] == id1].iloc[0] if len(left_df[left_df['_rid'] == id1]) > 0 else None\n",
    "            right_rec = right_df[right_df['_rid'] == id2].iloc[0] if len(right_df[right_df['_rid'] == id2]) > 0 else None\n",
    "            if left_rec is not None and right_rec is not None:\n",
    "                print(f\"{id1} <-> {id2}\")\n",
    "                print(f\"Left:  '{left_rec.get('full_name', 'N/A')}' | Season: {left_rec.get('season_year', 'N/A')} | Birth: {left_rec.get('birth_year', 'N/A')}\")\n",
    "                print(f\"Right: '{right_rec.get('full_name', 'N/A')}' | Season: {right_rec.get('season_year', 'N/A')} | Birth: {right_rec.get('birth_year', 'N/A')}\")\n",
    "                in_candidates = len(candidates[edge_name][(candidates[edge_name]['id1'] == id1) & (candidates[edge_name]['id2'] == id2)]) > 0\n",
    "                print(f\"In candidates: {in_candidates}\")\n",
    "                score_row = correspondences[(correspondences['id1'] == id1) & (correspondences['id2'] == id2)]\n",
    "                if len(score_row) > 0 and 'score' in score_row.columns:\n",
    "                    print(f\"    XGBoost Score: {score_row['score'].iloc[0]:.3f}\")\n",
    "    else:\n",
    "        print(\"  No false negatives found!\")\n",
    "    \n",
    "    fp_pairs = (pred_set & val_set) & false_set\n",
    "    print(f\"[FALSE POSITIVES] ({len(fp_pairs)} cases):\")\n",
    "    if fp_pairs:\n",
    "        for id1, id2 in fp_pairs:\n",
    "            left_rec = left_df[left_df['_rid'] == id1].iloc[0] if len(left_df[left_df['_rid'] == id1]) > 0 else None\n",
    "            right_rec = right_df[right_df['_rid'] == id2].iloc[0] if len(right_df[right_df['_rid'] == id2]) > 0 else None\n",
    "            score_row = correspondences[(correspondences['id1'] == id1) & (correspondences['id2'] == id2)]\n",
    "            score = score_row['score'].iloc[0] if len(score_row) > 0 and 'score' in score_row.columns else None\n",
    "            if left_rec is not None and right_rec is not None:\n",
    "                score_str = f\"{score:.3f}\" if score is not None else 'N/A'\n",
    "                print(f\" {id1} <-> {id2} (score: {score_str})\")\n",
    "                print(f\" Left:  '{left_rec.get('full_name', 'N/A')}' | Season: {left_rec.get('season_year', 'N/A')} | Birth: {left_rec.get('birth_year', 'N/A')}\")\n",
    "                print(f\" Right: '{right_rec.get('full_name', 'N/A')}' | Season: {right_rec.get('season_year', 'N/A')} | Birth: {right_rec.get('birth_year', 'N/A')}\")\n",
    "    else:\n",
    "        print(\"  No false positives found!\")\n",
    "\n",
    "print(\"XGBoostClassifier error analysis complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
