{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89f5c27a",
   "metadata": {},
   "source": [
    "# Identity Resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761e68cf",
   "metadata": {},
   "source": [
    "## Rule-Based Entity Resolution (Three XML Sources)\n",
    "\n",
    "This notebook implements a reproducible, rule-based entity resolution pipeline for three mapped XML datasets: `Lahman_Mapped.xml`, `Reference_Mapped.xml`, and `Savant_Mapped.xml`.\n",
    "\n",
    "- **Purpose**: Match the same real-world player-season records across sources, generate high-quality ground truth (GT), and prepare evaluation-ready artifacts.\n",
    "- **Channel design**:\n",
    "  - **Main channel (non-ID)**: Blocking on `season_year + name_prefix` (using normalized names), matching with string comparators. Used for training/validation/test.\n",
    "  - **Audit channel (ID)**: `(player_id, season_year)` only for conservative prelabeling; not used for training/evaluation to avoid leakage.\n",
    "\n",
    "### Workflow\n",
    "\n",
    "**0) Setup**\n",
    "- Unified imports (all modules in Section 0.4)\n",
    "- Logging configuration and directory setup\n",
    "- Shared utility functions: `add_name_prefix`, `extract_core_ids`, `merge_candidate_columns`, `create_nonid_keys`\n",
    "\n",
    "**1) Data loading and quality checks**\n",
    "- Load three XMLs using PyDI `load_xml`\n",
    "- Apply name normalization using `name_utils.normalize_name_for_blocking` (handles encoding issues, accents, suffixes)\n",
    "- Profile missingness, uniqueness, text normalization cardinality, and season-year ranges\n",
    "- Analyze duplicates (pair-keys and exact-row) and deduplicate Lahman; persist cleaned XML\n",
    "\n",
    "**2) Candidate generation (blocking)**\n",
    "- **Non-ID channel**: PyDI `StandardBlocker` on `season_year + normalized_name_prefix` for LR and LS edges\n",
    "- Uses shared utility function `create_nonid_keys` to ensure consistent blocking key generation\n",
    "- Blocking keys built from normalized names to handle encoding variations\n",
    "\n",
    "**3) Similarity scoring**\n",
    "- PyDI `RuleBasedMatcher` with name comparators (Levenshtein 0.7, Jaccard 0.3) on normalized names\n",
    "- Uses pre-generated candidate pairs (from Section 2) to avoid duplicate blocking\n",
    "- Function `score_edge_with_pydi_nonid` accepts candidate DataFrame directly\n",
    "- Sort candidates by similarity score for stratified sampling\n",
    "\n",
    "**4) Stratified sampling for GT**\n",
    "- Score-threshold buckets: low < 0.60, mid ∈ [0.60, 0.99), high ≥ 0.99\n",
    "- Corner rules: boundary windows and hard pos/neg heuristics; backfill to 500 rows per edge when possible\n",
    "- Export annotation lists with id1/id2, names, years, birth years, empty `label`, and `source_channel='non_id'`\n",
    "- Includes manual error cases, accent/suffix variants, and birth year conflicts\n",
    "\n",
    "**5) Conservative prelabeling (id1/id2-based)**\n",
    "- Define core identifier by stripping trailing side tag (`|L/|R/|S`) from `id1`/`id2`, so `player_id|season_year` is the core ID\n",
    "- Uses shared utility function `extract_core_ids`\n",
    "- **TRUE** when core IDs of `id1` and `id2` are equal\n",
    "- **FALSE** when core IDs of `id1` and `id2` are different\n",
    "- All sampled pairs receive either TRUE or FALSE; no labels left empty. Write `samples_{edge}_v1_prelabel_pid.csv`\n",
    "\n",
    "**6) Ground truth assessment**\n",
    "- Evaluate labeled GT using `assess_edge` function (returns anomaly cases dictionary)\n",
    "- Check basic counts, label distribution, similarity-by-label stats, core-ID consistency\n",
    "- Identify potential inconsistencies: high-similarity FALSE, low-similarity TRUE\n",
    "- Detailed inspection of anomaly cases (high-similarity FALSE and low-similarity TRUE)\n",
    "\n",
    "**7) Split ground truth into train/validation/test sets**\n",
    "- Split by entity groups (player_id-based) to avoid data leakage\n",
    "- All samples from the same `player_id` stay in the same split\n",
    "- Use `_load_gt` function for unified GT loading\n",
    "- Save splits to `gt/splits/` directory for use in subsequent notebooks\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Name normalization**: Centralized normalization via `name_utils.normalize_name_for_blocking` handles UTF-8 hex escapes, Unicode accents, suffixes (Jr/Sr/II/III), and punctuation\n",
    "- **Shared utilities**: Reusable functions reduce code duplication and ensure consistency\n",
    "- **No data leakage**: `player_id` excluded from training features and main blocking; used only for prelabeling and entity-based splitting\n",
    "- **Unified imports**: All module imports consolidated in Section 0.4 for maintainability\n",
    "\n",
    "### Outputs\n",
    "\n",
    "- Cleaned Lahman XML (`data/output/clean/Lahman_Mapped_dedup.xml`)\n",
    "- Blocked candidate pairs (`cand_lr_nonid`, `cand_ls_nonid`)\n",
    "- Scored candidate pairs with similarity scores\n",
    "- Sampled annotation CSVs (`samples_{edge}_v1.csv`)\n",
    "- Prelabeled CSVs (`samples_{edge}_v1_prelabel_pid.csv`)\n",
    "- Ground truth splits (`gt/splits/gt_{edge}_{split}.csv`)\n",
    "- Logs and debug artifacts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad2f9e3",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45a5876",
   "metadata": {},
   "source": [
    "### 0.0 Import all required modules\n",
    "\n",
    "Unified imports for the entire notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b60522db",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU uma-pydi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3a442dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All required modules imported\n"
     ]
    }
   ],
   "source": [
    "# Unified imports for the entire notebook\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Optional, Sequence\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# PyDI imports\n",
    "from PyDI.entitymatching import RuleBasedMatcher\n",
    "from PyDI.entitymatching.blocking import StandardBlocker\n",
    "from PyDI.entitymatching.comparators import StringComparator\n",
    "from PyDI.io import load_xml\n",
    "\n",
    "# Local utility imports\n",
    "from name_utils import normalize_name_for_blocking\n",
    "\n",
    "print(\"✓ All required modules imported\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e58738",
   "metadata": {},
   "source": [
    "### 0.1 DEBUG logging setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2412aea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DEBUG] root - Debug logging enabled\n"
     ]
    }
   ],
   "source": [
    "# DEBUG logging setup and file paths for three XML sources\n",
    "# Enable DEBUG-level logging and write to both console and file\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='[%(levelname)-5s] %(name)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('logs/pydi.log'),\n",
    "        logging.StreamHandler()\n",
    "    ],\n",
    "    force=True\n",
    ")\n",
    "logging.getLogger().debug('Debug logging enabled')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e41dff9",
   "metadata": {},
   "source": [
    "### 0.2 Base directory setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eceead2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR: /Users/zhangzihan/Desktop/WBI_project/Schema_Mapped_Datasets\n",
      "Output directory: /Users/zhangzihan/Desktop/WBI_project/Schema_Mapped_Datasets/data/output\n",
      "Lahman XML: /Users/zhangzihan/Desktop/WBI_project/Schema_Mapped_Datasets/Lahman_Mapped.xml\n",
      "Reference XML: /Users/zhangzihan/Desktop/WBI_project/Schema_Mapped_Datasets/Reference_Mapped.xml\n",
      "Savant XML: /Users/zhangzihan/Desktop/WBI_project/Schema_Mapped_Datasets/Savant_Mapped.xml\n"
     ]
    }
   ],
   "source": [
    "# Base directory of this project (absolute)\n",
    "BASE_DIR = Path('/Users/zhangzihan/Desktop/WBI_project/Schema_Mapped_Datasets')\n",
    "\n",
    "# Output directory for intermediate results/debug\n",
    "OUTPUT_DIR = BASE_DIR / 'data' / 'output'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Absolute paths to the three mapped XML files\n",
    "LAHMAN_FILE    = BASE_DIR / 'Lahman_Mapped.xml'\n",
    "REFERENCE_FILE = BASE_DIR / 'Reference_Mapped.xml'\n",
    "SAVANT_FILE    = BASE_DIR / 'Savant_Mapped.xml'\n",
    "\n",
    "print('BASE_DIR:', BASE_DIR)\n",
    "print('Output directory:', OUTPUT_DIR)\n",
    "print('Lahman XML:', LAHMAN_FILE.resolve())\n",
    "print('Reference XML:', REFERENCE_FILE.resolve())\n",
    "print('Savant XML:', SAVANT_FILE.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acdb2ad",
   "metadata": {},
   "source": [
    "### 0.3 Common utility functions\n",
    "\n",
    "Shared utility functions used throughout the notebook for name processing, ID extraction, and column merging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c2d93ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Common utility functions defined: add_name_prefix, extract_core_ids, merge_candidate_columns, create_nonid_keys\n"
     ]
    }
   ],
   "source": [
    "def add_name_prefix(df: pd.DataFrame, name_col: str = 'full_name_normalized',\n",
    "                    out: str = 'name_prefix', n: int = 3, inplace: bool = False) -> pd.DataFrame | None:\n",
    "    \"\"\"Build a short name prefix on normalized names for blocking keys.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to process\n",
    "        name_col: Column name containing the name (default: 'full_name_normalized')\n",
    "        out: Output column name (default: 'name_prefix')\n",
    "        n: Number of characters per token (default: 3)\n",
    "        inplace: If True, modify df in place and return None; otherwise return modified copy\n",
    "    \n",
    "    Returns:\n",
    "        Modified DataFrame if inplace=False, None if inplace=True\n",
    "    \"\"\"\n",
    "    result_df = df if inplace else df.copy()\n",
    "    \n",
    "    # Fallback: if normalized column is missing, fall back to raw full_name\n",
    "    if name_col not in result_df.columns and 'full_name' in result_df.columns:\n",
    "        name_col = 'full_name'\n",
    "    \n",
    "    s = result_df[name_col].astype(str).str.lower().str.strip()\n",
    "    s = s.str.replace(r'[^\\w\\s]', ' ', regex=True).str.replace(r'\\s+', ' ', regex=True)\n",
    "    result_df[out] = s.str.split().map(\n",
    "        lambda toks: ''.join(tok[:n] for tok in toks[:2]) if isinstance(toks, list) else ''\n",
    "    )\n",
    "    \n",
    "    return None if inplace else result_df\n",
    "\n",
    "\n",
    "def extract_core_ids(df: pd.DataFrame) -> tuple[pd.Series, pd.Series]:\n",
    "    \"\"\"Extract core IDs by stripping trailing side tag (|L/|R/|S) from id1 and id2.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with 'id1' and 'id2' columns\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (core1, core2) Series\n",
    "    \"\"\"\n",
    "    core1 = df['id1'].astype(str).str.replace(r'\\|[LRS]$', '', regex=True)\n",
    "    core2 = df['id2'].astype(str).str.replace(r'\\|[LRS]$', '', regex=True)\n",
    "    return core1, core2\n",
    "\n",
    "\n",
    "def merge_candidate_columns(correspondences: pd.DataFrame, \n",
    "                            df_left: pd.DataFrame, \n",
    "                            df_right: pd.DataFrame,\n",
    "                            left_tag: str, \n",
    "                            right_tag: str) -> pd.DataFrame:\n",
    "    \"\"\"Merge candidate correspondences with left and right DataFrame columns.\n",
    "    \n",
    "    Args:\n",
    "        correspondences: DataFrame with 'id1' and 'id2' columns (matching _rid values)\n",
    "        df_left: Left DataFrame\n",
    "        df_right: Right DataFrame\n",
    "        left_tag: Tag for left side (e.g., 'L')\n",
    "        right_tag: Tag for right side (e.g., 'R')\n",
    "    \n",
    "    Returns:\n",
    "        Merged DataFrame with suffixed columns\n",
    "    \"\"\"\n",
    "    l_keep = ['_rid', 'player_id', 'season_year', 'full_name', 'birth_year']\n",
    "    r_keep = ['_rid', 'player_id', 'season_year', 'full_name', 'birth_year']\n",
    "    l_use = [c for c in l_keep if c in df_left.columns]\n",
    "    r_use = [c for c in r_keep if c in df_right.columns]\n",
    "    \n",
    "    c = (correspondences\n",
    "         .merge(df_left[l_use], left_on='id1', right_on='_rid', how='left')\n",
    "         .merge(df_right[r_use], left_on='id2', right_on='_rid', how='left', \n",
    "                suffixes=(f'_{left_tag}', f'_{right_tag}')))\n",
    "    \n",
    "    return c\n",
    "\n",
    "\n",
    "def create_nonid_keys(df: pd.DataFrame, tag: str, inplace: bool = False) -> pd.DataFrame | None:\n",
    "    \"\"\"Create _key_nonid and _rid columns for non-ID blocking.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to process\n",
    "        tag: Side tag (e.g., 'L', 'R', 'S')\n",
    "        inplace: If True, modify df in place and return None; otherwise return modified copy\n",
    "    \n",
    "    Returns:\n",
    "        Modified DataFrame if inplace=False, None if inplace=True\n",
    "    \n",
    "    Creates:\n",
    "        - _key_nonid: season_year|name_prefix (for blocking)\n",
    "        - _rid: player_id|season_year|tag (for row identification)\n",
    "    \"\"\"\n",
    "    result_df = df if inplace else df.copy()\n",
    "    \n",
    "    # Ensure season_year is numeric\n",
    "    result_df['season_year'] = pd.to_numeric(result_df['season_year'], errors='coerce')\n",
    "    \n",
    "    # Ensure name_prefix exists (create if missing)\n",
    "    if 'name_prefix' not in result_df.columns:\n",
    "        add_name_prefix(result_df, 'full_name_normalized', 'name_prefix', n=3, inplace=True)\n",
    "    \n",
    "    # Create _key_nonid: season_year|name_prefix\n",
    "    result_df['_key_nonid'] = result_df['season_year'].astype('Int64').astype(str) + '|' + result_df['name_prefix']\n",
    "    \n",
    "    # Create _rid: player_id|season_year|tag\n",
    "    result_df['_rid'] = (result_df['player_id'].astype(str) \n",
    "                         + '|' + result_df['season_year'].astype('Int64').astype(str) \n",
    "                         + f'|{tag}')\n",
    "    \n",
    "    return None if inplace else result_df\n",
    "\n",
    "\n",
    "print(\"✓ Common utility functions defined: add_name_prefix, extract_core_ids, merge_candidate_columns, create_nonid_keys\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ab1784",
   "metadata": {},
   "source": [
    "## 1. Load the XML datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e089e2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] data_loading - Loaded datasets - Lahman:115450 Reference:15215 Savant:6743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Datasets loaded:\n",
      "  Lahman   : 115450 records\n",
      "  Reference: 15215 records\n",
      "  Savant   : 6743 records\n"
     ]
    }
   ],
   "source": [
    "# Load mapped XMLs into DataFrames using PyDI\n",
    "logger = logging.getLogger('data_loading')\n",
    "\n",
    "# Load\n",
    "df_lahman    = load_xml(LAHMAN_FILE, name='lahman')\n",
    "df_reference = load_xml(REFERENCE_FILE, name='reference')\n",
    "df_savant    = load_xml(SAVANT_FILE, name='savant')\n",
    "\n",
    "# Optional: annotate sources\n",
    "for df, src in [(df_lahman, 'lahman'), (df_reference, 'reference'), (df_savant, 'savant')]:\n",
    "    df.attrs['source'] = src\n",
    "\n",
    "# Quick sanity prints\n",
    "print('\\nDatasets loaded:')\n",
    "print('  Lahman   :', len(df_lahman), 'records')\n",
    "print('  Reference:', len(df_reference), 'records')\n",
    "print('  Savant   :', len(df_savant), 'records')\n",
    "\n",
    "logger.info('Loaded datasets - Lahman:%d Reference:%d Savant:%d', len(df_lahman), len(df_reference), len(df_savant))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7925a3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied name_utils.normalize_name_for_blocking to Lahman/Reference/Savant.\n"
     ]
    }
   ],
   "source": [
    "# Apply shared name normalization to all three source DataFrames\n",
    "\n",
    "for df in (df_lahman, df_reference, df_savant):\n",
    "    if 'full_name' in df.columns:\n",
    "        df['full_name_normalized'] = df['full_name'].astype('string').map(normalize_name_for_blocking)\n",
    "    else:\n",
    "        df['full_name_normalized'] = ''\n",
    "\n",
    "print(\"Applied name_utils.normalize_name_for_blocking to Lahman/Reference/Savant.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b17cee89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Lahman: name normalization check ===\n",
      "  Total rows      : 115,450\n",
      "  Changed by norm : 1 (0.00%)\n",
      "  Still suspicious: 0 rows with '\\x..' or '\\'\n",
      "\n",
      "  Sample changed names (up to 20):\n",
      "    'c v matteson'  ->  'c matteson'\n",
      "\n",
      "  No suspicious patterns found in normalized names.\n",
      "\n",
      "=== Reference: name normalization check ===\n",
      "  Total rows      : 15,215\n",
      "  Changed by norm : 1,939 (12.74%)\n",
      "  Still suspicious: 0 rows with '\\x..' or '\\'\n",
      "\n",
      "  Sample changed names (up to 20):\n",
      "    'eli\\xc3\\xa9zer alfonzo'  ->  'eliezer alfonzo'\n",
      "    'mois\\xc3\\xa9s alou'  ->  'moises alou'\n",
      "    'alfredo am\\xc3\\xa9zaga'  ->  'alfredo amezaga'\n",
      "    'alberto \\xc3\\x81rias'  ->  'alberto arias'\n",
      "    'joaqu\\xc3\\xadn arias'  ->  'joaquin arias'\n",
      "    'jos\\xc3\\xa9 bautista'  ->  'jose bautista'\n",
      "    '\\xc3\\x89rik b\\xc3\\xa9dard'  ->  'erik bedard'\n",
      "    'carlos beltr\\xc3\\xa1n'  ->  'carlos beltran'\n",
      "    'adrian beltr\\xc3\\xa9'  ->  'adrian beltre'\n",
      "    '\\xc3\\x81ngel berroa'  ->  'angel berroa'\n",
      "    'gr\\xc3\\xa9gor blanco'  ->  'gregor blanco'\n",
      "    'emilio bonif\\xc3\\xa1cio'  ->  'emilio bonifacio'\n",
      "    'asdr\\xc3\\xbabal cabrera'  ->  'asdrubal cabrera'\n",
      "    'robinson can\\xc3\\xb3'  ->  'robinson cano'\n",
      "    'jorge cant\\xc3\\xba'  ->  'jorge cantu'\n",
      "    'jos\\xc3\\xa9 capell\\xc3\\xa1n'  ->  'jose capellan'\n",
      "    'ra\\xc3\\xbal casanova'  ->  'raul casanova'\n",
      "    'jos\\xc3\\xa9 castillo'  ->  'jose castillo'\n",
      "    'ram\\xc3\\xb3n castro'  ->  'ramon castro'\n",
      "    'ronny cede\\xc3\\xb1o'  ->  'ronny cedeno'\n",
      "\n",
      "  No suspicious patterns found in normalized names.\n",
      "\n",
      "=== Savant: name normalization check ===\n",
      "  Total rows      : 6,743\n",
      "  Changed by norm : 188 (2.79%)\n",
      "  Still suspicious: 0 rows with '\\x..' or '\\'\n",
      "\n",
      "  Sample changed names (up to 20):\n",
      "    'b j upton jr'  ->  'b j upton'\n",
      "    'rickie weeks jr'  ->  'rickie weeks'\n",
      "    'howie kendrick iii'  ->  'howie kendrick'\n",
      "    'nelson cruz jr'  ->  'nelson cruz'\n",
      "    'buster posey iii'  ->  'buster posey'\n",
      "    'eric young jr'  ->  'eric young'\n",
      "    'john mayberry jr'  ->  'john mayberry'\n",
      "    'ivan de jesus jr'  ->  'ivan de jesus'\n",
      "    'michael brantley jr'  ->  'michael brantley'\n",
      "    'steven souza jr'  ->  'steven souza'\n",
      "    'steve lombardozzi jr'  ->  'steve lombardozzi'\n",
      "    'george springer iii'  ->  'george springer'\n",
      "    'jackie bradley jr'  ->  'jackie bradley'\n",
      "    'b j upton jr'  ->  'b j upton'\n",
      "    'rickie weeks jr'  ->  'rickie weeks'\n",
      "    'howie kendrick iii'  ->  'howie kendrick'\n",
      "    'nelson cruz jr'  ->  'nelson cruz'\n",
      "    'buster posey iii'  ->  'buster posey'\n",
      "    'ivan de jesus jr'  ->  'ivan de jesus'\n",
      "    'michael brantley jr'  ->  'michael brantley'\n",
      "\n",
      "  No suspicious patterns found in normalized names.\n"
     ]
    }
   ],
   "source": [
    "def inspect_name_normalization(df: pd.DataFrame, name: str, n_samples: int = 20):\n",
    "    print(f\"\\n=== {name}: name normalization check ===\")\n",
    "    if 'full_name' not in df.columns or 'full_name_normalized' not in df.columns:\n",
    "        print(\"  Missing 'full_name' or 'full_name_normalized'.\")\n",
    "        return\n",
    "\n",
    "    # Compare raw vs normalized\n",
    "    raw = df['full_name'].astype(str)\n",
    "    norm = df['full_name_normalized'].astype(str)\n",
    "\n",
    "    changed = df[raw != norm]\n",
    "    print(f\"  Total rows      : {len(df):,}\")\n",
    "    print(f\"  Changed by norm : {len(changed):,} ({len(changed)/len(df):.2%})\")\n",
    "\n",
    "    # Look for remaining hex escapes or backslashes in normalized names\n",
    "    suspicious_mask = norm.str.contains(r'\\\\x[0-9a-fA-F]{2}', regex=True, na=False) | \\\n",
    "                      norm.str.contains(r'\\\\', regex=True, na=False)\n",
    "    suspicious = df[suspicious_mask]\n",
    "    print(f\"  Still suspicious: {len(suspicious):,} rows with '\\\\x..' or '\\\\'\")\n",
    "\n",
    "    # Show sample of changed names\n",
    "    print(f\"\\n  Sample changed names (up to {n_samples}):\")\n",
    "    for _, row in changed.head(n_samples).iterrows():\n",
    "        print(f\"    '{row['full_name']}'  ->  '{row['full_name_normalized']}'\")\n",
    "\n",
    "    # Show sample of suspicious normalized names\n",
    "    if len(suspicious) > 0:\n",
    "        print(f\"\\n  Sample suspicious normalized names (up to {n_samples}):\")\n",
    "        for _, row in suspicious.head(n_samples).iterrows():\n",
    "            print(f\"    '{row['full_name']}'  ->  '{row['full_name_normalized']}'\")\n",
    "    else:\n",
    "        print(\"\\n  No suspicious patterns found in normalized names.\")\n",
    "\n",
    "inspect_name_normalization(df_lahman, \"Lahman\")\n",
    "inspect_name_normalization(df_reference, \"Reference\")\n",
    "inspect_name_normalization(df_savant, \"Savant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65aefc4",
   "metadata": {},
   "source": [
    "## 2. Data quality checks and usability assessment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "117a30ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Lahman ==\n",
      "rows x cols: (115450, 24)\n",
      "columns: ['player_id', 'full_name', 'birth_year', 'season_year', 'league', 'team', 'games', 'at_bats', 'runs', 'hits', 'doubles', 'triples', 'home_runs', 'runs_batted_in', 'walks', 'intentional_walks', 'strikeouts', 'hit_by_pitch', 'sac_hits', 'sac_flies'] ...\n",
      "missing ratio (top):\n",
      "intentional_walks          31.75%\n",
      "sac_flies                  31.27%\n",
      "ground_into_double_play    22.04%\n",
      "caught_stealing            20.39%\n",
      "sac_hits                    5.26%\n",
      "hit_by_pitch                2.44%\n",
      "stolen_bases                2.05%\n",
      "strikeouts                  1.82%\n",
      "dtype: object\n",
      "key[player_id] null%=0.05% dup%=7.71%\n",
      "combo_key check: skipped\n",
      "text[full_name] unique(raw->20325, norm->20325)\n",
      "date[season_year] parse_fail%=0.00% year_range=(1871, 2024)\n",
      "\n",
      "== Reference ==\n",
      "rows x cols: (15215, 26)\n",
      "columns: ['player_id', 'full_name', 'birth_year', 'season_year', 'level', 'team', 'games', 'plate_appearances', 'at_bats', 'runs', 'hits', 'doubles', 'triples', 'home_runs', 'runs_batted_in', 'walks', 'intentional_walks', 'strikeouts', 'hit_by_pitch', 'sac_hits'] ...\n",
      "missing ratio (top):\n",
      "batting_average            1.08%\n",
      "player_id                   0.0%\n",
      "full_name                   0.0%\n",
      "caught_stealing             0.0%\n",
      "stolen_bases                0.0%\n",
      "ground_into_double_play     0.0%\n",
      "sac_flies                   0.0%\n",
      "sac_hits                    0.0%\n",
      "dtype: object\n",
      "key[player_id] null%=0.00% dup%=0.00%\n",
      "combo_key check: skipped\n",
      "text[full_name] unique(raw->3877, norm->3877)\n",
      "date[season_year] parse_fail%=0.00% year_range=(2008, 2024)\n",
      "\n",
      "== Savant ==\n",
      "rows x cols: (6743, 17)\n",
      "columns: ['player_id', 'full_name', 'birth_year', 'season_year', 'plate_appearances', 'at_bats', 'hits', 'doubles', 'triples', 'home_runs', 'walks', 'strikeouts', 'stolen_bases', 'caught_stealing', 'batting_average', 'average_best_speed', 'full_name_normalized'] ...\n",
      "missing ratio (top):\n",
      "average_best_speed    0.03%\n",
      "player_id              0.0%\n",
      "home_runs              0.0%\n",
      "batting_average        0.0%\n",
      "caught_stealing        0.0%\n",
      "stolen_bases           0.0%\n",
      "strikeouts             0.0%\n",
      "walks                  0.0%\n",
      "dtype: object\n",
      "key[player_id] null%=0.00% dup%=0.00%\n",
      "combo_key check: skipped\n",
      "text[full_name] unique(raw->1878, norm->1878)\n",
      "date[season_year] parse_fail%=0.00% year_range=(2015, 2024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] quality_checks - Quality checks completed\n"
     ]
    }
   ],
   "source": [
    "# Lightweight profiling of key data quality aspects\n",
    "logger = logging.getLogger('quality_checks')\n",
    "\n",
    "def safe_parse_dates(series: pd.Series) -> pd.Series:\n",
    "    try:\n",
    "        return pd.to_datetime(series, errors='coerce')\n",
    "    except Exception:\n",
    "        return pd.Series([pd.NaT] * len(series))\n",
    "\n",
    "def quick_profile(\n",
    "    df: pd.DataFrame,\n",
    "    name: str,\n",
    "    candidate_id_cols: Sequence[str] = (\"player_id\",),\n",
    "    text_cols: Sequence[str] = (\"full_name\",),\n",
    "    date_cols: Sequence[str] = (\"season_year\",),\n",
    "    combo_key_cols: Sequence[str] = (\"player_id\", \"season_year\"),\n",
    ") -> None:\n",
    "    print(f\"\\n== {name} ==\")\n",
    "    print('rows x cols:', df.shape)\n",
    "    # Show columns\n",
    "    print('columns:', list(df.columns)[:20], '...')\n",
    "\n",
    "    # Missing rate (top 8)\n",
    "    miss = df.isna().mean().sort_values(ascending=False)\n",
    "    if len(miss) > 0:\n",
    "        print('missing ratio (top):')\n",
    "        print((miss.head(8) * 100).round(2).astype(str) + '%')\n",
    "\n",
    "    # Candidate key uniqueness (single-column)\n",
    "    single_key_found = False\n",
    "    for k in candidate_id_cols:\n",
    "        if k in df.columns:\n",
    "            single_key_found = True\n",
    "            null_ratio = df[k].isna().mean()\n",
    "            dup_ratio = 1.0 - (df[k].nunique(dropna=True) / max(len(df), 1))\n",
    "            print(f\"key[{k}] null%={null_ratio:.2%} dup%={dup_ratio:.2%}\")\n",
    "            break\n",
    "    if not single_key_found:\n",
    "        print('key: no obvious single-column id found in', candidate_id_cols)\n",
    "\n",
    "    # Combination key uniqueness (e.g., player_id + season_year)\n",
    "    if combo_key_cols and all(c in df.columns for c in combo_key_cols):\n",
    "        combo_null = df[list(combo_key_cols)].isna().any(axis=1).mean()\n",
    "        nunique_combo = df[list(combo_key_cols)].astype(str).agg('|'.join, axis=1).nunique(dropna=True)\n",
    "        combo_dup = 1.0 - (nunique_combo / max(len(df), 1))\n",
    "        print(f\"combo_key{tuple(combo_key_cols)} null_row%={combo_null:.2%} dup%={combo_dup:.2%}\")\n",
    "    elif combo_key_cols:\n",
    "        print(f\"combo_key missing cols: {combo_key_cols}\")\n",
    "    else:\n",
    "        print('combo_key check: skipped')\n",
    "\n",
    "    # Text columns uniqueness before/after normalization\n",
    "    for c in text_cols:\n",
    "        if c in df.columns:\n",
    "            raw_u = df[c].nunique(dropna=True)\n",
    "            norm = df[c].astype(str).str.strip().str.lower()\n",
    "            norm_u = norm.nunique(dropna=True)\n",
    "            print(f\"text[{c}] unique(raw->{raw_u}, norm->{norm_u})\")\n",
    "\n",
    "    # Date/year columns: parse/inspect year distribution\n",
    "    for c in date_cols:\n",
    "        if c in df.columns:\n",
    "            # If it's numeric year, compute distribution directly\n",
    "            if pd.api.types.is_numeric_dtype(df[c]):\n",
    "                yrs = pd.to_numeric(df[c], errors='coerce').dropna()\n",
    "                year_range: Optional[tuple] = (int(yrs.min()), int(yrs.max())) if len(yrs) else None\n",
    "                print(f\"year[{c}] range={year_range}\")\n",
    "                if len(yrs):\n",
    "                    vc = yrs.value_counts().sort_index()\n",
    "                    head = vc.head(10)\n",
    "                    tail = vc.tail(3) if len(vc) > 10 else pd.Series(dtype='int64')\n",
    "                    print('year distribution (head):')\n",
    "                    print(head.to_string())\n",
    "                    if len(tail):\n",
    "                        print('...')\n",
    "                        print(tail.to_string())\n",
    "            else:\n",
    "                d = safe_parse_dates(df[c])\n",
    "                fail = d.isna().mean()\n",
    "                yrs = d.dt.year.dropna()\n",
    "                year_range: Optional[tuple] = (int(yrs.min()), int(yrs.max())) if len(yrs) else None\n",
    "                print(f\"date[{c}] parse_fail%={fail:.2%} year_range={year_range}\")\n",
    "\n",
    "# Run checks for three datasets with updated defaults (single key only)\n",
    "quick_profile(df_lahman, 'Lahman', candidate_id_cols=(\"player_id\",), text_cols=(\"full_name\",), date_cols=(\"season_year\",), combo_key_cols=())\n",
    "quick_profile(df_reference, 'Reference', candidate_id_cols=(\"player_id\",), text_cols=(\"full_name\",), date_cols=(\"season_year\",), combo_key_cols=())\n",
    "quick_profile(df_savant, 'Savant', candidate_id_cols=(\"player_id\",), text_cols=(\"full_name\",), date_cols=(\"season_year\",), combo_key_cols=())\n",
    "\n",
    "logger.info('Quality checks completed')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d20acd",
   "metadata": {},
   "source": [
    "### 2.a Duplicate check using composite key: player_id + full_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "36f47551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Lahman: duplicate report for (player_id, full_name) ==\n",
      "total rows=115450  unique_pairs=106583  pair_dup_ratio=7.68%  dup_row_ratio=14.89%\n",
      "top duplicate keys (pair,count):\n",
      "  (nan, jack taylor) -> 11\n",
      "  (nan, darby o brien) -> 6\n",
      "  (1135201892, tom dowse) -> 5\n",
      "  (1162691904, frank huelsman) -> 5\n",
      "  (6575082024, mike baumann) -> 5\n",
      "\n",
      "== Reference: duplicate report for (player_id, full_name) ==\n",
      "total rows=15215  unique_pairs=15215  pair_dup_ratio=0.00%  dup_row_ratio=0.00%\n",
      "no duplicate pairs found\n",
      "\n",
      "== Savant: duplicate report for (player_id, full_name) ==\n",
      "total rows=6743  unique_pairs=6743  pair_dup_ratio=0.00%  dup_row_ratio=0.00%\n",
      "no duplicate pairs found\n"
     ]
    }
   ],
   "source": [
    "def dup_report(df: pd.DataFrame, name: str, id_col='player_id', name_col='full_name', show_examples=5):\n",
    "    print(f\"\\n== {name}: duplicate report for ({id_col}, {name_col}) ==\")\n",
    "    if id_col not in df.columns or name_col not in df.columns:\n",
    "        print('missing required columns')\n",
    "        return\n",
    "    key_df = df[[id_col, name_col]].copy()\n",
    "    key_df[name_col] = key_df[name_col].astype(str).str.strip().str.lower()\n",
    "\n",
    "    n = len(key_df)\n",
    "    nunique_pairs = key_df.astype(str).agg('|'.join, axis=1).nunique(dropna=True)\n",
    "    dup_ratio = 1.0 - (nunique_pairs / max(n, 1))\n",
    "\n",
    "    # Row-level duplicate mask\n",
    "    mask_dup = key_df.duplicated(keep=False)\n",
    "    dup_rows = mask_dup.sum()\n",
    "    dup_row_ratio = dup_rows / max(n, 1)\n",
    "\n",
    "    print(f'total rows={n}  unique_pairs={nunique_pairs}  pair_dup_ratio={dup_ratio:.2%}  dup_row_ratio={dup_row_ratio:.2%}')\n",
    "\n",
    "    # Show top offending keys\n",
    "    vc = key_df.astype(str).agg('|'.join, axis=1).value_counts()\n",
    "    offenders = vc[vc > 1].head(show_examples)\n",
    "    if len(offenders) == 0:\n",
    "        print('no duplicate pairs found')\n",
    "    else:\n",
    "        print('top duplicate keys (pair,count):')\n",
    "        for k, c in offenders.items():\n",
    "            pid, nm = k.split('|', 1)\n",
    "            print(f'  ({pid}, {nm}) -> {c}')\n",
    "\n",
    "# Run for three datasets\n",
    "dup_report(df_lahman, 'Lahman')\n",
    "dup_report(df_reference, 'Reference')\n",
    "dup_report(df_savant, 'Savant')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fb587c",
   "metadata": {},
   "source": [
    "### 2.b Duplicate check on Lahman using composite key: player_id + team +games\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9030aafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lahman (player_id, team, games): total=115450 unique_pairs=115439 pair_dup_ratio=0.01% dup_row_ratio=0.02%\n",
      "top duplicate (player_id, team, games) triples:\n",
      "  (1127931890, ny1, 2) -> 2\n",
      "  (1122561914, brf, 3) -> 2\n",
      "  (1102121972, oak, 10) -> 2\n",
      "  (1196181925, sln, 1) -> 2\n",
      "  (1140751971, oak, 2) -> 2\n",
      "  (nan, br2, 2) -> 2\n",
      "  (2795702004, tex, 8) -> 2\n",
      "  (1146061888, ws8, 1) -> 2\n",
      "  (1161002000, ari, 4) -> 2\n",
      "  (6683382023, lan, 1) -> 2\n"
     ]
    }
   ],
   "source": [
    "# Duplicate report for Lahman on (player_id, team, games)\n",
    "\n",
    "req = ['player_id', 'team', 'games']\n",
    "missing = [c for c in req if c not in df_lahman.columns]\n",
    "if missing:\n",
    "    print('Lahman is missing required columns:', missing)\n",
    "else:\n",
    "    key_df = df_lahman[req].copy()\n",
    "    # normalize\n",
    "    key_df['team'] = key_df['team'].astype(str).str.strip().str.lower()\n",
    "    key_df['games'] = pd.to_numeric(key_df['games'], errors='coerce')\n",
    "\n",
    "    n = len(key_df)\n",
    "    pair_keys = key_df.astype(str).agg('|'.join, axis=1)\n",
    "    nunique_pairs = pair_keys.nunique(dropna=True)\n",
    "    pair_dup_ratio = 1.0 - (nunique_pairs / max(n, 1))\n",
    "\n",
    "    dup_mask = pair_keys.duplicated(keep=False)\n",
    "    dup_row_ratio = dup_mask.mean()\n",
    "\n",
    "    print(f\"Lahman (player_id, team, games): total={n} unique_pairs={nunique_pairs} \"\n",
    "          f\"pair_dup_ratio={pair_dup_ratio:.2%} dup_row_ratio={dup_row_ratio:.2%}\")\n",
    "\n",
    "    offenders = pair_keys.value_counts()\n",
    "    offenders = offenders[offenders > 1].head(10)\n",
    "    if len(offenders) == 0:\n",
    "        print('no duplicate (player_id, team, games) triples found')\n",
    "    else:\n",
    "        print('top duplicate (player_id, team, games) triples:')\n",
    "        for k, c in offenders.items():\n",
    "            pid, team, games = k.split('|', 2)\n",
    "            print(f'  ({pid}, {team}, {games}) -> {c}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d69a15b",
   "metadata": {},
   "source": [
    "### 2.c Full-row duplicate check (all columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6295c9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Exact duplicate rows (ALL columns) - Lahman ==\n",
      "total rows = 115450\n",
      "rows in duplicated groups (keep=False) = 6\n",
      "exact duplicate rows (excluding first occurrences) = 3\n",
      "number of unique duplicate groups = 3\n",
      "top duplicate groups (show up to 5):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>player_id</th>\n",
       "      <th>full_name</th>\n",
       "      <th>birth_year</th>\n",
       "      <th>season_year</th>\n",
       "      <th>league</th>\n",
       "      <th>team</th>\n",
       "      <th>games</th>\n",
       "      <th>at_bats</th>\n",
       "      <th>runs</th>\n",
       "      <th>hits</th>\n",
       "      <th>...</th>\n",
       "      <th>intentional_walks</th>\n",
       "      <th>strikeouts</th>\n",
       "      <th>hit_by_pitch</th>\n",
       "      <th>sac_hits</th>\n",
       "      <th>sac_flies</th>\n",
       "      <th>ground_into_double_play</th>\n",
       "      <th>stolen_bases</th>\n",
       "      <th>caught_stealing</th>\n",
       "      <th>full_name_normalized</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34191</th>\n",
       "      <td>1161002000</td>\n",
       "      <td>darren holmes</td>\n",
       "      <td>1966</td>\n",
       "      <td>2000</td>\n",
       "      <td>NL</td>\n",
       "      <td>ARI</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>darren holmes</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113618</th>\n",
       "      <td>6683382023</td>\n",
       "      <td>tyson miller</td>\n",
       "      <td>1995</td>\n",
       "      <td>2023</td>\n",
       "      <td>NL</td>\n",
       "      <td>LAN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>tyson miller</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15692</th>\n",
       "      <td>1127931890</td>\n",
       "      <td>sam crane</td>\n",
       "      <td>1854</td>\n",
       "      <td>1890</td>\n",
       "      <td>NL</td>\n",
       "      <td>NY1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sam crane</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         player_id      full_name birth_year season_year league team games  \\\n",
       "34191   1161002000  darren holmes       1966        2000     NL  ARI     4   \n",
       "113618  6683382023   tyson miller       1995        2023     NL  LAN     1   \n",
       "15692   1127931890      sam crane       1854        1890     NL  NY1     2   \n",
       "\n",
       "       at_bats runs hits  ... intentional_walks strikeouts hit_by_pitch  \\\n",
       "34191        0    0    0  ...                 0          0            0   \n",
       "113618       0    0    0  ...                 0          0            0   \n",
       "15692        6    0    0  ...               NaN          0            0   \n",
       "\n",
       "       sac_hits sac_flies ground_into_double_play stolen_bases  \\\n",
       "34191         0         0                       0            0   \n",
       "113618        0         0                       0            0   \n",
       "15692       NaN       NaN                     NaN            1   \n",
       "\n",
       "       caught_stealing full_name_normalized count  \n",
       "34191                0        darren holmes     2  \n",
       "113618               0         tyson miller     2  \n",
       "15692              NaN            sam crane     2  \n",
       "\n",
       "[3 rows x 25 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('\\n== Exact duplicate rows (ALL columns) - Lahman ==')\n",
    "\n",
    "n_total = len(df_lahman)\n",
    "# Mask of rows that have an identical predecessor across ALL columns\n",
    "mask_exact_dups_any = df_lahman.duplicated(keep=False)\n",
    "mask_exact_dups_first = df_lahman.duplicated(keep='first')\n",
    "num_rows_in_duplicated_groups = int(mask_exact_dups_any.sum())\n",
    "num_exact_dup_rows_excluding_first = int(mask_exact_dups_first.sum())\n",
    "\n",
    "print(f'total rows = {n_total}')\n",
    "print(f'rows in duplicated groups (keep=False) = {num_rows_in_duplicated_groups}')\n",
    "print(f'exact duplicate rows (excluding first occurrences) = {num_exact_dup_rows_excluding_first}')\n",
    "\n",
    "if num_rows_in_duplicated_groups > 0:\n",
    "    # Group by all columns to find groups with count > 1\n",
    "    grp_sizes = (df_lahman\n",
    "                 .groupby(list(df_lahman.columns), dropna=False)\n",
    "                 .size()\n",
    "                 .reset_index(name='count')\n",
    "                 .sort_values('count', ascending=False))\n",
    "    dup_groups = grp_sizes[grp_sizes['count'] > 1]\n",
    "    print(f'number of unique duplicate groups = {len(dup_groups)}')\n",
    "    print('top duplicate groups (show up to 5):')\n",
    "    display(dup_groups.head(5))\n",
    "else:\n",
    "    print('No exact duplicate rows found.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f0dd64",
   "metadata": {},
   "source": [
    "## 3. Deduplicate Lahman\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64535484",
   "metadata": {},
   "source": [
    "### 3.1 Exact-row Business-key deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5c775f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] dedup - Exact dedup finished: 115450 -> 115447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exact dedup] Lahman: 115450 -> 115447 (removed 3)\n",
      "\n",
      "[Primary key check] Before dedup:\n",
      "  key=('player_id',) dup_ratio=7.70% dup_row_ratio=14.91%\n",
      "[Primary key dedup] Applied dedup on ('player_id',)\n",
      "  key=('player_id',) dup_ratio=0.00% dup_row_ratio=0.00%\n",
      "Final Lahman rows: 106553\n"
     ]
    }
   ],
   "source": [
    "log = logging.getLogger('dedup')\n",
    "\n",
    "# --- Step 1: exact-row dedup ---\n",
    "before = len(df_lahman)\n",
    "df_lahman = df_lahman.drop_duplicates()\n",
    "after = len(df_lahman)\n",
    "print(f'[Exact dedup] Lahman: {before} -> {after} (removed {before-after})')\n",
    "log.info('Exact dedup finished: %d -> %d', before, after)\n",
    "\n",
    "# Helpers\n",
    "\n",
    "def report_dup_ratio(df: pd.DataFrame, key_cols):\n",
    "    keys = df[key_cols].astype(str).agg('|'.join, axis=1)\n",
    "    nunique = keys.nunique(dropna=True)\n",
    "    dup_ratio = 1.0 - (nunique / max(len(df), 1))\n",
    "    dup_rows = keys.duplicated(keep=False).mean()\n",
    "    print(f'  key={tuple(key_cols)} dup_ratio={dup_ratio:.2%} dup_row_ratio={dup_rows:.2%}')\n",
    "    return dup_ratio, dup_rows\n",
    "\n",
    "\n",
    "def dedup_by_business_key(df: pd.DataFrame, key_cols, prefer_desc_cols=None):\n",
    "    \"\"\"Keep one row per key using a deterministic preference:\n",
    "    - Fewer NaNs first\n",
    "    - Then by prefer_desc_cols (e.g., games, plate_appearances) descending\n",
    "    - Stable tie-breaker by index ascending\n",
    "    \"\"\"\n",
    "    if prefer_desc_cols is None:\n",
    "        prefer_desc_cols = []\n",
    "    work = df.copy()\n",
    "    work['_na_cnt'] = work.isna().sum(axis=1)\n",
    "    sort_cols = list(key_cols) + ['_na_cnt'] + prefer_desc_cols\n",
    "    ascending = [True]*len(key_cols) + [True] + [False]*len(prefer_desc_cols)\n",
    "    work = work.sort_values(sort_cols, ascending=ascending, kind='mergesort')\n",
    "    result = work.drop_duplicates(subset=key_cols, keep='first').drop(columns=['_na_cnt'])\n",
    "    return result\n",
    "\n",
    "# --- Step 2: Primary key dedup on player_id only ---\n",
    "key = [c for c in ['player_id'] if c in df_lahman.columns]\n",
    "print('\\n[Primary key check] Before dedup:')\n",
    "report_dup_ratio(df_lahman, key)\n",
    "\n",
    "prefer_cols = [c for c in ['games','plate_appearances','at_bats','hits','runs'] if c in df_lahman.columns]\n",
    "\n",
    "df_lahman = dedup_by_business_key(df_lahman, key, prefer_desc_cols=prefer_cols)\n",
    "print('[Primary key dedup] Applied dedup on', tuple(key))\n",
    "report_dup_ratio(df_lahman, key)\n",
    "print('Final Lahman rows:', len(df_lahman))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba6c1c3",
   "metadata": {},
   "source": [
    "### 3.2 Save deduplicated Lahman to XML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "caf98e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved deduplicated Lahman XML to: /Users/zhangzihan/Desktop/WBI_project/Schema_Mapped_Datasets/data/output/clean/Lahman_Mapped_dedup.xml\n",
      "Rows written: 106553\n"
     ]
    }
   ],
   "source": [
    "# Write df_lahman (deduplicated) to XML\n",
    "\n",
    "save_dir = OUTPUT_DIR / 'clean'\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "xml_path = save_dir / 'Lahman_Mapped_dedup.xml'\n",
    "\n",
    "# pandas >=1.3 supports DataFrame.to_xml\n",
    "# Use generic structure: <dataset><record>...</record>...</dataset>\n",
    "df_lahman.to_xml(\n",
    "    xml_path,\n",
    "    index=False,\n",
    "    root_name='dataset',\n",
    "    row_name='record',\n",
    "    encoding='utf-8'\n",
    ")\n",
    "\n",
    "print('Saved deduplicated Lahman XML to:', xml_path)\n",
    "print('Rows written:', len(df_lahman))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0b4e22ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized dataframes ready: 106553 15215 6743\n"
     ]
    }
   ],
   "source": [
    "# Quick size check for the three source tables\n",
    "# Step 5.0: Normalize inputs (adds *_norm columns, keeps originals)\n",
    "\n",
    "L, R, S = df_lahman.copy(), df_reference.copy(), df_savant.copy()\n",
    "\n",
    "print('Normalized dataframes ready:', len(L), len(R), len(S))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540a47af",
   "metadata": {},
   "source": [
    "## 4. Build candidate pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d8174d",
   "metadata": {},
   "source": [
    "### 4.1 Prepare normalized names and blocking keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52d6b57",
   "metadata": {},
   "source": [
    "### 4.2 Build candidates on (season_year + name_prefix)\n",
    "\n",
    "This section builds the main candidate channel for LR and LS by blocking on `season_year + name_prefix` using PyDI `StandardBlocker`. \n",
    "It does not rely on `player_id`, so the resulting candidate pairs can be used directly for training, validation, and evaluation of \n",
    "blocking and matching methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9a0d68af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - Creating blocking key values for dataset1: 106553 records\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - Creating blocking key values for dataset2: 15215 records\n",
      "[INFO ] PyDI.entitymatching.blocking.standard.StandardBlocker - created 105016 blocking keys for first dataset\n",
      "[INFO ] PyDI.entitymatching.blocking.standard.StandardBlocker - created 15018 blocking keys for second dataset\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - Joining blocking key values: 105016 x 15018 blocks\n",
      "[INFO ] PyDI.entitymatching.blocking.standard.StandardBlocker - created 14961 blocks from blocking keys\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - Block size distribution:\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - Size Frequency\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 14603       1\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 171         2\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 150         4\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 11          3\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 9           6\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 8           9\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 3           12\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 2           8\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 1           48\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 1           35\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 1           24\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 1           16\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - Blocking key values:\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - BlockingKeyValue\tFrequency\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 2016|aj\t\t\t48\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 2017|aj\t\t\t35\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 2015|aj\t\t\t24\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 2013|aj\t\t\t16\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 2008|aj\t\t\t12\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 2012|aj\t\t\t12\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 2014|aj\t\t\t12\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 2010|chrcar\t\t\t9\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 2009|aj\t\t\t9\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 2010|aj\t\t\t9\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 2011|aj\t\t\t9\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 2010|jeffra\t\t\t9\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 2019|nicmar\t\t\t9\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 2019|jd\t\t\t9\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 2021|jd\t\t\t9\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 2018|aj\t\t\t8\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 2024|jp\t\t\t8\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 2011|chrcar\t\t\t6\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 2012|chrcar\t\t\t6\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 2014|jj\t\t\t6\n",
      "[INFO ] PyDI.entitymatching.blocking.standard.StandardBlocker - Debug results written to file: /Users/zhangzihan/Desktop/WBI_project/Schema_Mapped_Datasets/data/output/blocking-evaluation/debugResultsBlocking_StandardBlocker.csv\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - Creating candidate record pairs from 14961 blocks\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - Creating blocking key values for dataset1: 106553 records\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - Creating blocking key values for dataset2: 6743 records\n",
      "[INFO ] PyDI.entitymatching.blocking.standard.StandardBlocker - created 105016 blocking keys for first dataset\n",
      "[INFO ] PyDI.entitymatching.blocking.standard.StandardBlocker - created 6696 blocking keys for second dataset\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - Joining blocking key values: 105016 x 6696 blocks\n",
      "[INFO ] PyDI.entitymatching.blocking.standard.StandardBlocker - created 6649 blocks from blocking keys\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - Block size distribution:\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - Size Frequency\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 6488        1\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 108         2\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 36          4\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 9           3\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 3           6\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 1           21\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 1           18\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 1           16\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 1           9\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 1           8\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - Blocking key values:\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - BlockingKeyValue\tFrequency\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 2017|aj\t\t\t21\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 2015|aj\t\t\t18\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 2016|aj\t\t\t16\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 2019|nicmar\t\t\t9\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 2024|jp\t\t\t8\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 2019|jd\t\t\t6\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 2021|jd\t\t\t6\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 2023|jp\t\t\t6\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 2019|zacgre\t\t\t4\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 2015|carcor\t\t\t4\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 2021|maxsch\t\t\t4\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 2018|aj\t\t\t4\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 2018|chrste\t\t\t4\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 2018|nicmar\t\t\t4\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 2017|jaybru\t\t\t4\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 2015|chrcol\t\t\t4\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 2016|chrcol\t\t\t4\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 2017|davfre\t\t\t4\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 2018|davfre\t\t\t4\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - 2019|davfre\t\t\t4\n",
      "[INFO ] PyDI.entitymatching.blocking.standard.StandardBlocker - Debug results written to file: /Users/zhangzihan/Desktop/WBI_project/Schema_Mapped_Datasets/data/output/blocking-evaluation/debugResultsBlocking_StandardBlocker.csv\n",
      "[DEBUG] PyDI.entitymatching.blocking.standard.StandardBlocker - Creating candidate record pairs from 6649 blocks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built NON-ID candidates for LR/RS/LS (season_year + normalized name_prefix)\n"
     ]
    }
   ],
   "source": [
    "# NON-ID candidates: StandardBlocker on (season_year + normalized name_prefix)\n",
    "# Note: Uses shared utility functions from Section 0.3\n",
    "\n",
    "def build_nonid_candidates(left: pd.DataFrame, right: pd.DataFrame, lt: str, rt: str) -> pd.DataFrame:\n",
    "    \"\"\"Build non-ID candidate pairs using StandardBlocker on season_year + name_prefix.\n",
    "    \n",
    "    Uses shared utility functions: create_nonid_keys, merge_candidate_columns\n",
    "    \"\"\"\n",
    "    Lx = left.copy(); Rx = right.copy()\n",
    "    \n",
    "    # Create non-ID keys using shared function (creates _key_nonid and _rid)\n",
    "    create_nonid_keys(Lx, lt, inplace=True)\n",
    "    create_nonid_keys(Rx, rt, inplace=True)\n",
    "\n",
    "    blocker = StandardBlocker(Lx, Rx, on=['_key_nonid'], id_column='_rid', output_dir=OUTPUT_DIR / 'blocking-evaluation')\n",
    "    pairs = blocker.materialize()\n",
    "\n",
    "    # attach context using shared merge function\n",
    "    c = merge_candidate_columns(pairs, Lx, Rx, lt, rt)\n",
    "\n",
    "    # attach normalized names for inspection (do not overwrite raw full_name)\n",
    "    if 'full_name_normalized' in Lx.columns:\n",
    "        left_norm = Lx.set_index('_rid')['full_name_normalized']\n",
    "        c['full_name_norm_L'] = c['id1'].map(left_norm)\n",
    "    if 'full_name_normalized' in Rx.columns:\n",
    "        right_norm = Rx.set_index('_rid')['full_name_normalized']\n",
    "        c['full_name_norm_R'] = c['id2'].map(right_norm)\n",
    "\n",
    "    # standardized columns (merge creates suffixed columns already)\n",
    "    if f'season_year_{lt}' not in c.columns and 'season_year' in c.columns:\n",
    "        c[f'season_year_{lt}'] = c['season_year']\n",
    "    if f'season_year_{rt}' not in c.columns and 'season_year' in c.columns:\n",
    "        c[f'season_year_{rt}'] = c['season_year']\n",
    "    c['block_type'] = 'non_id_eq'\n",
    "\n",
    "    # keep compact\n",
    "    keep = [\n",
    "        'id1', 'id2',\n",
    "        f'player_id_{lt}' if f'player_id_{lt}' in c.columns else None,\n",
    "        f'player_id_{rt}' if f'player_id_{rt}' in c.columns else None,\n",
    "        f'season_year_{lt}', f'season_year_{rt}',\n",
    "        f'full_name_{lt}' if f'full_name_{lt}' in c.columns else None,\n",
    "        f'full_name_{rt}' if f'full_name_{rt}' in c.columns else None,\n",
    "        f'birth_year_{lt}' if f'birth_year_{lt}' in c.columns else None,\n",
    "        f'birth_year_{rt}' if f'birth_year_{rt}' in c.columns else None,\n",
    "        'full_name_norm_L' if 'full_name_norm_L' in c.columns else None,\n",
    "        'full_name_norm_R' if 'full_name_norm_R' in c.columns else None,\n",
    "        'block_type'\n",
    "    ]\n",
    "    keep = [k for k in keep if k and k in c.columns]\n",
    "    return c[keep]\n",
    "\n",
    "\n",
    "# Build NON-ID candidates for three edges\n",
    "cand_lr_nonid = build_nonid_candidates(L, R, 'L', 'R')\n",
    "# cand_rs_nonid = build_nonid_candidates(R, S, 'R', 'S')  # RS skipped for now\n",
    "cand_ls_nonid = build_nonid_candidates(L, S, 'L', 'S')\n",
    "\n",
    "print('Built NON-ID candidates for LR/RS/LS (season_year + normalized name_prefix)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675fb8d7",
   "metadata": {},
   "source": [
    "### 4.3 Inspect candidate quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3c5f87c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id1</th>\n",
       "      <th>id2</th>\n",
       "      <th>player_id_L</th>\n",
       "      <th>player_id_R</th>\n",
       "      <th>season_year_L</th>\n",
       "      <th>season_year_R</th>\n",
       "      <th>full_name_L</th>\n",
       "      <th>full_name_R</th>\n",
       "      <th>birth_year_L</th>\n",
       "      <th>birth_year_R</th>\n",
       "      <th>full_name_norm_L</th>\n",
       "      <th>full_name_norm_R</th>\n",
       "      <th>block_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1100292008|2008|L</td>\n",
       "      <td>1100292008|2008|R</td>\n",
       "      <td>1100292008</td>\n",
       "      <td>1100292008</td>\n",
       "      <td>2008</td>\n",
       "      <td>2008</td>\n",
       "      <td>bobby abreu</td>\n",
       "      <td>bobby abreu</td>\n",
       "      <td>1974</td>\n",
       "      <td>1974</td>\n",
       "      <td>bobby abreu</td>\n",
       "      <td>bobby abreu</td>\n",
       "      <td>non_id_eq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1100292009|2009|L</td>\n",
       "      <td>1100292009|2009|R</td>\n",
       "      <td>1100292009</td>\n",
       "      <td>1100292009</td>\n",
       "      <td>2009</td>\n",
       "      <td>2009</td>\n",
       "      <td>bobby abreu</td>\n",
       "      <td>bobby abreu</td>\n",
       "      <td>1974</td>\n",
       "      <td>1974</td>\n",
       "      <td>bobby abreu</td>\n",
       "      <td>bobby abreu</td>\n",
       "      <td>non_id_eq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1100292010|2010|L</td>\n",
       "      <td>1100292010|2010|R</td>\n",
       "      <td>1100292010</td>\n",
       "      <td>1100292010</td>\n",
       "      <td>2010</td>\n",
       "      <td>2010</td>\n",
       "      <td>bobby abreu</td>\n",
       "      <td>bobby abreu</td>\n",
       "      <td>1974</td>\n",
       "      <td>1974</td>\n",
       "      <td>bobby abreu</td>\n",
       "      <td>bobby abreu</td>\n",
       "      <td>non_id_eq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1100292011|2011|L</td>\n",
       "      <td>1100292011|2011|R</td>\n",
       "      <td>1100292011</td>\n",
       "      <td>1100292011</td>\n",
       "      <td>2011</td>\n",
       "      <td>2011</td>\n",
       "      <td>bobby abreu</td>\n",
       "      <td>bobby abreu</td>\n",
       "      <td>1974</td>\n",
       "      <td>1974</td>\n",
       "      <td>bobby abreu</td>\n",
       "      <td>bobby abreu</td>\n",
       "      <td>non_id_eq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1100292012|2012|L</td>\n",
       "      <td>1100292012|2012|R</td>\n",
       "      <td>1100292012</td>\n",
       "      <td>1100292012</td>\n",
       "      <td>2012</td>\n",
       "      <td>2012</td>\n",
       "      <td>bobby abreu</td>\n",
       "      <td>bobby abreu</td>\n",
       "      <td>1974</td>\n",
       "      <td>1974</td>\n",
       "      <td>bobby abreu</td>\n",
       "      <td>bobby abreu</td>\n",
       "      <td>non_id_eq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15874</th>\n",
       "      <td>8053732024|2024|L</td>\n",
       "      <td>8053732024|2024|R</td>\n",
       "      <td>8053732024</td>\n",
       "      <td>8053732024</td>\n",
       "      <td>2024</td>\n",
       "      <td>2024</td>\n",
       "      <td>nacho alvarez</td>\n",
       "      <td>nacho alvarez jr</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>nacho alvarez</td>\n",
       "      <td>nacho alvarez</td>\n",
       "      <td>non_id_eq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15875</th>\n",
       "      <td>8057792024|2024|L</td>\n",
       "      <td>8057792024|2024|R</td>\n",
       "      <td>8057792024</td>\n",
       "      <td>8057792024</td>\n",
       "      <td>2024</td>\n",
       "      <td>2024</td>\n",
       "      <td>jacob wilson</td>\n",
       "      <td>jacob wilson</td>\n",
       "      <td>2002</td>\n",
       "      <td>2002</td>\n",
       "      <td>jacob wilson</td>\n",
       "      <td>jacob wilson</td>\n",
       "      <td>non_id_eq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15876</th>\n",
       "      <td>8077992023|2023|L</td>\n",
       "      <td>8077992023|2023|R</td>\n",
       "      <td>8077992023</td>\n",
       "      <td>8077992023</td>\n",
       "      <td>2023</td>\n",
       "      <td>2023</td>\n",
       "      <td>masataka yoshida</td>\n",
       "      <td>masataka yoshida</td>\n",
       "      <td>1993</td>\n",
       "      <td>1994</td>\n",
       "      <td>masataka yoshida</td>\n",
       "      <td>masataka yoshida</td>\n",
       "      <td>non_id_eq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15877</th>\n",
       "      <td>8077992024|2024|L</td>\n",
       "      <td>8077992024|2024|R</td>\n",
       "      <td>8077992024</td>\n",
       "      <td>8077992024</td>\n",
       "      <td>2024</td>\n",
       "      <td>2024</td>\n",
       "      <td>masataka yoshida</td>\n",
       "      <td>masataka yoshida</td>\n",
       "      <td>1993</td>\n",
       "      <td>1994</td>\n",
       "      <td>masataka yoshida</td>\n",
       "      <td>masataka yoshida</td>\n",
       "      <td>non_id_eq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15878</th>\n",
       "      <td>8089822024|2024|L</td>\n",
       "      <td>8089822024|2024|R</td>\n",
       "      <td>8089822024</td>\n",
       "      <td>8089822024</td>\n",
       "      <td>2024</td>\n",
       "      <td>2024</td>\n",
       "      <td>jung hoo lee</td>\n",
       "      <td>jung hoo lee</td>\n",
       "      <td>1998</td>\n",
       "      <td>1999</td>\n",
       "      <td>jung hoo lee</td>\n",
       "      <td>jung hoo lee</td>\n",
       "      <td>non_id_eq</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15879 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id1                id2 player_id_L player_id_R  \\\n",
       "0      1100292008|2008|L  1100292008|2008|R  1100292008  1100292008   \n",
       "1      1100292009|2009|L  1100292009|2009|R  1100292009  1100292009   \n",
       "2      1100292010|2010|L  1100292010|2010|R  1100292010  1100292010   \n",
       "3      1100292011|2011|L  1100292011|2011|R  1100292011  1100292011   \n",
       "4      1100292012|2012|L  1100292012|2012|R  1100292012  1100292012   \n",
       "...                  ...                ...         ...         ...   \n",
       "15874  8053732024|2024|L  8053732024|2024|R  8053732024  8053732024   \n",
       "15875  8057792024|2024|L  8057792024|2024|R  8057792024  8057792024   \n",
       "15876  8077992023|2023|L  8077992023|2023|R  8077992023  8077992023   \n",
       "15877  8077992024|2024|L  8077992024|2024|R  8077992024  8077992024   \n",
       "15878  8089822024|2024|L  8089822024|2024|R  8089822024  8089822024   \n",
       "\n",
       "       season_year_L  season_year_R       full_name_L       full_name_R  \\\n",
       "0               2008           2008       bobby abreu       bobby abreu   \n",
       "1               2009           2009       bobby abreu       bobby abreu   \n",
       "2               2010           2010       bobby abreu       bobby abreu   \n",
       "3               2011           2011       bobby abreu       bobby abreu   \n",
       "4               2012           2012       bobby abreu       bobby abreu   \n",
       "...              ...            ...               ...               ...   \n",
       "15874           2024           2024     nacho alvarez  nacho alvarez jr   \n",
       "15875           2024           2024      jacob wilson      jacob wilson   \n",
       "15876           2023           2023  masataka yoshida  masataka yoshida   \n",
       "15877           2024           2024  masataka yoshida  masataka yoshida   \n",
       "15878           2024           2024      jung hoo lee      jung hoo lee   \n",
       "\n",
       "      birth_year_L birth_year_R  full_name_norm_L  full_name_norm_R block_type  \n",
       "0             1974         1974       bobby abreu       bobby abreu  non_id_eq  \n",
       "1             1974         1974       bobby abreu       bobby abreu  non_id_eq  \n",
       "2             1974         1974       bobby abreu       bobby abreu  non_id_eq  \n",
       "3             1974         1974       bobby abreu       bobby abreu  non_id_eq  \n",
       "4             1974         1974       bobby abreu       bobby abreu  non_id_eq  \n",
       "...            ...          ...               ...               ...        ...  \n",
       "15874         2003         2003     nacho alvarez     nacho alvarez  non_id_eq  \n",
       "15875         2002         2002      jacob wilson      jacob wilson  non_id_eq  \n",
       "15876         1993         1994  masataka yoshida  masataka yoshida  non_id_eq  \n",
       "15877         1993         1994  masataka yoshida  masataka yoshida  non_id_eq  \n",
       "15878         1998         1999      jung hoo lee      jung hoo lee  non_id_eq  \n",
       "\n",
       "[15879 rows x 13 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cand_lr_nonid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "17d2a6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== cand_lr_nonid: garbled name check ===\n",
      "  Column 'full_name_norm_L': 0 suspicious values\n",
      "  Column 'full_name_norm_R': 0 suspicious values\n",
      "\n",
      "  Total rows with suspicious names: 0\n",
      "  No garbled names detected.\n"
     ]
    }
   ],
   "source": [
    "def inspect_garbled_names(df: pd.DataFrame,\n",
    "                          name: str,\n",
    "                          cols=('full_name_norm_L', 'full_name_norm_R'),\n",
    "                          n_samples: int = 20) -> None:\n",
    "    \"\"\"Check for garbled names (hex escapes, double backslashes, non-ASCII).\"\"\"\n",
    "    print(f\"\\n=== {name}: garbled name check ===\")\n",
    "\n",
    "    # Start with an all-False boolean mask over rows\n",
    "    bad_mask = pd.Series(False, index=df.index)\n",
    "\n",
    "    for col in cols:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        s = df[col].astype(str)\n",
    "\n",
    "        # Patterns for typical encoding artifacts (\\xHH, over-escaped backslashes)\n",
    "        hex_mask = s.str.contains(r'\\\\x[0-9a-fA-F]{2}', regex=True, na=False)\n",
    "        backslash_mask = s.str.contains(r'\\\\\\\\', regex=True, na=False)\n",
    "\n",
    "        # Non-ASCII characters (after our pipeline these are suspicious)\n",
    "        non_ascii_mask = s.str.contains(r'[^\\x00-\\x7F]', regex=True, na=False)\n",
    "\n",
    "        col_mask = hex_mask | backslash_mask | non_ascii_mask\n",
    "        bad_mask = bad_mask | col_mask\n",
    "\n",
    "        print(f\"  Column '{col}': {col_mask.sum():,} suspicious values\")\n",
    "\n",
    "    bad_rows = df[bad_mask]\n",
    "    print(f\"\\n  Total rows with suspicious names: {len(bad_rows):,}\")\n",
    "\n",
    "    if len(bad_rows) == 0:\n",
    "        print(\"  No garbled names detected.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n  Sample suspicious rows (up to {n_samples}):\")\n",
    "    for _, row in bad_rows.head(n_samples).iterrows():\n",
    "        # Prefer normalized names for display, fall back to raw names\n",
    "        left = str(row.get('full_name_norm_L', row.get('full_name_L', '')))\n",
    "        right = str(row.get('full_name_norm_R', row.get('full_name_R', '')))\n",
    "        print(f\"    L: '{left}'  |  R: '{right}'\")\n",
    "\n",
    "inspect_garbled_names(cand_lr_nonid, \"cand_lr_nonid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e34c62d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR_nonid candidates: total=15879  non_id_eq=15879\n",
      "LS_nonid candidates: total=6965  non_id_eq=6965\n"
     ]
    }
   ],
   "source": [
    "# Quick counts for NON-ID candidates\n",
    "for name, df in [('LR_nonid', cand_lr_nonid), ('LS_nonid', cand_ls_nonid)]:\n",
    "    n = len(df)\n",
    "    nonid = (df['block_type'] == 'non_id_eq').sum() if 'block_type' in df.columns else 0\n",
    "    print(f'{name} candidates: total={n}  non_id_eq={nonid}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43cd738",
   "metadata": {},
   "source": [
    "## 5. Compute similarity scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9eab19",
   "metadata": {},
   "source": [
    "  ### 5.1 Define comparators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2ad274d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarity with PyDI RuleBasedMatcher (name comparators)\n",
    "# Note: All imports are in Section 0.4\n",
    "\n",
    "# Define comparators (name-only to avoid custom functions)\n",
    "name_comparators = [\n",
    "    StringComparator(\n",
    "        column=\"full_name_normalized\" if \"full_name_normalized\" in L.columns else \"full_name\",\n",
    "        similarity_function=\"levenshtein\",\n",
    "        preprocess=str.lower,\n",
    "    ),\n",
    "    StringComparator(\n",
    "        column=\"full_name_normalized\" if \"full_name_normalized\" in L.columns else \"full_name\",\n",
    "        similarity_function=\"jaccard\",\n",
    "        tokenization=\"word\",\n",
    "        preprocess=str.lower,\n",
    "    ),\n",
    "]\n",
    "name_weights = [0.7, 0.3]  # emphasize Levenshtein\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25666893",
   "metadata": {},
   "source": [
    "### 5.2 Score candidates with PyDI RuleBasedMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "96d617f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarity with PyDI (NON-ID channel, use pre-generated candidates; ignore team)\n",
    "# Note: Uses shared utility function merge_candidate_columns from Section 0.3\n",
    "\n",
    "# Reuse name_comparators and name_weights defined above\n",
    "\n",
    "def score_edge_with_pydi_nonid(candidates_df: pd.DataFrame, df_left: pd.DataFrame, df_right: pd.DataFrame, left_tag: str, right_tag: str) -> pd.DataFrame:\n",
    "    \"\"\"Score NON-ID candidates using RuleBasedMatcher on name-only comparators.\n",
    "    - Uses pre-generated candidate pairs (from build_nonid_candidates) to avoid duplicate blocking\n",
    "    - Candidates should have 'id1' and 'id2' columns matching _rid values\n",
    "    - Ignore team; no same_team column\n",
    "    \n",
    "    Uses shared utility function: merge_candidate_columns\n",
    "    \"\"\"\n",
    "    # Use pre-generated candidates instead of recreating blocker\n",
    "    # Ensure candidates_df has required columns\n",
    "    if 'id1' not in candidates_df.columns or 'id2' not in candidates_df.columns:\n",
    "        raise ValueError(\"candidates_df must have 'id1' and 'id2' columns\")\n",
    "    \n",
    "    matcher = RuleBasedMatcher()\n",
    "    correspondences = matcher.match(\n",
    "        df_left=df_left, df_right=df_right,\n",
    "        candidates=candidates_df[['id1', 'id2']],  # Use pre-generated candidate pairs\n",
    "        comparators=name_comparators, weights=name_weights,\n",
    "        threshold=0.0, id_column='_rid'\n",
    "    )\n",
    "\n",
    "    score_alias = next((col for col in ['sim','score','similarity'] if col in correspondences.columns), None)\n",
    "    if score_alias and score_alias != 'sim':\n",
    "        correspondences = correspondences.rename(columns={score_alias:'sim'})\n",
    "\n",
    "    # Use shared merge function\n",
    "    c = merge_candidate_columns(correspondences, df_left, df_right, left_tag, right_tag)\n",
    "\n",
    "    c = c.rename(columns={'sim':'similarity_score'})\n",
    "    if 'season_year' in c.columns:\n",
    "        c[f'season_year_{left_tag}'] = c['season_year']\n",
    "        c[f'season_year_{right_tag}'] = c['season_year']\n",
    "    c['block_type'] = 'non_id_eq'\n",
    "\n",
    "    keep = [\n",
    "        'id1' if 'id1' in c.columns else None,\n",
    "        'id2' if 'id2' in c.columns else None,\n",
    "        'similarity_score',\n",
    "        f'player_id_{left_tag}' if f'player_id_{left_tag}' in c.columns else None,\n",
    "        f'player_id_{right_tag}' if f'player_id_{right_tag}' in c.columns else None,\n",
    "        f'season_year_{left_tag}', f'season_year_{right_tag}',\n",
    "        f'full_name_{left_tag}' if f'full_name_{left_tag}' in c.columns else None,\n",
    "        f'full_name_{right_tag}' if f'full_name_{right_tag}' in c.columns else None,\n",
    "        f'birth_year_{left_tag}' if f'birth_year_{left_tag}' in c.columns else None,\n",
    "        f'birth_year_{right_tag}' if f'birth_year_{right_tag}' in c.columns else None,\n",
    "        'block_type'\n",
    "    ]\n",
    "    keep = [k for k in keep if k and k in c.columns]\n",
    "    return c[keep]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1584b97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NON-ID keys prepared on L/R/S: True True True\n"
     ]
    }
   ],
   "source": [
    "# Prepare NON-ID blocking keys on L/R/S for scoring (if missing)\n",
    "# Note: Uses shared utility function add_name_prefix from Section 0.3\n",
    "\n",
    "def ensure_nonid_keys(df: pd.DataFrame, tag: str) -> None:\n",
    "    \"\"\"Ensure NON-ID blocking keys exist on dataframe.\n",
    "    \n",
    "    Uses shared utility function: create_nonid_keys\n",
    "    If both keys already exist, this function does nothing (idempotent).\n",
    "    If either key is missing, creates both keys to ensure consistency.\n",
    "    \"\"\"\n",
    "    # Only create keys if either is missing (create both to ensure consistency)\n",
    "    if '_key_nonid' not in df.columns or '_rid' not in df.columns:\n",
    "        create_nonid_keys(df, tag, inplace=True)\n",
    "\n",
    "# Apply to three dataframes\n",
    "ensure_nonid_keys(L, 'L')\n",
    "ensure_nonid_keys(R, 'R')\n",
    "ensure_nonid_keys(S, 'S')\n",
    "\n",
    "print('NON-ID keys prepared on L/R/S:', '_key_nonid' in L.columns, '_key_nonid' in R.columns, '_key_nonid' in S.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346c7327",
   "metadata": {},
   "source": [
    "### 6.2 Cluster Consistency Analysis\n",
    "\n",
    "Analyze the cluster structure to identify any inconsistencies that our evaluation set may miss. The `EntityMatchingEvaluator` offers the `create_cluster_size_distribution` method for this purpose.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "085952bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO ] PyDI.entitymatching.rule_based.RuleBasedMatcher - Starting Entity Matching\n",
      "[INFO ] PyDI.entitymatching.rule_based.RuleBasedMatcher - Blocking 106553 x 15215 elements\n",
      "[INFO ] PyDI.entitymatching.rule_based.RuleBasedMatcher - Matching 106553 x 15215 elements after 0:00:0.126; 15879 blocked pairs (reduction ratio: 0.9999902054269367)\n",
      "[INFO ] PyDI.entitymatching.rule_based.RuleBasedMatcher - Entity Matching finished after 0:00:1.593; found 15879 correspondences.\n",
      "[INFO ] PyDI.entitymatching.rule_based.RuleBasedMatcher - Starting Entity Matching\n",
      "[INFO ] PyDI.entitymatching.rule_based.RuleBasedMatcher - Blocking 106553 x 6743 elements\n",
      "[INFO ] PyDI.entitymatching.rule_based.RuleBasedMatcher - Matching 106553 x 6743 elements after 0:00:0.091; 6965 blocked pairs (reduction ratio: 0.9999903060164304)\n",
      "[INFO ] PyDI.entitymatching.rule_based.RuleBasedMatcher - Entity Matching finished after 0:00:0.518; found 6965 correspondences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity scores computed (PyDI) and candidates sorted\n",
      "LR: similarity_score range = 0.19999999999999998 to 1.0\n",
      "LS: similarity_score range = 0.19999999999999998 to 1.0\n"
     ]
    }
   ],
   "source": [
    "# Score three edges with PyDI matcher (name comparators)\n",
    "# Use pre-generated candidate pairs to avoid duplicate blocking\n",
    "cand_lr = score_edge_with_pydi_nonid(cand_lr_nonid, L, R, 'L', 'R').sort_values('similarity_score', ascending=False).reset_index(drop=True)\n",
    "# cand_rs = score_edge_with_pydi_nonid(cand_rs_nonid, R, S, 'R', 'S').sort_values('similarity_score', ascending=False).reset_index(drop=True)\n",
    "cand_ls = score_edge_with_pydi_nonid(cand_ls_nonid, L, S, 'L', 'S').sort_values('similarity_score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print('Similarity scores computed (PyDI) and candidates sorted')\n",
    "print('LR: similarity_score range =', cand_lr['similarity_score'].min(), 'to', cand_lr['similarity_score'].max())\n",
    "# print('RS: similarity_score range =', cand_rs['similarity_score'].min(), 'to', cand_rs['similarity_score'].max())\n",
    "print('LS: similarity_score range =', cand_ls['similarity_score'].min(), 'to', cand_ls['similarity_score'].max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c45885c",
   "metadata": {},
   "source": [
    "  ### 5.3 Inspect similarity distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a1828814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LR similarity distribution:\n",
      "  mean: 0.979\n",
      "  median: 1.000\n",
      "  min: 0.200\n",
      "  max: 1.000\n",
      "  std: 0.101\n",
      "  percentiles:\n",
      "    10th: 1.000\n",
      "    25th: 1.000\n",
      "    50th: 1.000\n",
      "    75th: 1.000\n",
      "    90th: 1.000\n",
      "\n",
      "LS similarity distribution:\n",
      "  mean: 0.981\n",
      "  median: 1.000\n",
      "  min: 0.200\n",
      "  max: 1.000\n",
      "  std: 0.097\n",
      "  percentiles:\n",
      "    10th: 1.000\n",
      "    25th: 1.000\n",
      "    50th: 1.000\n",
      "    75th: 1.000\n",
      "    90th: 1.000\n"
     ]
    }
   ],
   "source": [
    "# Print similarity distribution statistics\n",
    "for name, df in [('LR', cand_lr), ('LS', cand_ls)]:\n",
    "    print(f'\\n{name} similarity distribution:')\n",
    "    print(f'  mean: {df[\"similarity_score\"].mean():.3f}')\n",
    "    print(f'  median: {df[\"similarity_score\"].median():.3f}')\n",
    "    print(f'  min: {df[\"similarity_score\"].min():.3f}')\n",
    "    print(f'  max: {df[\"similarity_score\"].max():.3f}')\n",
    "    print(f'  std: {df[\"similarity_score\"].std():.3f}')\n",
    "    \n",
    "    # Percentiles for stratified sampling\n",
    "    print('  percentiles:')\n",
    "    for p in [10, 25, 50, 75, 90]:\n",
    "        val = df['similarity_score'].quantile(p/100)\n",
    "        print(f'    {p}th: {val:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54806196",
   "metadata": {},
   "source": [
    "## 6. Stratified sampling for annotation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ec67aae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MANUAL_CASES_DIR = OUTPUT_DIR / 'gt' / 'manual_cases'\n",
    "MANUAL_CASES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MANUAL_ERROR_CASES = defaultdict(set)\n",
    "for edge in ['LR', 'LS']:\n",
    "    manual_path = MANUAL_CASES_DIR / f'manual_cases_{edge}.csv'\n",
    "    if manual_path.exists():\n",
    "        manual_df = pd.read_csv(manual_path)\n",
    "        if {'id1', 'id2'}.issubset(manual_df.columns):\n",
    "            MANUAL_ERROR_CASES[edge] = set(zip(manual_df['id1'], manual_df['id2']))\n",
    "\n",
    "\n",
    "def _strip_accents(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    normalized = unicodedata.normalize('NFD', text)\n",
    "    return ''.join(ch for ch in normalized if unicodedata.category(ch) != 'Mn')\n",
    "\n",
    "\n",
    "def _has_accent_or_suffix_variant(row: pd.Series, left_tag: str, right_tag: str) -> bool:\n",
    "    left = str(row.get(f'full_name_{left_tag}', '')).lower().strip()\n",
    "    right = str(row.get(f'full_name_{right_tag}', '')).lower().strip()\n",
    "    if not left or not right:\n",
    "        return False\n",
    "    accent_left = _strip_accents(left)\n",
    "    accent_right = _strip_accents(right)\n",
    "    if accent_left and accent_left == accent_right and left != right:\n",
    "        return True\n",
    "    suffix_tokens = {'jr', 'sr', 'ii', 'iii', 'iv', 'v'}\n",
    "    left_suffix = any(left.endswith(f' {s}') for s in suffix_tokens)\n",
    "    right_suffix = any(right.endswith(f' {s}') for s in suffix_tokens)\n",
    "    if left_suffix != right_suffix:\n",
    "        def _strip_suffix(text: str) -> str:\n",
    "            parts = [tok for tok in text.split() if tok not in suffix_tokens]\n",
    "            return ' '.join(parts)\n",
    "        base_left = _strip_accents(_strip_suffix(left))\n",
    "        base_right = _strip_accents(_strip_suffix(right))\n",
    "        return base_left == base_right\n",
    "    return False\n",
    "\n",
    "\n",
    "def _has_birth_conflict(row: pd.Series, left_tag: str, right_tag: str, threshold: int = 2) -> bool:\n",
    "    by_l = pd.to_numeric(row.get(f'birth_year_{left_tag}'), errors='coerce')\n",
    "    by_r = pd.to_numeric(row.get(f'birth_year_{right_tag}'), errors='coerce')\n",
    "    if pd.isna(by_l) or pd.isna(by_r):\n",
    "        return False\n",
    "    return abs(by_l - by_r) >= threshold\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfa73c5",
   "metadata": {},
   "source": [
    "### 6.1 Build stratified samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6a68a471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: saved 500 rows to /Users/zhangzihan/Desktop/WBI_project/Schema_Mapped_Datasets/data/output/gt/samples/samples_LR_v1.csv\n",
      "LS: saved 500 rows to /Users/zhangzihan/Desktop/WBI_project/Schema_Mapped_Datasets/data/output/gt/samples/samples_LS_v1.csv\n",
      "\n",
      "LR sample preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id1</th>\n",
       "      <th>id2</th>\n",
       "      <th>similarity_score</th>\n",
       "      <th>full_name_L</th>\n",
       "      <th>full_name_R</th>\n",
       "      <th>season_year_L</th>\n",
       "      <th>season_year_R</th>\n",
       "      <th>birth_year_L</th>\n",
       "      <th>birth_year_R</th>\n",
       "      <th>label</th>\n",
       "      <th>source_channel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4613142010|2010|L</td>\n",
       "      <td>4613142010|2010|R</td>\n",
       "      <td>1.0</td>\n",
       "      <td>matt kemp</td>\n",
       "      <td>matt kemp</td>\n",
       "      <td>2010</td>\n",
       "      <td>2010</td>\n",
       "      <td>1984</td>\n",
       "      <td>1985</td>\n",
       "      <td></td>\n",
       "      <td>non_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3467982015|2015|L</td>\n",
       "      <td>3467982015|2015|R</td>\n",
       "      <td>1.0</td>\n",
       "      <td>kyle lohse</td>\n",
       "      <td>kyle lohse</td>\n",
       "      <td>2015</td>\n",
       "      <td>2015</td>\n",
       "      <td>1978</td>\n",
       "      <td>1979</td>\n",
       "      <td></td>\n",
       "      <td>non_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6135642021|2021|L</td>\n",
       "      <td>6135642021|2021|R</td>\n",
       "      <td>1.0</td>\n",
       "      <td>jason vosler</td>\n",
       "      <td>jason vosler</td>\n",
       "      <td>2021</td>\n",
       "      <td>2021</td>\n",
       "      <td>1993</td>\n",
       "      <td>1994</td>\n",
       "      <td></td>\n",
       "      <td>non_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5379532012|2012|L</td>\n",
       "      <td>5379532012|2012|R</td>\n",
       "      <td>1.0</td>\n",
       "      <td>daniel nava</td>\n",
       "      <td>daniel nava</td>\n",
       "      <td>2012</td>\n",
       "      <td>2012</td>\n",
       "      <td>1983</td>\n",
       "      <td>1983</td>\n",
       "      <td></td>\n",
       "      <td>non_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6053232015|2015|L</td>\n",
       "      <td>6053232015|2015|R</td>\n",
       "      <td>1.0</td>\n",
       "      <td>kyle kubitza</td>\n",
       "      <td>kyle kubitza</td>\n",
       "      <td>2015</td>\n",
       "      <td>2015</td>\n",
       "      <td>1990</td>\n",
       "      <td>1991</td>\n",
       "      <td></td>\n",
       "      <td>non_id</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id1                id2  similarity_score   full_name_L  \\\n",
       "0  4613142010|2010|L  4613142010|2010|R               1.0     matt kemp   \n",
       "1  3467982015|2015|L  3467982015|2015|R               1.0    kyle lohse   \n",
       "2  6135642021|2021|L  6135642021|2021|R               1.0  jason vosler   \n",
       "3  5379532012|2012|L  5379532012|2012|R               1.0   daniel nava   \n",
       "4  6053232015|2015|L  6053232015|2015|R               1.0  kyle kubitza   \n",
       "\n",
       "    full_name_R  season_year_L  season_year_R birth_year_L birth_year_R label  \\\n",
       "0     matt kemp           2010           2010         1984         1985         \n",
       "1    kyle lohse           2015           2015         1978         1979         \n",
       "2  jason vosler           2021           2021         1993         1994         \n",
       "3   daniel nava           2012           2012         1983         1983         \n",
       "4  kyle kubitza           2015           2015         1990         1991         \n",
       "\n",
       "  source_channel  \n",
       "0         non_id  \n",
       "1         non_id  \n",
       "2         non_id  \n",
       "3         non_id  \n",
       "4         non_id  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LS sample preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id1</th>\n",
       "      <th>id2</th>\n",
       "      <th>similarity_score</th>\n",
       "      <th>full_name_L</th>\n",
       "      <th>full_name_S</th>\n",
       "      <th>season_year_L</th>\n",
       "      <th>season_year_S</th>\n",
       "      <th>birth_year_L</th>\n",
       "      <th>birth_year_S</th>\n",
       "      <th>label</th>\n",
       "      <th>source_channel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4346582019|2019|L</td>\n",
       "      <td>4346582019|2019|S</td>\n",
       "      <td>1.0</td>\n",
       "      <td>rajai davis</td>\n",
       "      <td>rajai davis</td>\n",
       "      <td>2019</td>\n",
       "      <td>2019</td>\n",
       "      <td>1980</td>\n",
       "      <td>1981</td>\n",
       "      <td></td>\n",
       "      <td>non_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5423642022|2022|L</td>\n",
       "      <td>5423642022|2022|S</td>\n",
       "      <td>1.0</td>\n",
       "      <td>rafael ortega</td>\n",
       "      <td>rafael ortega</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022</td>\n",
       "      <td>1991</td>\n",
       "      <td>1991</td>\n",
       "      <td></td>\n",
       "      <td>non_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6083282021|2021|L</td>\n",
       "      <td>6083282021|2021|S</td>\n",
       "      <td>1.0</td>\n",
       "      <td>chase de jong</td>\n",
       "      <td>chase de jong</td>\n",
       "      <td>2021</td>\n",
       "      <td>2021</td>\n",
       "      <td>1993</td>\n",
       "      <td>1994</td>\n",
       "      <td></td>\n",
       "      <td>non_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5785702023|2023|L</td>\n",
       "      <td>5785702023|2023|S</td>\n",
       "      <td>1.0</td>\n",
       "      <td>juniel querecuto</td>\n",
       "      <td>juniel querecuto</td>\n",
       "      <td>2023</td>\n",
       "      <td>2023</td>\n",
       "      <td>1992</td>\n",
       "      <td>1993</td>\n",
       "      <td></td>\n",
       "      <td>non_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5431482017|2017|L</td>\n",
       "      <td>5431482017|2017|S</td>\n",
       "      <td>1.0</td>\n",
       "      <td>tim federowicz</td>\n",
       "      <td>tim federowicz</td>\n",
       "      <td>2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>1987</td>\n",
       "      <td>1988</td>\n",
       "      <td></td>\n",
       "      <td>non_id</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id1                id2  similarity_score       full_name_L  \\\n",
       "0  4346582019|2019|L  4346582019|2019|S               1.0       rajai davis   \n",
       "1  5423642022|2022|L  5423642022|2022|S               1.0     rafael ortega   \n",
       "2  6083282021|2021|L  6083282021|2021|S               1.0     chase de jong   \n",
       "3  5785702023|2023|L  5785702023|2023|S               1.0  juniel querecuto   \n",
       "4  5431482017|2017|L  5431482017|2017|S               1.0    tim federowicz   \n",
       "\n",
       "        full_name_S  season_year_L  season_year_S birth_year_L birth_year_S  \\\n",
       "0       rajai davis           2019           2019         1980         1981   \n",
       "1     rafael ortega           2022           2022         1991         1991   \n",
       "2     chase de jong           2021           2021         1993         1994   \n",
       "3  juniel querecuto           2023           2023         1992         1993   \n",
       "4    tim federowicz           2017           2017         1987         1988   \n",
       "\n",
       "  label source_channel  \n",
       "0               non_id  \n",
       "1               non_id  \n",
       "2               non_id  \n",
       "3               non_id  \n",
       "4               non_id  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build stratified samples for annotation (score-threshold bucketing)\n",
    "\n",
    "SAMPLE_OUT_DIR = OUTPUT_DIR / 'gt' / 'samples'\n",
    "SAMPLE_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def ensure_ids_for_annotation(df: pd.DataFrame, edge: str) -> pd.DataFrame:\n",
    "    left_tag, right_tag = edge[0], edge[1]\n",
    "    c = df.copy()\n",
    "    # If id1/id2 already exist, keep them\n",
    "    if 'id1' in c.columns and 'id2' in c.columns:\n",
    "        return c\n",
    "    # Build synthetic IDs from available columns (full_name + season_year), fallback to index\n",
    "    l_name = f'full_name_{left_tag}' if f'full_name_{left_tag}' in c.columns else None\n",
    "    r_name = f'full_name_{right_tag}' if f'full_name_{right_tag}' in c.columns else None\n",
    "    l_year_col = f'season_year_{left_tag}'\n",
    "    r_year_col = f'season_year_{right_tag}'\n",
    "    l_part = (c[l_name].astype(str) if l_name else c.index.astype(str))\n",
    "    r_part = (c[r_name].astype(str) if r_name else c.index.astype(str))\n",
    "    c['id1'] = l_part.str.strip().str.lower() + '|' + pd.to_numeric(c[l_year_col], errors='coerce').astype('Int64').astype(str) + f'|{left_tag}'\n",
    "    c['id2'] = r_part.str.strip().str.lower() + '|' + pd.to_numeric(c[r_year_col], errors='coerce').astype('Int64').astype(str) + f'|{right_tag}'\n",
    "    return c\n",
    "\n",
    "\n",
    "def stratified_sample_qcut(cand: pd.DataFrame, edge: str, n_total: int = 500,\n",
    "                           high_ratio: float = 0.20, corner_ratio: float = 0.30,\n",
    "                           seed: int = 42) -> pd.DataFrame:\n",
    "    df = ensure_ids_for_annotation(cand, edge)\n",
    "    # Score-threshold bucketing: low < 0.60, mid in [0.60, 0.99), high >= 0.99\n",
    "    scores = pd.to_numeric(df['similarity_score'], errors='coerce')\n",
    "    df['bucket'] = pd.cut(scores, bins=[-1, 0.60, 0.99, 1.01], labels=['low','mid','high'], include_lowest=True)\n",
    "    high = df[df['bucket']=='high']\n",
    "    mid  = df[df['bucket']=='mid']\n",
    "    low  = df[df['bucket']=='low']\n",
    "\n",
    "    # corner: boundary + hard positives + hard negatives (prefer sampling from mid bucket; backfill from all if insufficient)\n",
    "    l_tag, r_tag = edge[0], edge[1]\n",
    "    by_l, by_r = f'birth_year_{l_tag}', f'birth_year_{r_tag}'\n",
    "    sy_l, sy_r = f'season_year_{l_tag}', f'season_year_{r_tag}'\n",
    "    pid_l, pid_r = f'player_id_{l_tag}', f'player_id_{r_tag}'\n",
    "\n",
    "    # 1) Boundary samples: near-threshold ambiguous region (adjust the windows if needed)\n",
    "    boundary = df[\n",
    "        scores.between(0.58, 0.62, inclusive='both') |\n",
    "        scores.between(0.98, 0.99, inclusive='left')\n",
    "    ]\n",
    "\n",
    "    # 2) Hard positives: same player_id but conflicting attributes and/or low similarity\n",
    "    same_pid = False\n",
    "    if (pid_l in df.columns) and (pid_r in df.columns):\n",
    "        same_pid = (df[pid_l].astype(str).str.lower() == df[pid_r].astype(str).str.lower())\n",
    "    birth_diff = None\n",
    "    if (by_l in df.columns) and (by_r in df.columns):\n",
    "        birth_diff = (pd.to_numeric(df[by_l], errors='coerce') - pd.to_numeric(df[by_r], errors='coerce')).abs().gt(0)\n",
    "    hard_pos = df[same_pid] if isinstance(same_pid, pd.Series) else df.head(0)\n",
    "    if len(hard_pos) > 0:\n",
    "        idx = hard_pos.index\n",
    "        cond_low = scores.loc[idx] < 0.80\n",
    "        cond_birth = birth_diff.loc[idx] if isinstance(birth_diff, pd.Series) else pd.Series(False, index=idx)\n",
    "        hard_pos = hard_pos[cond_low | cond_birth.fillna(False)]\n",
    "\n",
    "    # 3) Hard negatives: different player_id but highly similar attributes (same season and birth year) with high similarity\n",
    "    diff_pid = False\n",
    "    if (pid_l in df.columns) and (pid_r in df.columns):\n",
    "        diff_pid = (df[pid_l].astype(str).str.lower() != df[pid_r].astype(str).str.lower())\n",
    "    same_season = False\n",
    "    if (sy_l in df.columns) and (sy_r in df.columns):\n",
    "        same_season = (pd.to_numeric(df[sy_l], errors='coerce') == pd.to_numeric(df[sy_r], errors='coerce'))\n",
    "    birth_eq = False\n",
    "    if (by_l in df.columns) and (by_r in df.columns):\n",
    "        birth_eq = (pd.to_numeric(df[by_l], errors='coerce') == pd.to_numeric(df[by_r], errors='coerce'))\n",
    "    mask_hn = (diff_pid if isinstance(diff_pid, pd.Series) else False)\n",
    "    if isinstance(mask_hn, pd.Series):\n",
    "        if isinstance(same_season, pd.Series):\n",
    "            mask_hn = mask_hn & same_season\n",
    "        if isinstance(birth_eq, pd.Series):\n",
    "            mask_hn = mask_hn & birth_eq\n",
    "        mask_hn = mask_hn & scores.ge(0.95)\n",
    "        hard_neg = df[mask_hn]\n",
    "    else:\n",
    "        hard_neg = df.head(0)\n",
    "\n",
    "    # Merge corner candidates; prefer mid bucket; backfill from all candidates if needed\n",
    "    corner_all = pd.concat([boundary, hard_pos, hard_neg], axis=0).drop_duplicates()\n",
    "\n",
    "    # Pattern boosters: manual error pairs, accent/suffix variants, and large birth-year conflicts\n",
    "    pattern_mask = None\n",
    "    manual_pairs = MANUAL_ERROR_CASES.get(edge, set())\n",
    "    if manual_pairs:\n",
    "        manual_mask = df.apply(lambda r: (r['id1'], r['id2']) in manual_pairs, axis=1)\n",
    "        pattern_mask = manual_mask if pattern_mask is None else (pattern_mask | manual_mask)\n",
    "\n",
    "    accent_mask = df.apply(lambda r: _has_accent_or_suffix_variant(r, l_tag, r_tag), axis=1)\n",
    "    birth_conflict_mask = df.apply(lambda r: _has_birth_conflict(r, l_tag, r_tag), axis=1)\n",
    "    for mask in [accent_mask, birth_conflict_mask]:\n",
    "        if isinstance(mask, pd.Series):\n",
    "            pattern_mask = mask if pattern_mask is None else (pattern_mask | mask)\n",
    "\n",
    "    if pattern_mask is not None:\n",
    "        pattern_cases = df[pattern_mask]\n",
    "        if len(pattern_cases) > 0:\n",
    "            corner_all = pd.concat([corner_all, pattern_cases], axis=0).drop_duplicates()\n",
    "\n",
    "    corner_mid = corner_all[corner_all.index.isin(mid.index)]\n",
    "    corner = corner_mid\n",
    "    need = int(n_total*corner_ratio) - len(corner)\n",
    "    if need > 0:\n",
    "        extra = corner_all[~corner_all.index.isin(corner.index)]\n",
    "        corner = pd.concat([corner, extra.head(max(0, need))])\n",
    "\n",
    "    n_high = min(int(n_total*high_ratio), len(high))\n",
    "    n_corner = min(int(n_total*corner_ratio), len(corner))\n",
    "    n_low = min(n_total - n_high - n_corner, len(low))\n",
    "\n",
    "    sampled = pd.concat([\n",
    "        high.sample(n=n_high, random_state=seed) if n_high>0 else high.head(0),\n",
    "        corner.sample(n=n_corner, random_state=seed) if n_corner>0 else corner.head(0),\n",
    "        low.sample(n=n_low, random_state=seed) if n_low>0 else low.head(0),\n",
    "    ], ignore_index=True)\n",
    "\n",
    "       # Backfill if total < n_total: prefer mid -> high -> low from remaining candidates\n",
    "    need = n_total - len(sampled)\n",
    "    if need > 0:\n",
    "        # Build a key to avoid duplicates (id1,id2)\n",
    "        chosen_keys = set(zip(sampled['id1'], sampled['id2']))\n",
    "        remaining = df[~list(zip(df['id1'], df['id2'])) \\\n",
    "                          .__iter__().__class__(zip(df['id1'], df['id2'])) \\\n",
    "                      ] if False else df  # placeholder; see below\n",
    "\n",
    "        # Robust way to filter remaining by keys without heavy memory:\n",
    "        rem_mask = ~df.apply(lambda r: (r['id1'], r['id2']) in chosen_keys, axis=1)\n",
    "        remaining = df[rem_mask]\n",
    "\n",
    "        # Priority pools\n",
    "        pools = [\n",
    "            remaining[remaining['bucket'] == 'mid'],\n",
    "            remaining[remaining['bucket'] == 'high'],\n",
    "            remaining[remaining['bucket'] == 'low'],\n",
    "        ]\n",
    "        for pool in pools:\n",
    "            if need <= 0 or len(pool) == 0:\n",
    "                continue\n",
    "            take = min(need, len(pool))\n",
    "            add = pool.sample(n=take, random_state=seed)\n",
    "            sampled = pd.concat([sampled, add], ignore_index=True)\n",
    "            # update chosen set to keep uniqueness in later loops\n",
    "            for t in zip(add['id1'], add['id2']):\n",
    "                chosen_keys.add(t)\n",
    "            need -= take\n",
    "\n",
    "    cols = [\n",
    "        'id1','id2','similarity_score',\n",
    "        f'full_name_{edge[0]}' if f'full_name_{edge[0]}' in sampled.columns else None,\n",
    "        f'full_name_{edge[1]}' if f'full_name_{edge[1]}' in sampled.columns else None,\n",
    "        f'season_year_{edge[0]}', f'season_year_{edge[1]}',\n",
    "        f'birth_year_{edge[0]}' if f'birth_year_{edge[0]}' in sampled.columns else None,\n",
    "        f'birth_year_{edge[1]}' if f'birth_year_{edge[1]}' in sampled.columns else None,\n",
    "    ]\n",
    "    cols = [c for c in cols if c and c in sampled.columns]\n",
    "    df_out = sampled[cols].copy()\n",
    "    # Prepare empty label column for manual annotation\n",
    "    df_out['label'] = ''\n",
    "    # Mark candidate source channel for evaluation stratification\n",
    "    df_out['source_channel'] = 'non_id'\n",
    "    return df_out\n",
    "\n",
    "\n",
    "# Apply to three edges and save (NON-ID channel, use scored candidates with similarity_score)\n",
    "edges = {\n",
    "    'LR': ('cand_lr', 'L', 'R'),\n",
    "    #'RS': ('cand_rs', 'R', 'S'),\n",
    "    'LS': ('cand_ls', 'L', 'S'),\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for edge_name, (var_name, lt, rt) in edges.items():\n",
    "    cand_df = globals()[var_name]\n",
    "    sampled = stratified_sample_qcut(cand_df, edge=f'{lt}{rt}', n_total=500)\n",
    "    results[edge_name] = sampled\n",
    "    out_path = SAMPLE_OUT_DIR / f'samples_{edge_name}_v1.csv'\n",
    "    sampled.to_csv(out_path, index=False)\n",
    "    print(f'{edge_name}: saved {len(sampled)} rows to', out_path)\n",
    "\n",
    "for edge_name, dfp in results.items():\n",
    "    print(f\"\\n{edge_name} sample preview:\")\n",
    "    display(dfp.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "46bce8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LR HIGH (top 10) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id1</th>\n",
       "      <th>id2</th>\n",
       "      <th>similarity_score</th>\n",
       "      <th>full_name_L</th>\n",
       "      <th>full_name_R</th>\n",
       "      <th>season_year_L</th>\n",
       "      <th>season_year_R</th>\n",
       "      <th>birth_year_L</th>\n",
       "      <th>birth_year_R</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1100292008|2008|L</td>\n",
       "      <td>1100292008|2008|R</td>\n",
       "      <td>1.0</td>\n",
       "      <td>bobby abreu</td>\n",
       "      <td>bobby abreu</td>\n",
       "      <td>2008</td>\n",
       "      <td>2008</td>\n",
       "      <td>1974</td>\n",
       "      <td>1974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10129</th>\n",
       "      <td>1504392008|2008|L</td>\n",
       "      <td>1504392008|2008|R</td>\n",
       "      <td>1.0</td>\n",
       "      <td>gary matthews</td>\n",
       "      <td>gary matthews</td>\n",
       "      <td>2008</td>\n",
       "      <td>2008</td>\n",
       "      <td>1974</td>\n",
       "      <td>1975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10117</th>\n",
       "      <td>1504152008|2008|L</td>\n",
       "      <td>1504152008|2008|R</td>\n",
       "      <td>1.0</td>\n",
       "      <td>brad wilkerson</td>\n",
       "      <td>brad wilkerson</td>\n",
       "      <td>2008</td>\n",
       "      <td>2008</td>\n",
       "      <td>1977</td>\n",
       "      <td>1977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10118</th>\n",
       "      <td>1504162008|2008|L</td>\n",
       "      <td>1504162008|2008|R</td>\n",
       "      <td>1.0</td>\n",
       "      <td>danny ardoin</td>\n",
       "      <td>danny ardoin</td>\n",
       "      <td>2008</td>\n",
       "      <td>2008</td>\n",
       "      <td>1974</td>\n",
       "      <td>1975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10119</th>\n",
       "      <td>1504212008|2008|L</td>\n",
       "      <td>1504212008|2008|R</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ramon hernandez</td>\n",
       "      <td>ram\\xc3\\xb3n hern\\xc3\\xa1ndez</td>\n",
       "      <td>2008</td>\n",
       "      <td>2008</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id1                id2  similarity_score  \\\n",
       "0      1100292008|2008|L  1100292008|2008|R               1.0   \n",
       "10129  1504392008|2008|L  1504392008|2008|R               1.0   \n",
       "10117  1504152008|2008|L  1504152008|2008|R               1.0   \n",
       "10118  1504162008|2008|L  1504162008|2008|R               1.0   \n",
       "10119  1504212008|2008|L  1504212008|2008|R               1.0   \n",
       "\n",
       "           full_name_L                    full_name_R  season_year_L  \\\n",
       "0          bobby abreu                    bobby abreu           2008   \n",
       "10129    gary matthews                  gary matthews           2008   \n",
       "10117   brad wilkerson                 brad wilkerson           2008   \n",
       "10118     danny ardoin                   danny ardoin           2008   \n",
       "10119  ramon hernandez  ram\\xc3\\xb3n hern\\xc3\\xa1ndez           2008   \n",
       "\n",
       "       season_year_R birth_year_L birth_year_R  \n",
       "0               2008         1974         1974  \n",
       "10129           2008         1974         1975  \n",
       "10117           2008         1977         1977  \n",
       "10118           2008         1974         1975  \n",
       "10119           2008         1976         1976  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LR MID (10 random) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id1</th>\n",
       "      <th>id2</th>\n",
       "      <th>similarity_score</th>\n",
       "      <th>full_name_L</th>\n",
       "      <th>full_name_R</th>\n",
       "      <th>season_year_L</th>\n",
       "      <th>season_year_R</th>\n",
       "      <th>birth_year_L</th>\n",
       "      <th>birth_year_R</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15241</th>\n",
       "      <td>5759292021|2021|L</td>\n",
       "      <td>6613882021|2021|R</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>willson contreras</td>\n",
       "      <td>william contreras</td>\n",
       "      <td>2021</td>\n",
       "      <td>2021</td>\n",
       "      <td>1992</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15291</th>\n",
       "      <td>1120202010|2010|L</td>\n",
       "      <td>4520802010|2010|R</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>chris carpenter</td>\n",
       "      <td>chris carter</td>\n",
       "      <td>2010</td>\n",
       "      <td>2010</td>\n",
       "      <td>1975</td>\n",
       "      <td>1983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15294</th>\n",
       "      <td>1120202010|2010|L</td>\n",
       "      <td>4748922010|2010|R</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>chris carpenter</td>\n",
       "      <td>chris carter</td>\n",
       "      <td>2010</td>\n",
       "      <td>2010</td>\n",
       "      <td>1975</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15280</th>\n",
       "      <td>2361552011|2011|L</td>\n",
       "      <td>4296642011|2011|R</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>robinson cancel</td>\n",
       "      <td>robinson can\\xc3\\xb3</td>\n",
       "      <td>2011</td>\n",
       "      <td>2011</td>\n",
       "      <td>1976</td>\n",
       "      <td>1983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15328</th>\n",
       "      <td>5925272016|2016|L</td>\n",
       "      <td>4928412016|2016|R</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>michael mariot</td>\n",
       "      <td>michael mart\\xc3\\xadnez</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016</td>\n",
       "      <td>1988</td>\n",
       "      <td>1983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id1                id2  similarity_score  \\\n",
       "15241  5759292021|2021|L  6613882021|2021|R          0.676471   \n",
       "15291  1120202010|2010|L  4520802010|2010|R          0.660000   \n",
       "15294  1120202010|2010|L  4748922010|2010|R          0.660000   \n",
       "15280  2361552011|2011|L  4296642011|2011|R          0.660000   \n",
       "15328  5925272016|2016|L  4928412016|2016|R          0.625000   \n",
       "\n",
       "             full_name_L              full_name_R  season_year_L  \\\n",
       "15241  willson contreras        william contreras           2021   \n",
       "15291    chris carpenter             chris carter           2010   \n",
       "15294    chris carpenter             chris carter           2010   \n",
       "15280    robinson cancel     robinson can\\xc3\\xb3           2011   \n",
       "15328     michael mariot  michael mart\\xc3\\xadnez           2016   \n",
       "\n",
       "       season_year_R birth_year_L birth_year_R  \n",
       "15241           2021         1992         1998  \n",
       "15291           2010         1975         1983  \n",
       "15294           2010         1975         1987  \n",
       "15280           2011         1976         1983  \n",
       "15328           2016         1988         1983  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LR LOW (bottom 10) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id1</th>\n",
       "      <th>id2</th>\n",
       "      <th>similarity_score</th>\n",
       "      <th>full_name_L</th>\n",
       "      <th>full_name_R</th>\n",
       "      <th>season_year_L</th>\n",
       "      <th>season_year_R</th>\n",
       "      <th>birth_year_L</th>\n",
       "      <th>birth_year_R</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15878</th>\n",
       "      <td>6723862024|2024|L</td>\n",
       "      <td>6661352024|2024|R</td>\n",
       "      <td>0.2</td>\n",
       "      <td>alejandro kirk</td>\n",
       "      <td>alex kirilloff</td>\n",
       "      <td>2024</td>\n",
       "      <td>2024</td>\n",
       "      <td>1998</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15871</th>\n",
       "      <td>6661352024|2024|L</td>\n",
       "      <td>6723862024|2024|R</td>\n",
       "      <td>0.2</td>\n",
       "      <td>alex kirilloff</td>\n",
       "      <td>alejandro kirk</td>\n",
       "      <td>2024</td>\n",
       "      <td>2024</td>\n",
       "      <td>1997</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15872</th>\n",
       "      <td>6723862023|2023|L</td>\n",
       "      <td>6661352023|2023|R</td>\n",
       "      <td>0.2</td>\n",
       "      <td>alejandro kirk</td>\n",
       "      <td>alex kirilloff</td>\n",
       "      <td>2023</td>\n",
       "      <td>2023</td>\n",
       "      <td>1998</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15873</th>\n",
       "      <td>6661352023|2023|L</td>\n",
       "      <td>6723862023|2023|R</td>\n",
       "      <td>0.2</td>\n",
       "      <td>alex kirilloff</td>\n",
       "      <td>alejandro kirk</td>\n",
       "      <td>2023</td>\n",
       "      <td>2023</td>\n",
       "      <td>1997</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15877</th>\n",
       "      <td>6661352021|2021|L</td>\n",
       "      <td>6723862021|2021|R</td>\n",
       "      <td>0.2</td>\n",
       "      <td>alex kirilloff</td>\n",
       "      <td>alejandro kirk</td>\n",
       "      <td>2021</td>\n",
       "      <td>2021</td>\n",
       "      <td>1997</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id1                id2  similarity_score     full_name_L  \\\n",
       "15878  6723862024|2024|L  6661352024|2024|R               0.2  alejandro kirk   \n",
       "15871  6661352024|2024|L  6723862024|2024|R               0.2  alex kirilloff   \n",
       "15872  6723862023|2023|L  6661352023|2023|R               0.2  alejandro kirk   \n",
       "15873  6661352023|2023|L  6723862023|2023|R               0.2  alex kirilloff   \n",
       "15877  6661352021|2021|L  6723862021|2021|R               0.2  alex kirilloff   \n",
       "\n",
       "          full_name_R  season_year_L  season_year_R birth_year_L birth_year_R  \n",
       "15878  alex kirilloff           2024           2024         1998         1998  \n",
       "15871  alejandro kirk           2024           2024         1997         1999  \n",
       "15872  alex kirilloff           2023           2023         1998         1998  \n",
       "15873  alejandro kirk           2023           2023         1997         1999  \n",
       "15877  alejandro kirk           2021           2021         1997         1999  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LS HIGH (top 10) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id1</th>\n",
       "      <th>id2</th>\n",
       "      <th>similarity_score</th>\n",
       "      <th>full_name_L</th>\n",
       "      <th>full_name_S</th>\n",
       "      <th>season_year_L</th>\n",
       "      <th>season_year_S</th>\n",
       "      <th>birth_year_L</th>\n",
       "      <th>birth_year_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1125262015|2015|L</td>\n",
       "      <td>1125262015|2015|S</td>\n",
       "      <td>1.0</td>\n",
       "      <td>bartolo colon</td>\n",
       "      <td>bartolo colon</td>\n",
       "      <td>2015</td>\n",
       "      <td>2015</td>\n",
       "      <td>1973</td>\n",
       "      <td>1973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4385</th>\n",
       "      <td>4448762017|2017|L</td>\n",
       "      <td>4448762017|2017|S</td>\n",
       "      <td>1.0</td>\n",
       "      <td>alcides escobar</td>\n",
       "      <td>alcides escobar</td>\n",
       "      <td>2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>1986</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4461</th>\n",
       "      <td>4444822015|2015|L</td>\n",
       "      <td>4444822015|2015|S</td>\n",
       "      <td>1.0</td>\n",
       "      <td>david peralta</td>\n",
       "      <td>david peralta</td>\n",
       "      <td>2015</td>\n",
       "      <td>2015</td>\n",
       "      <td>1987</td>\n",
       "      <td>1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4460</th>\n",
       "      <td>4444822016|2016|L</td>\n",
       "      <td>4444822016|2016|S</td>\n",
       "      <td>1.0</td>\n",
       "      <td>david peralta</td>\n",
       "      <td>david peralta</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016</td>\n",
       "      <td>1987</td>\n",
       "      <td>1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4459</th>\n",
       "      <td>4444822017|2017|L</td>\n",
       "      <td>4444822017|2017|S</td>\n",
       "      <td>1.0</td>\n",
       "      <td>david peralta</td>\n",
       "      <td>david peralta</td>\n",
       "      <td>2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>1987</td>\n",
       "      <td>1988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id1                id2  similarity_score      full_name_L  \\\n",
       "0     1125262015|2015|L  1125262015|2015|S               1.0    bartolo colon   \n",
       "4385  4448762017|2017|L  4448762017|2017|S               1.0  alcides escobar   \n",
       "4461  4444822015|2015|L  4444822015|2015|S               1.0    david peralta   \n",
       "4460  4444822016|2016|L  4444822016|2016|S               1.0    david peralta   \n",
       "4459  4444822017|2017|L  4444822017|2017|S               1.0    david peralta   \n",
       "\n",
       "          full_name_S  season_year_L  season_year_S birth_year_L birth_year_S  \n",
       "0       bartolo colon           2015           2015         1973         1973  \n",
       "4385  alcides escobar           2017           2017         1986         1987  \n",
       "4461    david peralta           2015           2015         1987         1988  \n",
       "4460    david peralta           2016           2016         1987         1988  \n",
       "4459    david peralta           2017           2017         1987         1988  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LS MID (10 random) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id1</th>\n",
       "      <th>id2</th>\n",
       "      <th>similarity_score</th>\n",
       "      <th>full_name_L</th>\n",
       "      <th>full_name_S</th>\n",
       "      <th>season_year_L</th>\n",
       "      <th>season_year_S</th>\n",
       "      <th>birth_year_L</th>\n",
       "      <th>birth_year_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6681</th>\n",
       "      <td>5471702019|2019|L</td>\n",
       "      <td>5471702019|2019|S</td>\n",
       "      <td>0.753333</td>\n",
       "      <td>nick delmonico</td>\n",
       "      <td>nicky delmonico</td>\n",
       "      <td>2019</td>\n",
       "      <td>2019</td>\n",
       "      <td>1992</td>\n",
       "      <td>1993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6693</th>\n",
       "      <td>6689422022|2022|L</td>\n",
       "      <td>6703512022|2022|S</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>josh rojas</td>\n",
       "      <td>jose rojas</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022</td>\n",
       "      <td>1994</td>\n",
       "      <td>1993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6703</th>\n",
       "      <td>5948382019|2019|L</td>\n",
       "      <td>5948382019|2019|S</td>\n",
       "      <td>0.706667</td>\n",
       "      <td>philip gosselin</td>\n",
       "      <td>phil gosselin</td>\n",
       "      <td>2019</td>\n",
       "      <td>2019</td>\n",
       "      <td>1988</td>\n",
       "      <td>1989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6756</th>\n",
       "      <td>6647472022|2022|L</td>\n",
       "      <td>6495572022|2022|S</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>alexis diaz</td>\n",
       "      <td>aledmys diaz</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022</td>\n",
       "      <td>1996</td>\n",
       "      <td>1991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6751</th>\n",
       "      <td>6072572017|2017|L</td>\n",
       "      <td>6072572017|2017|S</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>rafael lopez</td>\n",
       "      <td>raffy lopez</td>\n",
       "      <td>2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>1987</td>\n",
       "      <td>1988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id1                id2  similarity_score      full_name_L  \\\n",
       "6681  5471702019|2019|L  5471702019|2019|S          0.753333   nick delmonico   \n",
       "6693  6689422022|2022|L  6703512022|2022|S          0.730000       josh rojas   \n",
       "6703  5948382019|2019|L  5948382019|2019|S          0.706667  philip gosselin   \n",
       "6756  6647472022|2022|L  6495572022|2022|S          0.625000      alexis diaz   \n",
       "6751  6072572017|2017|L  6072572017|2017|S          0.625000     rafael lopez   \n",
       "\n",
       "          full_name_S  season_year_L  season_year_S birth_year_L birth_year_S  \n",
       "6681  nicky delmonico           2019           2019         1992         1993  \n",
       "6693       jose rojas           2022           2022         1994         1993  \n",
       "6703    phil gosselin           2019           2019         1988         1989  \n",
       "6756     aledmys diaz           2022           2022         1996         1991  \n",
       "6751      raffy lopez           2017           2017         1987         1988  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LS LOW (bottom 10) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id1</th>\n",
       "      <th>id2</th>\n",
       "      <th>similarity_score</th>\n",
       "      <th>full_name_L</th>\n",
       "      <th>full_name_S</th>\n",
       "      <th>season_year_L</th>\n",
       "      <th>season_year_S</th>\n",
       "      <th>birth_year_L</th>\n",
       "      <th>birth_year_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6964</th>\n",
       "      <td>6723862022|2022|L</td>\n",
       "      <td>6661352022|2022|S</td>\n",
       "      <td>0.2</td>\n",
       "      <td>alejandro kirk</td>\n",
       "      <td>alex kirilloff</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022</td>\n",
       "      <td>1998</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6957</th>\n",
       "      <td>6661352023|2023|L</td>\n",
       "      <td>6723862023|2023|S</td>\n",
       "      <td>0.2</td>\n",
       "      <td>alex kirilloff</td>\n",
       "      <td>alejandro kirk</td>\n",
       "      <td>2023</td>\n",
       "      <td>2023</td>\n",
       "      <td>1997</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6958</th>\n",
       "      <td>6723862023|2023|L</td>\n",
       "      <td>6661352023|2023|S</td>\n",
       "      <td>0.2</td>\n",
       "      <td>alejandro kirk</td>\n",
       "      <td>alex kirilloff</td>\n",
       "      <td>2023</td>\n",
       "      <td>2023</td>\n",
       "      <td>1998</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6959</th>\n",
       "      <td>6661352022|2022|L</td>\n",
       "      <td>6723862022|2022|S</td>\n",
       "      <td>0.2</td>\n",
       "      <td>alex kirilloff</td>\n",
       "      <td>alejandro kirk</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022</td>\n",
       "      <td>1997</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6963</th>\n",
       "      <td>6723862024|2024|L</td>\n",
       "      <td>6661352024|2024|S</td>\n",
       "      <td>0.2</td>\n",
       "      <td>alejandro kirk</td>\n",
       "      <td>alex kirilloff</td>\n",
       "      <td>2024</td>\n",
       "      <td>2024</td>\n",
       "      <td>1998</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id1                id2  similarity_score     full_name_L  \\\n",
       "6964  6723862022|2022|L  6661352022|2022|S               0.2  alejandro kirk   \n",
       "6957  6661352023|2023|L  6723862023|2023|S               0.2  alex kirilloff   \n",
       "6958  6723862023|2023|L  6661352023|2023|S               0.2  alejandro kirk   \n",
       "6959  6661352022|2022|L  6723862022|2022|S               0.2  alex kirilloff   \n",
       "6963  6723862024|2024|L  6661352024|2024|S               0.2  alejandro kirk   \n",
       "\n",
       "         full_name_S  season_year_L  season_year_S birth_year_L birth_year_S  \n",
       "6964  alex kirilloff           2022           2022         1998         1998  \n",
       "6957  alejandro kirk           2023           2023         1997         1999  \n",
       "6958  alex kirilloff           2023           2023         1998         1998  \n",
       "6959  alejandro kirk           2022           2022         1997         1999  \n",
       "6963  alex kirilloff           2024           2024         1998         1998  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def simple_cases(df: pd.DataFrame, edge: str, k: int = 10):\n",
    "    lt, rt = edge[0], edge[1]\n",
    "    cols = [\n",
    "        'id1','id2','similarity_score',\n",
    "        f'full_name_{lt}' if f'full_name_{lt}' in df.columns else None,\n",
    "        f'full_name_{rt}' if f'full_name_{rt}' in df.columns else None,\n",
    "        f'season_year_{lt}', f'season_year_{rt}',\n",
    "        f'birth_year_{lt}' if f'birth_year_{lt}' in df.columns else None,\n",
    "        f'birth_year_{rt}' if f'birth_year_{rt}' in df.columns else None,\n",
    "    ]\n",
    "    cols = [c for c in cols if c and c in df.columns]\n",
    "\n",
    "    d = df.copy()\n",
    "    scores = pd.to_numeric(d['similarity_score'], errors='coerce')\n",
    "    d['bucket'] = pd.cut(scores, bins=[-1, 0.60, 0.99, 1.01], labels=['low','mid','high'], include_lowest=True)\n",
    "\n",
    "    print(f'\\n=== {edge} HIGH (top 10) ===')\n",
    "    hi = d[d['bucket']=='high'].sort_values('similarity_score', ascending=False).head(k)\n",
    "    display(hi[cols])\n",
    "\n",
    "    print(f'\\n=== {edge} MID (10 random) ===')\n",
    "    mid = d[d['bucket']=='mid']\n",
    "    mid = (mid.sample(n=min(k, len(mid)), random_state=42) if len(mid) > k else mid.head(k))\n",
    "    display(mid.sort_values('similarity_score', ascending=False)[cols])\n",
    "\n",
    "    print(f'\\n=== {edge} LOW (bottom 10) ===')\n",
    "    lo = d[d['bucket']=='low'].sort_values('similarity_score', ascending=True).head(k)\n",
    "    display(lo[cols])\n",
    "\n",
    "\n",
    "# Run for three edges using NON-ID scored candidates (cand_lr/cand_rs/cand_ls)\n",
    "simple_cases(cand_lr, 'LR', k=5)\n",
    "# simple_cases(cand_rs, 'RS', k=5)\n",
    "simple_cases(cand_ls, 'LS', k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "62814738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR pattern counts: {'manual_cases': np.int64(5), 'accent_suffix': np.int64(1), 'birth_conflict': np.int64(292)}\n",
      "LS pattern counts: {'manual_cases': np.int64(9), 'accent_suffix': np.int64(5), 'birth_conflict': np.int64(205)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'manual_cases': np.int64(9),\n",
       " 'accent_suffix': np.int64(5),\n",
       " 'birth_conflict': np.int64(205)}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def summarize_patterns(sample_df, edge):\n",
    "    lt, rt = edge[0], edge[1]\n",
    "    manual_pairs = MANUAL_ERROR_CASES.get(edge, set())\n",
    "\n",
    "    def _is_manual(row):\n",
    "        return (row['id1'], row['id2']) in manual_pairs\n",
    "\n",
    "    summary = {\n",
    "        'manual_cases': sample_df.apply(_is_manual, axis=1).sum(),\n",
    "        'accent_suffix': sample_df.apply(lambda r: _has_accent_or_suffix_variant(r, lt, rt), axis=1).sum(),\n",
    "        'birth_conflict': sample_df.apply(lambda r: _has_birth_conflict(r, lt, rt), axis=1).sum(),\n",
    "    }\n",
    "    print(f'{edge} pattern counts:', summary)\n",
    "    return summary\n",
    "\n",
    "summarize_patterns(results['LR'], 'LR')\n",
    "summarize_patterns(results['LS'], 'LS')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3ce47c",
   "metadata": {},
   "source": [
    "### 6.2 Auto-labeling with conservative rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8a031721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR prelabeled -> /Users/zhangzihan/Desktop/WBI_project/Schema_Mapped_Datasets/data/output/gt/samples/samples_LR_v1_prelabel_pid.csv TRUE= 150 FALSE= 350 REVIEW= 0\n",
      "LS prelabeled -> /Users/zhangzihan/Desktop/WBI_project/Schema_Mapped_Datasets/data/output/gt/samples/samples_LS_v1_prelabel_pid.csv TRUE= 238 FALSE= 262 REVIEW= 0\n"
     ]
    }
   ],
   "source": [
    "# Conservative prelabel: only update existing 'label' column (no extra columns)\n",
    "\n",
    "SAMPLE_OUT_DIR = OUTPUT_DIR / 'gt' / 'samples'\n",
    "for edge in ['LR','LS']:\n",
    "    in_path = SAMPLE_OUT_DIR / f'samples_{edge}_v1.csv'\n",
    "    if not in_path.exists():\n",
    "        print('MISS:', in_path)\n",
    "        continue\n",
    "    df = pd.read_csv(in_path)\n",
    "    if 'label' not in df.columns:\n",
    "        df['label'] = ''\n",
    "    df['label'] = df['label'].astype('string').fillna('')\n",
    "\n",
    "    lt, rt = edge[0], edge[1]\n",
    "    sy_l, sy_r = f'season_year_{lt}', f'season_year_{rt}'\n",
    "    by_l, by_r = f'birth_year_{lt}', f'birth_year_{rt}'\n",
    "\n",
    "    score = pd.to_numeric(df['similarity_score'], errors='coerce')\n",
    "    y_eq = (pd.to_numeric(df.get(sy_l), errors='coerce') == pd.to_numeric(df.get(sy_r), errors='coerce'))\n",
    "    by_diff = (pd.to_numeric(df.get(by_l), errors='coerce') - pd.to_numeric(df.get(by_r), errors='coerce')).abs()\n",
    "\n",
    "    # compare ids ignoring the trailing side tag (|L/|R/|S)\n",
    "    # Extract core IDs using shared utility function\n",
    "    core1, core2 = extract_core_ids(df)\n",
    "    same_core = core1 == core2\n",
    "\n",
    "    empty = (df['label'] == '')\n",
    "    # TRUE: core(id1) == core(id2)\n",
    "    fill_true = empty & same_core\n",
    "    df.loc[fill_true, 'label'] = 'TRUE'\n",
    "\n",
    "    # FALSE: for remaining empty labels where core(id1) != core(id2)\n",
    "    fill_false = empty & (~same_core)\n",
    "    df.loc[fill_false, 'label'] = 'FALSE'\n",
    "\n",
    "    out_path = SAMPLE_OUT_DIR / f'samples_{edge}_v1_prelabel_pid.csv'\n",
    "    df.to_csv(out_path, index=False)\n",
    "    print(edge, 'prelabeled ->', out_path, 'TRUE=', (df['label']=='TRUE').sum(), 'FALSE=', (df['label']=='FALSE').sum(), 'REVIEW=', (df['label']=='').sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8a4a0e",
   "metadata": {},
   "source": [
    "## 7. Ground-truth quality checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "321bcb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Ground Truth Assessment - LR\n",
      "============================================================\n",
      "Total: 500\n",
      "TRUE: 150 (30.0%)  FALSE: 350 (70.0%)  EMPTY: 0\n",
      "\n",
      "Similarity statistics for TRUE:\n",
      "  mean=0.889  median=1.000  min=0.550  max=1.000  std=0.159\n",
      "\n",
      "Similarity statistics for FALSE:\n",
      "  mean=0.530  median=0.538  min=0.200  max=1.000  std=0.112\n",
      "\n",
      "Core-ID consistency:\n",
      "  TRUE with same core-id: 150\n",
      "  TRUE with diff core-id: 0\n",
      "  FALSE with same core-id: 0\n",
      "  FALSE with diff core-id: 350\n",
      "\n",
      "Potential issues (for manual review):\n",
      "  High-similarity FALSE (score >= 0.99): 1\n",
      "  Low-similarity TRUE (score < 0.60): 5\n",
      "  Hard negatives (diff core-id, score >= 0.95): 1\n",
      "  Hard positives (same core-id, score < 0.80): 51\n",
      "\n",
      "Duplicate checks:\n",
      "  Rows in duplicate (id1,id2) groups: 18\n",
      "\n",
      "============================================================\n",
      "Ground Truth Assessment - LS\n",
      "============================================================\n",
      "Total: 500\n",
      "TRUE: 238 (47.6%)  FALSE: 262 (52.4%)  EMPTY: 0\n",
      "\n",
      "Similarity statistics for TRUE:\n",
      "  mean=0.941  median=1.000  min=0.550  max=1.000  std=0.127\n",
      "\n",
      "Similarity statistics for FALSE:\n",
      "  mean=0.510  median=0.513  min=0.200  max=1.000  std=0.122\n",
      "\n",
      "Core-ID consistency:\n",
      "  TRUE with same core-id: 238\n",
      "  TRUE with diff core-id: 0\n",
      "  FALSE with same core-id: 0\n",
      "  FALSE with diff core-id: 262\n",
      "\n",
      "Potential issues (for manual review):\n",
      "  High-similarity FALSE (score >= 0.99): 1\n",
      "  Low-similarity TRUE (score < 0.60): 3\n",
      "  Hard negatives (diff core-id, score >= 0.95): 1\n",
      "  Hard positives (same core-id, score < 0.80): 44\n",
      "\n",
      "Duplicate checks:\n",
      "  Rows in duplicate (id1,id2) groups: 42\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# This cell evaluates the labeled ground-truth CSVs using simple, interpretable metrics.\n",
    "# It checks: basic counts, label distribution, similarity-by-label stats, core-ID consistency,\n",
    "# and potential labeling inconsistencies (e.g., high-similarity FALSE, low-similarity TRUE).\n",
    "\n",
    "SAMPLE_OUT_DIR = OUTPUT_DIR / 'gt' / 'samples'\n",
    "\n",
    "\n",
    "def _load_gt(edge: str) -> pd.DataFrame:\n",
    "    path = SAMPLE_OUT_DIR / f'samples_{edge}_v1_prelabel_pid.csv'\n",
    "    if not path.exists():\n",
    "        print(f\"MISSING: {path}\")\n",
    "        return pd.DataFrame()\n",
    "    df = pd.read_csv(path)\n",
    "    # Ensure expected columns exist\n",
    "    for c in ['id1','id2','similarity_score','label']:\n",
    "        if c not in df.columns:\n",
    "            df[c] = pd.NA\n",
    "    return df\n",
    "\n",
    "\n",
    "# Note: Uses shared utility function extract_core_ids from Section 0.3\n",
    "\n",
    "def assess_edge(df: pd.DataFrame, edge: str) -> dict:\n",
    "    \"\"\"Assess ground truth quality and return anomaly cases for detailed inspection.\n",
    "    \n",
    "    Returns:\n",
    "        dict with keys:\n",
    "            - 'high_similarity_false': DataFrame of FALSE cases with similarity >= 0.99\n",
    "            - 'low_similarity_true': DataFrame of TRUE cases with similarity < 0.60\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return {'high_similarity_false': pd.DataFrame(), 'low_similarity_true': pd.DataFrame()}\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Ground Truth Assessment - {edge}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Basic totals (normalize labels first)\n",
    "    n = len(df)\n",
    "    lab = (df.get('label')\n",
    "           .astype('string')\n",
    "           .fillna('')\n",
    "           .str.strip()\n",
    "           .str.upper())\n",
    "    n_true = int((lab == 'TRUE').sum())\n",
    "    n_false = int((lab == 'FALSE').sum())\n",
    "    n_empty = int((~lab.isin(['TRUE','FALSE'])).sum())\n",
    "    print(f\"Total: {n}\")\n",
    "    print(f\"TRUE: {n_true} ({(n_true/max(n,1))*100:.1f}%)  FALSE: {n_false} ({(n_false/max(n,1))*100:.1f}%)  EMPTY: {n_empty}\")\n",
    "\n",
    "    # Similarity distributions by label\n",
    "    df['similarity_score'] = pd.to_numeric(df['similarity_score'], errors='coerce')\n",
    "    for lab_name in ['TRUE','FALSE']:\n",
    "        sub = df.loc[lab == lab_name, 'similarity_score'].dropna()\n",
    "        if len(sub) == 0:\n",
    "            continue\n",
    "        print(f\"\\nSimilarity statistics for {lab_name}:\")\n",
    "        print(f\"  mean={sub.mean():.3f}  median={sub.median():.3f}  min={sub.min():.3f}  max={sub.max():.3f}  std={sub.std():.3f}\")\n",
    "\n",
    "    # Core-ID consistency (using shared utility function)\n",
    "    core1, core2 = extract_core_ids(df)\n",
    "    same_core = core1 == core2\n",
    "    n_true_same = int(((lab == 'TRUE') & same_core).sum())\n",
    "    n_true_diff = int(((lab == 'TRUE') & ~same_core).sum())\n",
    "    n_false_same = int(((lab == 'FALSE') & same_core).sum())\n",
    "    n_false_diff = int(((lab == 'FALSE') & ~same_core).sum())\n",
    "    print(\"\\nCore-ID consistency:\")\n",
    "    print(f\"  TRUE with same core-id: {n_true_same}\")\n",
    "    print(f\"  TRUE with diff core-id: {n_true_diff}\")\n",
    "    print(f\"  FALSE with same core-id: {n_false_same}\")\n",
    "    print(f\"  FALSE with diff core-id: {n_false_diff}\")\n",
    "\n",
    "    # Potential inconsistencies to review (calculate and store DataFrames)\n",
    "    high_similarity_false = df[(lab == 'FALSE') & (df['similarity_score'] >= 0.99)].copy()\n",
    "    low_similarity_true = df[(lab == 'TRUE') & (df['similarity_score'] < 0.60)].copy()\n",
    "    \n",
    "    hi_false = len(high_similarity_false)\n",
    "    lo_true = len(low_similarity_true)\n",
    "    print(\"\\nPotential issues (for manual review):\")\n",
    "    print(f\"  High-similarity FALSE (score >= 0.99): {hi_false}\")\n",
    "    print(f\"  Low-similarity TRUE (score < 0.60): {lo_true}\")\n",
    "\n",
    "    # Hard negatives (diff core-id, high similarity)\n",
    "    hard_neg = int(((lab == 'FALSE') & (df['similarity_score'] >= 0.95)).sum())\n",
    "    print(f\"  Hard negatives (diff core-id, score >= 0.95): {hard_neg}\")\n",
    "\n",
    "    # Hard positives (same core-id, but lower similarity)\n",
    "    hard_pos = int(((lab == 'TRUE') & (df['similarity_score'] < 0.80)).sum())\n",
    "    print(f\"  Hard positives (same core-id, score < 0.80): {hard_pos}\")\n",
    "\n",
    "    # Duplicate checks on pair keys\n",
    "    pair_dups = int(df.duplicated(subset=['id1','id2'], keep=False).sum())\n",
    "    print(\"\\nDuplicate checks:\")\n",
    "    print(f\"  Rows in duplicate (id1,id2) groups: {pair_dups}\")\n",
    "    \n",
    "    # Return anomaly cases for detailed inspection\n",
    "    return {\n",
    "        'high_similarity_false': high_similarity_false,\n",
    "        'low_similarity_true': low_similarity_true\n",
    "    }\n",
    "\n",
    "\n",
    "# Run assessment for LR and LS and store anomaly cases\n",
    "lr = _load_gt('LR')\n",
    "ls = _load_gt('LS')\n",
    "lr_anomalies = assess_edge(lr, 'LR')\n",
    "ls_anomalies = assess_edge(ls, 'LS')\n",
    "print(\"\\nDone.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "04f21b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "1. LS: High-similarity FALSE cases (similarity >= 0.99)\n",
      "================================================================================\n",
      "Found 1 cases\n",
      "\n",
      "Full details:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id1</th>\n",
       "      <th>id2</th>\n",
       "      <th>similarity_score</th>\n",
       "      <th>full_name_L</th>\n",
       "      <th>full_name_S</th>\n",
       "      <th>season_year_L</th>\n",
       "      <th>season_year_S</th>\n",
       "      <th>birth_year_L</th>\n",
       "      <th>birth_year_S</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>4329342015|2015|L</td>\n",
       "      <td>4557592015|2015|S</td>\n",
       "      <td>1.0</td>\n",
       "      <td>chris young</td>\n",
       "      <td>chris young</td>\n",
       "      <td>2015</td>\n",
       "      <td>2015</td>\n",
       "      <td>1979</td>\n",
       "      <td>1984</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id1                id2  similarity_score  full_name_L  \\\n",
       "488  4329342015|2015|L  4557592015|2015|S               1.0  chris young   \n",
       "\n",
       "     full_name_S  season_year_L  season_year_S  birth_year_L  birth_year_S  \\\n",
       "488  chris young           2015           2015          1979          1984   \n",
       "\n",
       "     label  \n",
       "488  False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Core ID comparison:\n",
      "  Row 488: core1='4329342015|2015' vs core2='4557592015|2015' (match: False)\n",
      "    Name L: 'chris young'\n",
      "    Name S: 'chris young'\n",
      "    Season: 2015 vs 2015\n",
      "    Birth: 1979 vs 1984\n",
      "\n",
      "\n",
      "================================================================================\n",
      "2. LR: Low-similarity TRUE cases (similarity < 0.60)\n",
      "================================================================================\n",
      "Found 5 cases\n",
      "\n",
      "Summary statistics:\n",
      "  Similarity range: [0.550, 0.550]\n",
      "  Mean similarity: 0.550\n",
      "  Median similarity: 0.550\n",
      "\n",
      "  Birth year differences:\n",
      "    Same year: 0 cases\n",
      "    Diff by 1: 5 cases\n",
      "    Diff by 2+: 0 cases\n",
      "    Missing: 0 cases\n",
      "\n",
      "  Season year matches: 5 / 5\n",
      "\n",
      "First 20 cases (sorted by similarity ascending):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id1</th>\n",
       "      <th>id2</th>\n",
       "      <th>similarity_score</th>\n",
       "      <th>full_name_L</th>\n",
       "      <th>full_name_R</th>\n",
       "      <th>season_year_L</th>\n",
       "      <th>season_year_R</th>\n",
       "      <th>birth_year_L</th>\n",
       "      <th>birth_year_R</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>4770032011|2011|L</td>\n",
       "      <td>4770032011|2011|R</td>\n",
       "      <td>0.55</td>\n",
       "      <td>jonathon niese</td>\n",
       "      <td>jon niese</td>\n",
       "      <td>2011</td>\n",
       "      <td>2011</td>\n",
       "      <td>1986</td>\n",
       "      <td>1987</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>4770032014|2014|L</td>\n",
       "      <td>4770032014|2014|R</td>\n",
       "      <td>0.55</td>\n",
       "      <td>jonathon niese</td>\n",
       "      <td>jon niese</td>\n",
       "      <td>2014</td>\n",
       "      <td>2014</td>\n",
       "      <td>1986</td>\n",
       "      <td>1987</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>4770032008|2008|L</td>\n",
       "      <td>4770032008|2008|R</td>\n",
       "      <td>0.55</td>\n",
       "      <td>jonathon niese</td>\n",
       "      <td>jon niese</td>\n",
       "      <td>2008</td>\n",
       "      <td>2008</td>\n",
       "      <td>1986</td>\n",
       "      <td>1987</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>4770032010|2010|L</td>\n",
       "      <td>4770032010|2010|R</td>\n",
       "      <td>0.55</td>\n",
       "      <td>jonathon niese</td>\n",
       "      <td>jon niese</td>\n",
       "      <td>2010</td>\n",
       "      <td>2010</td>\n",
       "      <td>1986</td>\n",
       "      <td>1987</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>4770032009|2009|L</td>\n",
       "      <td>4770032009|2009|R</td>\n",
       "      <td>0.55</td>\n",
       "      <td>jonathon niese</td>\n",
       "      <td>jon niese</td>\n",
       "      <td>2009</td>\n",
       "      <td>2009</td>\n",
       "      <td>1986</td>\n",
       "      <td>1987</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id1                id2  similarity_score     full_name_L  \\\n",
       "338  4770032011|2011|L  4770032011|2011|R              0.55  jonathon niese   \n",
       "339  4770032014|2014|L  4770032014|2014|R              0.55  jonathon niese   \n",
       "345  4770032008|2008|L  4770032008|2008|R              0.55  jonathon niese   \n",
       "448  4770032010|2010|L  4770032010|2010|R              0.55  jonathon niese   \n",
       "497  4770032009|2009|L  4770032009|2009|R              0.55  jonathon niese   \n",
       "\n",
       "    full_name_R  season_year_L  season_year_R  birth_year_L  birth_year_R  \\\n",
       "338   jon niese           2011           2011          1986          1987   \n",
       "339   jon niese           2014           2014          1986          1987   \n",
       "345   jon niese           2008           2008          1986          1987   \n",
       "448   jon niese           2010           2010          1986          1987   \n",
       "497   jon niese           2009           2009          1986          1987   \n",
       "\n",
       "     label  \n",
       "338   True  \n",
       "339   True  \n",
       "345   True  \n",
       "448   True  \n",
       "497   True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 lowest similarity cases (detailed name analysis):\n",
      "\n",
      "  Case (sim=0.550):\n",
      "    Name L: 'jonathon niese'\n",
      "    Name R: 'jon niese'\n",
      "    Length L: 14, Length R: 9\n",
      "    Season: 2011 vs 2011\n",
      "    Birth: 1986 vs 1987\n",
      "\n",
      "  Case (sim=0.550):\n",
      "    Name L: 'jonathon niese'\n",
      "    Name R: 'jon niese'\n",
      "    Length L: 14, Length R: 9\n",
      "    Season: 2014 vs 2014\n",
      "    Birth: 1986 vs 1987\n",
      "\n",
      "  Case (sim=0.550):\n",
      "    Name L: 'jonathon niese'\n",
      "    Name R: 'jon niese'\n",
      "    Length L: 14, Length R: 9\n",
      "    Season: 2008 vs 2008\n",
      "    Birth: 1986 vs 1987\n",
      "\n",
      "  Case (sim=0.550):\n",
      "    Name L: 'jonathon niese'\n",
      "    Name R: 'jon niese'\n",
      "    Length L: 14, Length R: 9\n",
      "    Season: 2010 vs 2010\n",
      "    Birth: 1986 vs 1987\n",
      "\n",
      "  Case (sim=0.550):\n",
      "    Name L: 'jonathon niese'\n",
      "    Name R: 'jon niese'\n",
      "    Length L: 14, Length R: 9\n",
      "    Season: 2009 vs 2009\n",
      "    Birth: 1986 vs 1987\n",
      "\n",
      "================================================================================\n",
      "Done.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 7. Detailed inspection of potential issues\n",
    "# Check LS high-similarity FALSE and LR low-similarity TRUE cases\n",
    "# Use anomaly cases from assess_edge (Cell 57) - ensure Cell 57 has been executed\n",
    "# If anomalies are not available, fall back to loading and filtering\n",
    "if 'ls_anomalies' not in globals() or 'lr_anomalies' not in globals():\n",
    "    print(\"Warning: Anomaly cases not found. Please run Cell 57 (assess_edge) first.\")\n",
    "    print(\"Falling back to loading and filtering...\")\n",
    "    SAMPLE_OUT_DIR = OUTPUT_DIR / 'gt' / 'samples'\n",
    "    df_lr = _load_gt('LR')\n",
    "    df_ls = _load_gt('LS')\n",
    "    for df in [df_lr, df_ls]:\n",
    "        df['label_norm'] = df['label'].astype('string').fillna('').str.strip().str.upper()\n",
    "        df['similarity_score'] = pd.to_numeric(df['similarity_score'], errors='coerce')\n",
    "    ls_high_false = df_ls[(df_ls['label_norm'] == 'FALSE') & (df_ls['similarity_score'] >= 0.99)]\n",
    "    lr_low_true = df_lr[(df_lr['label_norm'] == 'TRUE') & (df_lr['similarity_score'] < 0.60)]\n",
    "else:\n",
    "    # Use pre-calculated anomaly cases from assess_edge\n",
    "    ls_high_false = ls_anomalies['high_similarity_false'].copy()\n",
    "    lr_low_true = lr_anomalies['low_similarity_true'].copy()\n",
    "    # Normalize labels for consistency\n",
    "    ls_high_false['label_norm'] = ls_high_false['label'].astype('string').fillna('').str.strip().str.upper()\n",
    "    lr_low_true['label_norm'] = lr_low_true['label'].astype('string').fillna('').str.strip().str.upper()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"1. LS: High-similarity FALSE cases (similarity >= 0.99)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Found {len(ls_high_false)} cases\\n\")\n",
    "\n",
    "if len(ls_high_false) > 0:\n",
    "    # Display all columns for inspection\n",
    "    cols_to_show = ['id1', 'id2', 'similarity_score', 'full_name_L', 'full_name_S', \n",
    "                    'season_year_L', 'season_year_S', 'birth_year_L', 'birth_year_S', 'label']\n",
    "    display_cols = [c for c in cols_to_show if c in ls_high_false.columns]\n",
    "    \n",
    "    print(\"Full details:\")\n",
    "    display(ls_high_false[display_cols])\n",
    "    \n",
    "    # Extract core IDs for comparison (using shared utility function)\n",
    "    core1, core2 = extract_core_ids(ls_high_false)\n",
    "    print(\"\\nCore ID comparison:\")\n",
    "    for idx, row in ls_high_false.iterrows():\n",
    "        print(f\"  Row {idx}: core1='{core1.loc[idx]}' vs core2='{core2.loc[idx]}' (match: {core1.loc[idx] == core2.loc[idx]})\")\n",
    "        print(f\"    Name L: '{row.get('full_name_L', 'N/A')}'\")\n",
    "        print(f\"    Name S: '{row.get('full_name_S', 'N/A')}'\")\n",
    "        print(f\"    Season: {row.get('season_year_L', 'N/A')} vs {row.get('season_year_S', 'N/A')}\")\n",
    "        print(f\"    Birth: {row.get('birth_year_L', 'N/A')} vs {row.get('birth_year_S', 'N/A')}\")\n",
    "        print()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"2. LR: Low-similarity TRUE cases (similarity < 0.60)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Found {len(lr_low_true)} cases\\n\")\n",
    "\n",
    "if len(lr_low_true) > 0:\n",
    "    # Show summary statistics\n",
    "    print(\"Summary statistics:\")\n",
    "    print(f\"  Similarity range: [{lr_low_true['similarity_score'].min():.3f}, {lr_low_true['similarity_score'].max():.3f}]\")\n",
    "    print(f\"  Mean similarity: {lr_low_true['similarity_score'].mean():.3f}\")\n",
    "    print(f\"  Median similarity: {lr_low_true['similarity_score'].median():.3f}\")\n",
    "    \n",
    "    # Check birth year differences\n",
    "    by_diff = (pd.to_numeric(lr_low_true.get('birth_year_L'), errors='coerce') - \n",
    "               pd.to_numeric(lr_low_true.get('birth_year_R'), errors='coerce')).abs()\n",
    "    print(f\"\\n  Birth year differences:\")\n",
    "    print(f\"    Same year: {(by_diff == 0).sum()} cases\")\n",
    "    print(f\"    Diff by 1: {(by_diff == 1).sum()} cases\")\n",
    "    print(f\"    Diff by 2+: {(by_diff > 1).sum()} cases\")\n",
    "    print(f\"    Missing: {by_diff.isna().sum()} cases\")\n",
    "    \n",
    "    # Check season year matches\n",
    "    sy_match = (pd.to_numeric(lr_low_true.get('season_year_L'), errors='coerce') == \n",
    "                pd.to_numeric(lr_low_true.get('season_year_R'), errors='coerce'))\n",
    "    print(f\"\\n  Season year matches: {sy_match.sum()} / {len(lr_low_true)}\")\n",
    "    \n",
    "    # Show first 20 cases for detailed inspection\n",
    "    print(\"\\nFirst 20 cases (sorted by similarity ascending):\")\n",
    "    cols_to_show = ['id1', 'id2', 'similarity_score', 'full_name_L', 'full_name_R', \n",
    "                    'season_year_L', 'season_year_R', 'birth_year_L', 'birth_year_R', 'label']\n",
    "    display_cols = [c for c in cols_to_show if c in lr_low_true.columns]\n",
    "    \n",
    "    lr_low_true_sorted = lr_low_true.sort_values('similarity_score', ascending=True)\n",
    "    display(lr_low_true_sorted[display_cols].head(20))\n",
    "    \n",
    "    # Analyze name differences for lowest similarity cases\n",
    "    print(\"\\nTop 10 lowest similarity cases (detailed name analysis):\")\n",
    "    top10_lowest = lr_low_true_sorted.head(10)\n",
    "    for idx, row in top10_lowest.iterrows():\n",
    "        name_l = str(row.get('full_name_L', '')).lower().strip()\n",
    "        name_r = str(row.get('full_name_R', '')).lower().strip()\n",
    "        print(f\"\\n  Case (sim={row['similarity_score']:.3f}):\")\n",
    "        print(f\"    Name L: '{name_l}'\")\n",
    "        print(f\"    Name R: '{name_r}'\")\n",
    "        print(f\"    Length L: {len(name_l)}, Length R: {len(name_r)}\")\n",
    "        print(f\"    Season: {row.get('season_year_L', 'N/A')} vs {row.get('season_year_R', 'N/A')}\")\n",
    "        print(f\"    Birth: {row.get('birth_year_L', 'N/A')} vs {row.get('birth_year_R', 'N/A')}\")\n",
    "        \n",
    "        # Check for encoding issues\n",
    "        if '\\\\x' in name_l or '\\\\x' in name_r:\n",
    "            print(f\"    WARNING: Potential encoding issue detected!\")\n",
    "        \n",
    "        # Check for suffix differences (jr, iii, etc.)\n",
    "        suffixes = ['jr', 'ii', 'iii', 'iv', 'sr']\n",
    "        has_suffix_l = any(name_l.endswith(f' {s}') or name_l.endswith(f' {s}.') for s in suffixes)\n",
    "        has_suffix_r = any(name_r.endswith(f' {s}') or name_r.endswith(f' {s}.') for s in suffixes)\n",
    "        if has_suffix_l != has_suffix_r:\n",
    "            print(f\"    NOTE: Suffix difference detected (L has suffix: {has_suffix_l}, R has suffix: {has_suffix_r})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Done.\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2964d943",
   "metadata": {},
   "source": [
    "## 8. Split ground truth into train/validation/test sets\n",
    "This ensures no data leakage: all samples from the same player_id stay in the same split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "69a1499f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Splitting Ground Truth by Player ID Groups (No Data Leakage)\n",
      "================================================================================\n",
      "\n",
      "LR: Found 451 unique player_ids from 500 samples\n",
      "\n",
      "LR Split Statistics:\n",
      "  train:  304 samples ( 60.8%) | TRUE:  90 ( 29.6%) | FALSE: 214 ( 70.4%)\n",
      "  val  :   96 samples ( 19.2%) | TRUE:  30 ( 31.2%) | FALSE:  66 ( 68.8%)\n",
      "  test :  100 samples ( 20.0%) | TRUE:  30 ( 30.0%) | FALSE:  70 ( 70.0%)\n",
      "\n",
      "LR Leakage Check:\n",
      "  Train-Val overlap: 0 player_ids\n",
      "  Train-Test overlap: 0 player_ids\n",
      "  Val-Test overlap: 0 player_ids\n",
      "  ✓ No player_id leakage detected!\n",
      "\n",
      "LS: Found 444 unique player_ids from 500 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fh/_tq_jbr50895yx1hkbgpct5m0000gn/T/ipykernel_2714/3673439390.py:49: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  player_groups = df.groupby('player_id').apply(lambda x: x.index.tolist()).to_dict()\n",
      "/var/folders/fh/_tq_jbr50895yx1hkbgpct5m0000gn/T/ipykernel_2714/3673439390.py:49: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  player_groups = df.groupby('player_id').apply(lambda x: x.index.tolist()).to_dict()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LS Split Statistics:\n",
      "  train:  296 samples ( 59.2%) | TRUE: 142 ( 48.0%) | FALSE: 154 ( 52.0%)\n",
      "  val  :  101 samples ( 20.2%) | TRUE:  49 ( 48.5%) | FALSE:  52 ( 51.5%)\n",
      "  test :  103 samples ( 20.6%) | TRUE:  47 ( 45.6%) | FALSE:  56 ( 54.4%)\n",
      "\n",
      "LS Leakage Check:\n",
      "  Train-Val overlap: 0 player_ids\n",
      "  Train-Test overlap: 0 player_ids\n",
      "  Val-Test overlap: 0 player_ids\n",
      "  ✓ No player_id leakage detected!\n",
      "\n",
      "================================================================================\n",
      "Saving split files...\n",
      "================================================================================\n",
      "Saved: /Users/zhangzihan/Desktop/WBI_project/Schema_Mapped_Datasets/data/output/gt/splits/gt_LR_train.csv (304 rows)\n",
      "Saved: /Users/zhangzihan/Desktop/WBI_project/Schema_Mapped_Datasets/data/output/gt/splits/gt_LR_val.csv (96 rows)\n",
      "Saved: /Users/zhangzihan/Desktop/WBI_project/Schema_Mapped_Datasets/data/output/gt/splits/gt_LR_test.csv (100 rows)\n",
      "Saved: /Users/zhangzihan/Desktop/WBI_project/Schema_Mapped_Datasets/data/output/gt/splits/gt_LS_train.csv (296 rows)\n",
      "Saved: /Users/zhangzihan/Desktop/WBI_project/Schema_Mapped_Datasets/data/output/gt/splits/gt_LS_val.csv (101 rows)\n",
      "Saved: /Users/zhangzihan/Desktop/WBI_project/Schema_Mapped_Datasets/data/output/gt/splits/gt_LS_test.csv (103 rows)\n",
      "\n",
      "================================================================================\n",
      "Split complete. All files saved to: /Users/zhangzihan/Desktop/WBI_project/Schema_Mapped_Datasets/data/output/gt/splits\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_OUT_DIR = OUTPUT_DIR / 'gt' / 'samples'\n",
    "GT_SPLIT_DIR = OUTPUT_DIR / 'gt' / 'splits'\n",
    "GT_SPLIT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load ground truth files using _load_gt function (defined in Section 7)\n",
    "# Note: Ensure Cell 54 (Section 7) has been executed to define _load_gt function\n",
    "df_lr = _load_gt('LR')\n",
    "df_ls = _load_gt('LS')\n",
    "\n",
    "\n",
    "def extract_player_id_from_id(id_str: str) -> str:\n",
    "    \"\"\"Extract player_id from id string (format: player_id|season_year|L/R/S).\"\"\"\n",
    "    # Remove trailing |L/|R/|S\n",
    "    core = str(id_str).replace('|L', '').replace('|R', '').replace('|S', '')\n",
    "    # Extract player_id (first part before |)\n",
    "    parts = core.split('|')\n",
    "    if len(parts) >= 1:\n",
    "        return parts[0]  # player_id\n",
    "    return core\n",
    "\n",
    "\n",
    "def split_gt_by_player_id(df: pd.DataFrame, edge: str, \n",
    "                          train_ratio: float = 0.6,\n",
    "                          val_ratio: float = 0.2,\n",
    "                          test_ratio: float = 0.2,\n",
    "                          seed: int = 42) -> dict:\n",
    "    \"\"\"\n",
    "    Split ground truth by player_id groups to avoid data leakage.\n",
    "    \n",
    "    Strategy:\n",
    "    1. Extract player_id from id1 (for TRUE cases, id1 and id2 have same player_id)\n",
    "    2. Group samples by player_id\n",
    "    3. Split player_id groups (not individual samples) into train/val/test\n",
    "    4. Ensure label distribution is preserved across splits\n",
    "    \n",
    "    Returns dict with keys: 'train', 'val', 'test', each containing a DataFrame\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Normalize labels\n",
    "    df['label_norm'] = df['label'].astype('string').fillna('').str.strip().str.upper()\n",
    "    \n",
    "    # Extract player_id from id1\n",
    "    # For TRUE cases: id1 and id2 have same player_id, so either works\n",
    "    # For FALSE cases: we use id1 as the primary entity identifier\n",
    "    df['player_id'] = df['id1'].apply(extract_player_id_from_id)\n",
    "    \n",
    "    # Group by player_id and collect indices\n",
    "    player_groups = df.groupby('player_id').apply(lambda x: x.index.tolist()).to_dict()\n",
    "    player_ids = list(player_groups.keys())\n",
    "    \n",
    "    print(f\"\\n{edge}: Found {len(player_ids)} unique player_ids from {len(df)} samples\")\n",
    "    \n",
    "    # Calculate label distribution for stratification\n",
    "    # For each player_id group, determine its primary label (most common label in that group)\n",
    "    player_labels = {}\n",
    "    for pid, indices in player_groups.items():\n",
    "        labels = df.loc[indices, 'label_norm'].value_counts()\n",
    "        player_labels[pid] = labels.index[0] if len(labels) > 0 else 'FALSE'\n",
    "    \n",
    "    # Create player_id-level DataFrame for splitting\n",
    "    player_df = pd.DataFrame({\n",
    "        'player_id': player_ids,\n",
    "        'primary_label': [player_labels[pid] for pid in player_ids],\n",
    "        'sample_count': [len(player_groups[pid]) for pid in player_ids]\n",
    "    })\n",
    "    \n",
    "    # First split: train vs (val+test)\n",
    "    train_players, temp_players = train_test_split(\n",
    "        player_df,\n",
    "        test_size=(val_ratio + test_ratio),\n",
    "        stratify=player_df['primary_label'],\n",
    "        random_state=seed\n",
    "    )\n",
    "    \n",
    "    # Second split: val vs test\n",
    "    val_size = val_ratio / (val_ratio + test_ratio)\n",
    "    val_players, test_players = train_test_split(\n",
    "        temp_players,\n",
    "        test_size=(1 - val_size),\n",
    "        stratify=temp_players['primary_label'],\n",
    "        random_state=seed\n",
    "    )\n",
    "    \n",
    "    # Collect samples for each split\n",
    "    train_indices = []\n",
    "    for pid in train_players['player_id']:\n",
    "        train_indices.extend(player_groups[pid])\n",
    "    \n",
    "    val_indices = []\n",
    "    for pid in val_players['player_id']:\n",
    "        val_indices.extend(player_groups[pid])\n",
    "    \n",
    "    test_indices = []\n",
    "    for pid in test_players['player_id']:\n",
    "        test_indices.extend(player_groups[pid])\n",
    "    \n",
    "    # Create split DataFrames\n",
    "    splits = {\n",
    "        'train': df.loc[train_indices].drop(columns=['player_id', 'label_norm']).reset_index(drop=True),\n",
    "        'val': df.loc[val_indices].drop(columns=['player_id', 'label_norm']).reset_index(drop=True),\n",
    "        'test': df.loc[test_indices].drop(columns=['player_id', 'label_norm']).reset_index(drop=True)\n",
    "    }\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"\\n{edge} Split Statistics:\")\n",
    "    for split_name, split_df in splits.items():\n",
    "        n = len(split_df)\n",
    "        lab = split_df['label'].astype('string').fillna('').str.strip().str.upper()\n",
    "        n_true = (lab == 'TRUE').sum()\n",
    "        n_false = (lab == 'FALSE').sum()\n",
    "        print(f\"  {split_name:5s}: {n:4d} samples ({n/len(df)*100:5.1f}%) | \"\n",
    "              f\"TRUE: {n_true:3d} ({n_true/max(n,1)*100:5.1f}%) | \"\n",
    "              f\"FALSE: {n_false:3d} ({n_false/max(n,1)*100:5.1f}%)\")\n",
    "    \n",
    "    # Verify no player_id leakage\n",
    "    train_pids = set(train_players['player_id'])\n",
    "    val_pids = set(val_players['player_id'])\n",
    "    test_pids = set(test_players['player_id'])\n",
    "    \n",
    "    overlap_train_val = train_pids & val_pids\n",
    "    overlap_train_test = train_pids & test_pids\n",
    "    overlap_val_test = val_pids & test_pids\n",
    "    \n",
    "    print(f\"\\n{edge} Leakage Check:\")\n",
    "    print(f\"  Train-Val overlap: {len(overlap_train_val)} player_ids\")\n",
    "    print(f\"  Train-Test overlap: {len(overlap_train_test)} player_ids\")\n",
    "    print(f\"  Val-Test overlap: {len(overlap_val_test)} player_ids\")\n",
    "    \n",
    "    if len(overlap_train_val) == 0 and len(overlap_train_test) == 0 and len(overlap_val_test) == 0:\n",
    "        print(f\"  ✓ No player_id leakage detected!\")\n",
    "    else:\n",
    "        print(f\"  ⚠ WARNING: Player_id leakage detected!\")\n",
    "    \n",
    "    return splits\n",
    "\n",
    "\n",
    "# Load ground truth files using _load_gt function (already loaded above, but keeping for clarity)\n",
    "# Note: The files are already loaded at the beginning of this cell using _load_gt\n",
    "# This section is kept for backward compatibility, but the actual loading happens above\n",
    "\n",
    "# Split both edges\n",
    "print(\"=\"*80)\n",
    "print(\"Splitting Ground Truth by Player ID Groups (No Data Leakage)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "splits_lr = split_gt_by_player_id(df_lr, 'LR', train_ratio=0.6, val_ratio=0.2, test_ratio=0.2, seed=42)\n",
    "splits_ls = split_gt_by_player_id(df_ls, 'LS', train_ratio=0.6, val_ratio=0.2, test_ratio=0.2, seed=42)\n",
    "\n",
    "# Save splits\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Saving split files...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for edge, splits in [('LR', splits_lr), ('LS', splits_ls)]:\n",
    "    for split_name, split_df in splits.items():\n",
    "        out_path = GT_SPLIT_DIR / f'gt_{edge}_{split_name}.csv'\n",
    "        split_df.to_csv(out_path, index=False)\n",
    "        print(f\"Saved: {out_path} ({len(split_df)} rows)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Split complete. All files saved to:\", GT_SPLIT_DIR)\n",
    "print(\"=\"*80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
